{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe6ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:04:36,986 [INFO] - Random seed set to 42\n",
      "2025-05-10 20:04:36,986 [INFO] - --- Starting CALM Stage 3: Minimal Adaptation & Benchmarking on STL-10 ---\n",
      "2025-05-10 20:04:36,986 [INFO] - Configuration:\n",
      "{\n",
      "    \"sb_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\Baseline\\\\exports\\\\mutual_learning\\\\20250419_174414\\\\baseline_student_mutual_learning.pth\",\n",
      "    \"sd_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\EnsembleDistillation\\\\exports\\\\cal_aware_distilled_model.pth\",\n",
      "    \"sm_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MutualLearning\\\\exports\\\\mutual_learning_20250503_234230_final_student.pth\",\n",
      "    \"smeta_recalibrated_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MetaStudent_AKTP\\\\recalibration\\\\meta_student_recalibrated_final.pth\",\n",
      "    \"base_student_arch\": \"efficientnet_b0\",\n",
      "    \"meta_student_arch\": \"efficientnet_b1\",\n",
      "    \"num_classes_stl10\": 10,\n",
      "    \"input_size_stl10\": 96,\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 0,\n",
      "    \"pin_memory\": true,\n",
      "    \"use_amp\": true,\n",
      "    \"seed\": 42,\n",
      "    \"dataset_base_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\",\n",
      "    \"stl10_data_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\\\\stl10_binary\",\n",
      "    \"output_dir\": \"Results\\\\CALM_Stage3_STL10_Benchmark\",\n",
      "    \"mean_stl10\": [\n",
      "        0.485,\n",
      "        0.456,\n",
      "        0.406\n",
      "    ],\n",
      "    \"std_stl10\": [\n",
      "        0.229,\n",
      "        0.224,\n",
      "        0.225\n",
      "    ],\n",
      "    \"device\": \"cuda\"\n",
      "}\n",
      "2025-05-10 20:04:36,989 [INFO] - Initial GPU Memory: 8.12 MB\n",
      "2025-05-10 20:04:36,989 [INFO] - Preparing STL-10 test dataloader, input size 96x96\n",
      "2025-05-10 20:04:36,991 [INFO] - Using STL-10 dataset from: C:\\Users\\Gading\\Downloads\\Research\\Dataset\\stl10_binary\n",
      "2025-05-10 20:04:41,206 [INFO] - STL-10 Test Dataset Size: 8000\n",
      "2025-05-10 20:04:41,206 [INFO] - Loading and adapting MetaStudent (S_meta Recalibrated) (efficientnet_b1) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MetaStudent_AKTP\\recalibration\\meta_student_recalibrated_final.pth\n",
      "2025-05-10 20:04:41,393 [INFO] - Successfully loaded weights for MetaStudent (S_meta Recalibrated) with 10-class head.\n",
      "2025-05-10 20:04:41,395 [INFO] - Replaced classifier of MetaStudent (S_meta Recalibrated) for 10 classes (for STL-10).\n",
      "2025-05-10 20:04:41,476 [INFO] - MetaStudent (S_meta Recalibrated) adapted, frozen, and moved to device.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:04:47,558 [INFO] - Results for MetaStudent_Smeta_Recalibrated_STL10 on STL-10: Acc=8.00%, ECE=0.1217, Loss=2.6155, F1=0.0307\n",
      "2025-05-10 20:04:47,560 [INFO] - GPU Memory: Current=33.37MB, Peak=705.34MB, Reserved=138.00MB\n",
      "2025-05-10 20:04:47,562 [INFO] - Final benchmark results saved to Results\\CALM_Stage3_STL10_Benchmark\\stage3_stl10_benchmark_results_20250510_200447.json\n",
      "2025-05-10 20:04:47,562 [INFO] - --- CALM Stage 3 Benchmarking on STL-10 Completed ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CALM Framework - Stage 3: Minimal Adaptation & Benchmarking on STL-10\n",
    "\n",
    "- Loads S_b (Baseline from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_d (Distilled Student from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_m (Mutual Student from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_meta (Meta-Student from Stage 2.5, CIFAR-10 trained & recalibrated)\n",
    "- Adapts classifier heads of all models for STL-10 (10 classes), freezes backbones.\n",
    "- Evaluates all adapted models on the STL-10 test set.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, efficientnet_b1, EfficientNet_B1_Weights\n",
    "from torch.amp import autocast # Updated import for modern autocast API\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt # Not used if plotting functions are removed\n",
    "# import seaborn as sns           # Not used\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, log_loss, f1_score, precision_score, recall_score # ADDED f1, precision, recall\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# from itertools import cycle     # Not used\n",
    "import logging\n",
    "import gc\n",
    "import random # ADDED missing import\n",
    "import traceback # ADDED missing import\n",
    "from packaging import version\n",
    "\n",
    "# Add numpy.core.multiarray.scalar to torch's safe globals\n",
    "# This allows loading checkpoints containing this numpy type with weights_only=True.\n",
    "if hasattr(np, 'core') and hasattr(np.core.multiarray, 'scalar'):\n",
    "    torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n",
    "\n",
    "# --- Setup Logging ---\n",
    "STAGE3_MODEL_NAME = \"CALM_Stage3_STL10_Benchmark\"\n",
    "RESULTS_PATH_BASE = \"Results\" \n",
    "STAGE3_RESULTS_PATH = os.path.join(RESULTS_PATH_BASE, STAGE3_MODEL_NAME)\n",
    "os.makedirs(STAGE3_RESULTS_PATH, exist_ok=True)\n",
    "# os.makedirs(os.path.join(STAGE3_RESULTS_PATH, \"plots\"), exist_ok=True) # Plots directory not strictly needed if plotting is removed\n",
    "\n",
    "log_file_stage3 = os.path.join(STAGE3_RESULTS_PATH, \"stage3_stl10_benchmark.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_stage3, encoding='utf-8'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"CALM_Stage3_Benchmark\")\n",
    "\n",
    "# --- Configuration for Stage 3 ---\n",
    "class ConfigStage3:\n",
    "    def __init__(self):\n",
    "        # Paths to pre-trained models from previous stages\n",
    "        # self.sb_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\ensemble_distillation\\20250419_185329\\baseline_student_ensemble_distillation.pth\"\n",
    "        self.sb_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\mutual_learning\\20250419_174414\\baseline_student_mutual_learning.pth\"\n",
    "        \n",
    "        self.sd_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\EnsembleDistillation\\exports\\cal_aware_distilled_model.pth\"\n",
    "        self.sm_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\exports\\mutual_learning_20250503_234230_final_student.pth\"\n",
    "        self.smeta_recalibrated_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MetaStudent_AKTP\\recalibration\\meta_student_recalibrated_final.pth\"\n",
    "\n",
    "        self.base_student_arch = \"efficientnet_b0\"\n",
    "        self.meta_student_arch = \"efficientnet_b1\"\n",
    "        \n",
    "        self.num_classes_stl10 = 10  # STL-10 has 10 classes\n",
    "        self.input_size_stl10 = 96   # Native STL-10 resolution\n",
    "\n",
    "        # Evaluation settings\n",
    "        self.batch_size = 64 \n",
    "        self.num_workers = 0 \n",
    "        self.pin_memory = True\n",
    "        self.use_amp = True \n",
    "        self.seed = 42        # Dataset paths\n",
    "        self.dataset_base_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Dataset\"\n",
    "        self.stl10_data_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Dataset\\stl10_binary\"\n",
    "        \n",
    "        # Output directory for Stage 3 results\n",
    "        self.output_dir = STAGE3_RESULTS_PATH\n",
    "        \n",
    "        # STL-10 Normalization (ImageNet stats as a common default)\n",
    "        self.mean_stl10 = [0.485, 0.456, 0.406]\n",
    "        self.std_stl10 = [0.229, 0.224, 0.225]\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Utilities ---\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); os.environ[\"PYTHONHASHSEED\"] = str(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False \n",
    "    torch.backends.cudnn.benchmark = True \n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer): return int(obj)\n",
    "        elif isinstance(obj, np.floating): return float(obj)\n",
    "        elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        elif isinstance(obj, torch.device): return str(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# ADDED print_gpu_memory_stats function\n",
    "def print_gpu_memory_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        current_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        max_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        reserved_mem = torch.cuda.memory_reserved() / 1024**2\n",
    "        logger.info(f\"GPU Memory: Current={current_mem:.2f}MB, Peak={max_mem:.2f}MB, Reserved={reserved_mem:.2f}MB\")\n",
    "\n",
    "\n",
    "# --- Calibration Metrics ---\n",
    "class CalibrationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_ece(probs, labels, n_bins=15):\n",
    "        if isinstance(labels, torch.Tensor): labels = labels.cpu().numpy()\n",
    "        if isinstance(probs, torch.Tensor): probs = probs.cpu().numpy()\n",
    "\n",
    "        confidences = np.max(probs, axis=1)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        accuracies = (predictions == labels)\n",
    "        \n",
    "        bin_boundaries = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        ece = 0.0\n",
    "        total_samples = len(labels)\n",
    "        if total_samples == 0: return 0.0\n",
    "\n",
    "        for i in range(n_bins):\n",
    "            bin_lower = bin_boundaries[i]\n",
    "            bin_upper = bin_boundaries[i + 1]\n",
    "            if i == n_bins - 1: in_bin = (confidences >= bin_lower) & (confidences <= bin_upper)\n",
    "            else: in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "            bin_count = np.sum(in_bin)\n",
    "            \n",
    "            if bin_count > 0:\n",
    "                accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "                avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "                ece += (bin_count / total_samples) * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "        return ece\n",
    "\n",
    "# --- Data Preparation for STL-10 ---\n",
    "def get_stl10_test_loader(config):\n",
    "    logger.info(f\"Preparing STL-10 test dataloader, input size {config.input_size_stl10}x{config.input_size_stl10}\")\n",
    "    normalize = transforms.Normalize(mean=config.mean_stl10, std=config.std_stl10)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((config.input_size_stl10, config.input_size_stl10), antialias=True),\n",
    "        transforms.ToTensor(), \n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    # Use the direct path to stl10_binary\n",
    "    stl10_data_root = config.stl10_data_path\n",
    "    logger.info(f\"Using STL-10 dataset from: {stl10_data_root}\")\n",
    "    \n",
    "    try:\n",
    "        # Set download=False as we already have the dataset\n",
    "        test_dataset = datasets.STL10(root=os.path.dirname(stl10_data_root), split='test', download=False, transform=test_transform)\n",
    "    except Exception as e: \n",
    "        logger.error(f\"Failed to load STL-10 test set: {e}\"); raise\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, \n",
    "                             num_workers=config.num_workers, pin_memory=config.pin_memory)\n",
    "    logger.info(f\"STL-10 Test Dataset Size: {len(test_dataset)}\")\n",
    "    return test_loader\n",
    "\n",
    "# --- Model Loading and Adaptation ---\n",
    "def load_and_adapt_model(checkpoint_path, model_arch, original_num_classes, new_num_classes, device, model_name_log=\"Model\"):\n",
    "    logger.info(f\"Loading and adapting {model_name_log} ({model_arch}) from: {checkpoint_path}\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        logger.error(f\"Checkpoint not found for {model_name_log} at {checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    if model_arch == \"efficientnet_b0\":\n",
    "        model = efficientnet_b0(weights=None)\n",
    "        original_in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(original_in_features, original_num_classes)\n",
    "        )\n",
    "    elif model_arch == \"efficientnet_b1\":\n",
    "        model = efficientnet_b1(weights=None)\n",
    "        original_in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=True), \n",
    "            nn.Linear(original_in_features, original_num_classes)\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"Unsupported architecture for adaptation: {model_arch}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "        state_dict_key = 'model_state_dict' if 'model_state_dict' in checkpoint else \\\n",
    "                         'meta_student_state_dict' if 'meta_student_state_dict' in checkpoint else None\n",
    "        \n",
    "        model_state_to_load = checkpoint[state_dict_key] if state_dict_key else checkpoint\n",
    "\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(model_state_to_load, strict=True)\n",
    "        if missing_keys: logger.warning(f\"Missing keys loading {model_name_log} (original head): {missing_keys}\")\n",
    "        if unexpected_keys: logger.warning(f\"Unexpected keys loading {model_name_log} (original head): {unexpected_keys}\")\n",
    "        logger.info(f\"Successfully loaded weights for {model_name_log} with {original_num_classes}-class head.\")\n",
    "\n",
    "        if model_arch == \"efficientnet_b0\":\n",
    "            in_features_new = model.classifier[1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                 nn.Dropout(p=0.2, inplace=True), \n",
    "                 nn.Linear(in_features_new, new_num_classes)\n",
    "            )\n",
    "        elif model_arch == \"efficientnet_b1\":\n",
    "            in_features_new = model.classifier[1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                 nn.Dropout(p=0.3, inplace=True), \n",
    "                 nn.Linear(in_features_new, new_num_classes)\n",
    "            )\n",
    "        logger.info(f\"Replaced classifier of {model_name_log} for {new_num_classes} classes (for STL-10).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading or adapting {model_name_log}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "    model = model.to(device)\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    model.eval() \n",
    "    logger.info(f\"{model_name_log} adapted, frozen, and moved to device.\")\n",
    "    return model\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "@torch.no_grad()\n",
    "def evaluate_on_stl10(model, loader, device, config, model_name_log=\"Model\"):\n",
    "    model.eval()\n",
    "    all_probs_list = []\n",
    "    all_targets_list = []\n",
    "\n",
    "    criterion_ce_eval = nn.CrossEntropyLoss() \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, targets in tqdm(loader, desc=f\"Evaluating {model_name_log} on STL-10\", leave=False):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "        # Use torch.amp.autocast with device_type for modern API and to address FutureWarning\n",
    "        with autocast(device_type=device.type, enabled=config.use_amp and device.type == 'cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion_ce_eval(outputs, targets)\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(probs, 1)\n",
    "\n",
    "        all_probs_list.append(probs.cpu().numpy())\n",
    "        all_targets_list.append(targets.cpu().numpy())\n",
    "        total_correct += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "    if total_samples == 0: \n",
    "        logger.warning(f\"No samples processed for {model_name_log}.\")\n",
    "        return {'accuracy': 0, 'ece': float('inf'), 'loss': float('inf'), 'f1_score':0, 'precision':0, 'recall':0}\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = 100. * total_correct / total_samples\n",
    "\n",
    "    all_probs_np = np.concatenate(all_probs_list, axis=0)\n",
    "    all_targets_np = np.concatenate(all_targets_list, axis=0)\n",
    "\n",
    "    ece = CalibrationMetrics.compute_ece(all_probs_np, all_targets_np)\n",
    "    predictions_np = np.argmax(all_probs_np, axis=1)\n",
    "    f1 = f1_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "    precision = precision_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy, \n",
    "        'ece': ece, \n",
    "        'loss': avg_loss,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    logger.info(f\"Results for {model_name_log} on STL-10: Acc={accuracy:.2f}%, ECE={ece:.4f}, Loss={avg_loss:.4f}, F1={f1:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "def main():\n",
    "    config = ConfigStage3()\n",
    "    set_seed(config.seed)\n",
    "    logger.info(f\"--- Starting CALM Stage 3: Minimal Adaptation & Benchmarking on STL-10 ---\")\n",
    "    logger.info(f\"Configuration:\\n{json.dumps(config.__dict__, indent=4, cls=NumpyEncoder)}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "\n",
    "    # Get STL-10 Test Loader\n",
    "    stl10_test_loader = get_stl10_test_loader(config)\n",
    "\n",
    "    # Load and Adapt Models\n",
    "    # s_b = load_and_adapt_model(config.sb_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Baseline (S_b)\")\n",
    "    # s_d = load_and_adapt_model(config.sd_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Distilled (S_d)\")\n",
    "    # s_m = load_and_adapt_model(config.sm_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Mutual (S_m)\")\n",
    "    s_meta = load_and_adapt_model(config.smeta_recalibrated_path, config.meta_student_arch, 10, config.num_classes_stl10, config.device, \"MetaStudent (S_meta Recalibrated)\")\n",
    "\n",
    "    models_to_evaluate = {\n",
    "        # \"Baseline_Sb_STL10\": s_b,\n",
    "        # \"Distilled_Sd_STL10\": s_d,\n",
    "        # \"Mutual_Sm_STL10\": s_m,\n",
    "        \"MetaStudent_Smeta_Recalibrated_STL10\": s_meta\n",
    "    }\n",
    "\n",
    "    final_benchmark_results = {}\n",
    "\n",
    "    for name_log, model_instance in models_to_evaluate.items():\n",
    "        if model_instance is None:\n",
    "            logger.warning(f\"Skipping evaluation for {name_log} as model loading/adaptation failed.\")\n",
    "            final_benchmark_results[name_log] = \"Loading/Adaptation Failed\"\n",
    "            continue\n",
    "        \n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
    "        metrics = evaluate_on_stl10(model_instance, stl10_test_loader, config.device, config, model_name_log=name_log)\n",
    "        final_benchmark_results[name_log] = metrics\n",
    "        if torch.cuda.is_available(): print_gpu_memory_stats() # Log memory after each eval\n",
    "\n",
    "    # Save final comparative results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_results_filename = f\"stage3_stl10_benchmark_results_{timestamp}.json\"\n",
    "    final_results_path = os.path.join(config.output_dir, final_results_filename)\n",
    "    try:\n",
    "        with open(final_results_path, 'w') as f:\n",
    "            json.dump(final_benchmark_results, f, indent=4, cls=NumpyEncoder)\n",
    "        logger.info(f\"Final benchmark results saved to {final_results_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save final benchmark results: {e}\")\n",
    "\n",
    "    logger.info(\"--- CALM Stage 3 Benchmarking on STL-10 Completed ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9452c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 22:36:59,069 [INFO] - Random seed set to 42\n",
      "2025-05-10 22:36:59,070 [INFO] - --- Starting CALM Stage 3: Minimal Adaptation & Benchmarking on STL-10 ---\n",
      "2025-05-10 22:36:59,071 [INFO] - Configuration:\n",
      "{\n",
      "    \"sb_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\Baseline\\\\exports\\\\mutual_learning\\\\20250419_174414\\\\baseline_student_mutual_learning.pth\",\n",
      "    \"sd_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\EnsembleDistillation\\\\exports\\\\cal_aware_distilled_model.pth\",\n",
      "    \"sm_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MutualLearning\\\\exports\\\\mutual_learning_20250503_234230_final_student.pth\",\n",
      "    \"smeta_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MetaStudent_AKTP\\\\recalibration\\\\meta_student_recalibrated_best.pth\",\n",
      "    \"base_student_arch\": \"efficientnet_b0\",\n",
      "    \"meta_student_arch\": \"efficientnet_b1\",\n",
      "    \"num_classes_stl10\": 10,\n",
      "    \"input_size_stl10\": 224,\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 0,\n",
      "    \"pin_memory\": true,\n",
      "    \"use_amp\": true,\n",
      "    \"seed\": 42,\n",
      "    \"dataset_base_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\",\n",
      "    \"stl10_data_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\\\\stl10_binary\",\n",
      "    \"output_dir\": \"Results\\\\CALM_Stage3_STL10_Benchmark\",\n",
      "    \"mean_stl10\": [\n",
      "        0.485,\n",
      "        0.456,\n",
      "        0.406\n",
      "    ],\n",
      "    \"std_stl10\": [\n",
      "        0.229,\n",
      "        0.224,\n",
      "        0.225\n",
      "    ],\n",
      "    \"device\": \"cuda\"\n",
      "}\n",
      "2025-05-10 22:36:59,072 [INFO] - Initial GPU Memory: 8.12 MB\n",
      "2025-05-10 22:36:59,074 [INFO] - Preparing STL-10 test dataloader, input size 224x224\n",
      "2025-05-10 22:36:59,074 [INFO] - Using STL-10 dataset from: C:\\Users\\Gading\\Downloads\\Research\\Dataset\\stl10_binary\n",
      "2025-05-10 22:37:04,684 [INFO] - STL-10 Test Dataset Size: 8000\n",
      "2025-05-10 22:37:04,684 [INFO] - Loading and adapting Baseline (S_b) (efficientnet_b0) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\mutual_learning\\20250419_174414\\baseline_student_mutual_learning.pth\n",
      "2025-05-10 22:37:04,826 [INFO] - Successfully loaded weights for Baseline (S_b) with 10-class head.\n",
      "2025-05-10 22:37:04,830 [INFO] - Replaced classifier of Baseline (S_b) for 10 classes (for STL-10).\n",
      "2025-05-10 22:37:04,862 [INFO] - Baseline (S_b) adapted, frozen, and moved to device.\n",
      "2025-05-10 22:37:04,866 [INFO] - Loading and adapting Distilled (S_d) (efficientnet_b0) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\EnsembleDistillation\\exports\\cal_aware_distilled_model.pth\n",
      "2025-05-10 22:37:05,044 [INFO] - Successfully loaded weights for Distilled (S_d) with 10-class head.\n",
      "2025-05-10 22:37:05,045 [INFO] - Replaced classifier of Distilled (S_d) for 10 classes (for STL-10).\n",
      "2025-05-10 22:37:05,101 [INFO] - Distilled (S_d) adapted, frozen, and moved to device.\n",
      "2025-05-10 22:37:05,103 [INFO] - Loading and adapting Mutual (S_m) (efficientnet_b0) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\exports\\mutual_learning_20250503_234230_final_student.pth\n",
      "2025-05-10 22:37:05,265 [INFO] - Successfully loaded weights for Mutual (S_m) with 10-class head.\n",
      "2025-05-10 22:37:05,265 [INFO] - Replaced classifier of Mutual (S_m) for 10 classes (for STL-10).\n",
      "2025-05-10 22:37:05,332 [INFO] - Mutual (S_m) adapted, frozen, and moved to device.\n",
      "2025-05-10 22:37:05,334 [INFO] - Loading and adapting MetaStudent (S_meta Recalibrated) (efficientnet_b1) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MetaStudent_AKTP\\recalibration\\meta_student_recalibrated_best.pth\n",
      "2025-05-10 22:37:05,575 [INFO] - Successfully loaded weights for MetaStudent (S_meta Recalibrated) with 10-class head.\n",
      "2025-05-10 22:37:05,577 [INFO] - Replaced classifier of MetaStudent (S_meta Recalibrated) for 10 classes (for STL-10).\n",
      "2025-05-10 22:37:05,645 [INFO] - MetaStudent (S_meta Recalibrated) adapted, frozen, and moved to device.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 22:37:28,011 [INFO] - Results for MetaStudent_Smeta_STL10 on STL-10: Acc=6.97%, ECE=0.0515, Loss=2.3299, F1=0.0524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 22:37:28,014 [INFO] - GPU Memory: Current=80.09MB, Peak=657.06MB, Reserved=596.00MB\n",
      "2025-05-10 22:37:28,016 [INFO] - Final benchmark results saved to Results\\CALM_Stage3_STL10_Benchmark\\stage3_stl10_benchmark_results_20250510_223728.json\n",
      "2025-05-10 22:37:28,017 [INFO] - --- CALM Stage 3 Benchmarking on STL-10 Completed ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CALM Framework - Stage 3: Minimal Adaptation & Benchmarking on STL-10\n",
    "\n",
    "- Loads S_b (Baseline from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_d (Distilled Student from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_m (Mutual Student from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_meta (Meta-Student from Stage 2.5, CIFAR-10 trained & recalibrated)\n",
    "- Adapts classifier heads of all models for STL-10 (10 classes), freezes backbones.\n",
    "- Evaluates all adapted models on the STL-10 test set.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, efficientnet_b1, EfficientNet_B1_Weights\n",
    "from torch.amp import autocast # Updated import for modern autocast API\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt # Not used if plotting functions are removed\n",
    "# import seaborn as sns           # Not used\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, log_loss, f1_score, precision_score, recall_score # ADDED f1, precision, recall\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# from itertools import cycle     # Not used\n",
    "import logging\n",
    "import gc\n",
    "import random # ADDED missing import\n",
    "import traceback # ADDED missing import\n",
    "from packaging import version\n",
    "\n",
    "# Add numpy.core.multiarray.scalar to torch's safe globals\n",
    "# This allows loading checkpoints containing this numpy type with weights_only=True.\n",
    "if hasattr(np, 'core') and hasattr(np.core.multiarray, 'scalar'):\n",
    "    torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n",
    "\n",
    "# --- Setup Logging ---\n",
    "STAGE3_MODEL_NAME = \"CALM_Stage3_STL10_Benchmark\"\n",
    "RESULTS_PATH_BASE = \"Results\" \n",
    "STAGE3_RESULTS_PATH = os.path.join(RESULTS_PATH_BASE, STAGE3_MODEL_NAME)\n",
    "os.makedirs(STAGE3_RESULTS_PATH, exist_ok=True)\n",
    "# os.makedirs(os.path.join(STAGE3_RESULTS_PATH, \"plots\"), exist_ok=True) # Plots directory not strictly needed if plotting is removed\n",
    "\n",
    "log_file_stage3 = os.path.join(STAGE3_RESULTS_PATH, \"stage3_stl10_benchmark.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_stage3, encoding='utf-8'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"CALM_Stage3_Benchmark\")\n",
    "\n",
    "# --- Configuration for Stage 3 ---\n",
    "class ConfigStage3:\n",
    "    def __init__(self):\n",
    "        # Paths to pre-trained models from previous stages\n",
    "        # self.sb_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\ensemble_distillation\\20250419_185329\\baseline_student_ensemble_distillation.pth\"\n",
    "        self.sb_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\mutual_learning\\20250419_174414\\baseline_student_mutual_learning.pth\"\n",
    "        \n",
    "        self.sd_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\EnsembleDistillation\\exports\\cal_aware_distilled_model.pth\"\n",
    "        self.sm_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\exports\\mutual_learning_20250503_234230_final_student.pth\"\n",
    "        self.smeta_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MetaStudent_AKTP\\recalibration\\meta_student_recalibrated_best.pth\"\n",
    "\n",
    "        self.base_student_arch = \"efficientnet_b0\"\n",
    "        self.meta_student_arch = \"efficientnet_b1\"\n",
    "        \n",
    "        self.num_classes_stl10 = 10  # STL-10 has 10 classes\n",
    "        self.input_size_stl10 = 224   # Native STL-10 resolution\n",
    "\n",
    "        # Evaluation settings\n",
    "        self.batch_size = 64 \n",
    "        self.num_workers = 0 \n",
    "        self.pin_memory = True\n",
    "        self.use_amp = True \n",
    "        self.seed = 42        # Dataset paths\n",
    "        self.dataset_base_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Dataset\"\n",
    "        self.stl10_data_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Dataset\\stl10_binary\"\n",
    "        \n",
    "        # Output directory for Stage 3 results\n",
    "        self.output_dir = STAGE3_RESULTS_PATH\n",
    "        \n",
    "        # STL-10 Normalization (ImageNet stats as a common default)\n",
    "        self.mean_stl10 = [0.485, 0.456, 0.406]\n",
    "        self.std_stl10 = [0.229, 0.224, 0.225]\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Utilities ---\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); os.environ[\"PYTHONHASHSEED\"] = str(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False \n",
    "    torch.backends.cudnn.benchmark = True \n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer): return int(obj)\n",
    "        elif isinstance(obj, np.floating): return float(obj)\n",
    "        elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        elif isinstance(obj, torch.device): return str(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# ADDED print_gpu_memory_stats function\n",
    "def print_gpu_memory_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        current_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        max_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        reserved_mem = torch.cuda.memory_reserved() / 1024**2\n",
    "        logger.info(f\"GPU Memory: Current={current_mem:.2f}MB, Peak={max_mem:.2f}MB, Reserved={reserved_mem:.2f}MB\")\n",
    "\n",
    "\n",
    "# --- Calibration Metrics ---\n",
    "class CalibrationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_ece(probs, labels, n_bins=15):\n",
    "        if isinstance(labels, torch.Tensor): labels = labels.cpu().numpy()\n",
    "        if isinstance(probs, torch.Tensor): probs = probs.cpu().numpy()\n",
    "\n",
    "        confidences = np.max(probs, axis=1)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        accuracies = (predictions == labels)\n",
    "        \n",
    "        bin_boundaries = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        ece = 0.0\n",
    "        total_samples = len(labels)\n",
    "        if total_samples == 0: return 0.0\n",
    "\n",
    "        for i in range(n_bins):\n",
    "            bin_lower = bin_boundaries[i]\n",
    "            bin_upper = bin_boundaries[i + 1]\n",
    "            if i == n_bins - 1: in_bin = (confidences >= bin_lower) & (confidences <= bin_upper)\n",
    "            else: in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "            bin_count = np.sum(in_bin)\n",
    "            \n",
    "            if bin_count > 0:\n",
    "                accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "                avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "                ece += (bin_count / total_samples) * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "        return ece\n",
    "\n",
    "# --- Data Preparation for STL-10 ---\n",
    "def get_stl10_test_loader(config):\n",
    "    logger.info(f\"Preparing STL-10 test dataloader, input size {config.input_size_stl10}x{config.input_size_stl10}\")\n",
    "    normalize = transforms.Normalize(mean=config.mean_stl10, std=config.std_stl10)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((config.input_size_stl10, config.input_size_stl10), antialias=True),\n",
    "        transforms.ToTensor(), \n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    # Use the direct path to stl10_binary\n",
    "    stl10_data_root = config.stl10_data_path\n",
    "    logger.info(f\"Using STL-10 dataset from: {stl10_data_root}\")\n",
    "    \n",
    "    try:\n",
    "        # Set download=False as we already have the dataset\n",
    "        test_dataset = datasets.STL10(root=os.path.dirname(stl10_data_root), split='test', download=False, transform=test_transform)\n",
    "    except Exception as e: \n",
    "        logger.error(f\"Failed to load STL-10 test set: {e}\"); raise\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, \n",
    "                             num_workers=config.num_workers, pin_memory=config.pin_memory)\n",
    "    logger.info(f\"STL-10 Test Dataset Size: {len(test_dataset)}\")\n",
    "    return test_loader\n",
    "\n",
    "# --- Model Loading and Adaptation ---\n",
    "def load_and_adapt_model(checkpoint_path, model_arch, original_num_classes, new_num_classes, device, model_name_log=\"Model\"):\n",
    "    logger.info(f\"Loading and adapting {model_name_log} ({model_arch}) from: {checkpoint_path}\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        logger.error(f\"Checkpoint not found for {model_name_log} at {checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    if model_arch == \"efficientnet_b0\":\n",
    "        model = efficientnet_b0(weights=None)\n",
    "        original_in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(original_in_features, original_num_classes)\n",
    "        )\n",
    "    elif model_arch == \"efficientnet_b1\":\n",
    "        model = efficientnet_b1(weights=None)\n",
    "        original_in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=True), \n",
    "            nn.Linear(original_in_features, original_num_classes)\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"Unsupported architecture for adaptation: {model_arch}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "        state_dict_key = 'model_state_dict' if 'model_state_dict' in checkpoint else \\\n",
    "                         'meta_student_state_dict' if 'meta_student_state_dict' in checkpoint else None\n",
    "        \n",
    "        model_state_to_load = checkpoint[state_dict_key] if state_dict_key else checkpoint\n",
    "\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(model_state_to_load, strict=True)\n",
    "        if missing_keys: logger.warning(f\"Missing keys loading {model_name_log} (original head): {missing_keys}\")\n",
    "        if unexpected_keys: logger.warning(f\"Unexpected keys loading {model_name_log} (original head): {unexpected_keys}\")\n",
    "        logger.info(f\"Successfully loaded weights for {model_name_log} with {original_num_classes}-class head.\")\n",
    "\n",
    "        if model_arch == \"efficientnet_b0\":\n",
    "            in_features_new = model.classifier[1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                 nn.Dropout(p=0.2, inplace=True), \n",
    "                 nn.Linear(in_features_new, new_num_classes)\n",
    "            )\n",
    "        elif model_arch == \"efficientnet_b1\":\n",
    "            in_features_new = model.classifier[1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                 nn.Dropout(p=0.3, inplace=True), \n",
    "                 nn.Linear(in_features_new, new_num_classes)\n",
    "            )\n",
    "        logger.info(f\"Replaced classifier of {model_name_log} for {new_num_classes} classes (for STL-10).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading or adapting {model_name_log}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "    model = model.to(device)\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    model.eval() \n",
    "    logger.info(f\"{model_name_log} adapted, frozen, and moved to device.\")\n",
    "    return model\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "@torch.no_grad()\n",
    "def evaluate_on_stl10(model, loader, device, config, model_name_log=\"Model\"):\n",
    "    model.eval()\n",
    "    all_probs_list = []\n",
    "    all_targets_list = []\n",
    "\n",
    "    criterion_ce_eval = nn.CrossEntropyLoss() \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, targets in tqdm(loader, desc=f\"Evaluating {model_name_log} on STL-10\", leave=False):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "        # Use torch.amp.autocast with device_type for modern API and to address FutureWarning\n",
    "        with autocast(device_type=device.type, enabled=config.use_amp and device.type == 'cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion_ce_eval(outputs, targets)\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(probs, 1)\n",
    "\n",
    "        all_probs_list.append(probs.cpu().numpy())\n",
    "        all_targets_list.append(targets.cpu().numpy())\n",
    "        total_correct += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "    if total_samples == 0: \n",
    "        logger.warning(f\"No samples processed for {model_name_log}.\")\n",
    "        return {'accuracy': 0, 'ece': float('inf'), 'loss': float('inf'), 'f1_score':0, 'precision':0, 'recall':0}\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = 100. * total_correct / total_samples\n",
    "\n",
    "    all_probs_np = np.concatenate(all_probs_list, axis=0)\n",
    "    all_targets_np = np.concatenate(all_targets_list, axis=0)\n",
    "\n",
    "    ece = CalibrationMetrics.compute_ece(all_probs_np, all_targets_np)\n",
    "    predictions_np = np.argmax(all_probs_np, axis=1)\n",
    "    f1 = f1_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "    precision = precision_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy, \n",
    "        'ece': ece, \n",
    "        'loss': avg_loss,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    logger.info(f\"Results for {model_name_log} on STL-10: Acc={accuracy:.2f}%, ECE={ece:.4f}, Loss={avg_loss:.4f}, F1={f1:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "def main():\n",
    "    config = ConfigStage3()\n",
    "    set_seed(config.seed)\n",
    "    logger.info(f\"--- Starting CALM Stage 3: Minimal Adaptation & Benchmarking on STL-10 ---\")\n",
    "    logger.info(f\"Configuration:\\n{json.dumps(config.__dict__, indent=4, cls=NumpyEncoder)}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "\n",
    "    # Get STL-10 Test Loader\n",
    "    stl10_test_loader = get_stl10_test_loader(config)\n",
    "\n",
    "    # Load and Adapt Models\n",
    "    s_b = load_and_adapt_model(config.sb_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Baseline (S_b)\")\n",
    "    s_d = load_and_adapt_model(config.sd_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Distilled (S_d)\")\n",
    "    s_m = load_and_adapt_model(config.sm_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Mutual (S_m)\")\n",
    "    s_meta = load_and_adapt_model(config.smeta_path, config.meta_student_arch, 10, config.num_classes_stl10, config.device, \"MetaStudent (S_meta Recalibrated)\")\n",
    "\n",
    "    models_to_evaluate = {\n",
    "        # \"Baseline_Sb_STL10\": s_b,\n",
    "        # \"Distilled_Sd_STL10\": s_d,\n",
    "        # \"Mutual_Sm_STL10\": s_m,\n",
    "        \"MetaStudent_Smeta_STL10\": s_meta\n",
    "    }\n",
    "\n",
    "    final_benchmark_results = {}\n",
    "\n",
    "    for name_log, model_instance in models_to_evaluate.items():\n",
    "        if model_instance is None:\n",
    "            logger.warning(f\"Skipping evaluation for {name_log} as model loading/adaptation failed.\")\n",
    "            final_benchmark_results[name_log] = \"Loading/Adaptation Failed\"\n",
    "            continue\n",
    "        \n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
    "        metrics = evaluate_on_stl10(model_instance, stl10_test_loader, config.device, config, model_name_log=name_log)\n",
    "        final_benchmark_results[name_log] = metrics\n",
    "        if torch.cuda.is_available(): print_gpu_memory_stats() # Log memory after each eval\n",
    "\n",
    "    # Save final comparative results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_results_filename = f\"stage3_stl10_benchmark_results_{timestamp}.json\"\n",
    "    final_results_path = os.path.join(config.output_dir, final_results_filename)\n",
    "    try:\n",
    "        with open(final_results_path, 'w') as f:\n",
    "            json.dump(final_benchmark_results, f, indent=4, cls=NumpyEncoder)\n",
    "        logger.info(f\"Final benchmark results saved to {final_results_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save final benchmark results: {e}\")\n",
    "\n",
    "    logger.info(\"--- CALM Stage 3 Benchmarking on STL-10 Completed ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f0a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:55:23,730 [INFO] - Random seed set to 42\n",
      "2025-05-10 18:55:23,731 [INFO] - --- Starting CALM Stage 3: Minimal Adaptation & Benchmarking on STL-10 ---\n",
      "2025-05-10 18:55:23,731 [INFO] - Configuration:\n",
      "{\n",
      "    \"sb_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\Baseline\\\\exports\\\\mutual_learning\\\\20250419_174414\\\\baseline_student_mutual_learning.pth\",\n",
      "    \"sd_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\EnsembleDistillation\\\\exports\\\\cal_aware_distilled_model.pth\",\n",
      "    \"sm_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MutualLearning\\\\exports\\\\mutual_learning_20250503_234230_final_student.pth\",\n",
      "    \"smeta_recalibrated_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MetaStudent_AKTP\\\\recalibration_stl10\\\\meta_student_recalibrated_stl10_best.pth\",\n",
      "    \"base_student_arch\": \"efficientnet_b0\",\n",
      "    \"meta_student_arch\": \"efficientnet_b1\",\n",
      "    \"num_classes_stl10\": 10,\n",
      "    \"input_size_stl10\": 224,\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 0,\n",
      "    \"pin_memory\": true,\n",
      "    \"use_amp\": true,\n",
      "    \"seed\": 42,\n",
      "    \"dataset_base_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\",\n",
      "    \"stl10_data_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\\\\stl10_binary\",\n",
      "    \"output_dir\": \"Results\\\\CALM_Stage3_STL10_Benchmark\",\n",
      "    \"mean_stl10\": [\n",
      "        0.485,\n",
      "        0.456,\n",
      "        0.406\n",
      "    ],\n",
      "    \"std_stl10\": [\n",
      "        0.229,\n",
      "        0.224,\n",
      "        0.225\n",
      "    ],\n",
      "    \"device\": \"cuda\"\n",
      "}\n",
      "2025-05-10 18:55:23,732 [INFO] - Initial GPU Memory: 8.12 MB\n",
      "2025-05-10 18:55:23,733 [INFO] - Preparing STL-10 test dataloader, input size 224x224\n",
      "2025-05-10 18:55:23,734 [INFO] - Using STL-10 dataset from: C:\\Users\\Gading\\Downloads\\Research\\Dataset\\stl10_binary\n",
      "2025-05-10 18:55:28,022 [INFO] - STL-10 Test Dataset Size: 8000\n",
      "2025-05-10 18:55:28,023 [INFO] - Loading and adapting Baseline (S_b) (efficientnet_b0) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\mutual_learning\\20250419_174414\\baseline_student_mutual_learning.pth\n",
      "2025-05-10 18:55:28,115 [INFO] - Successfully loaded weights for Baseline (S_b) with 10-class head.\n",
      "2025-05-10 18:55:28,117 [INFO] - Replaced classifier of Baseline (S_b) for 10 classes (for STL-10).\n",
      "2025-05-10 18:55:28,183 [INFO] - Baseline (S_b) adapted, frozen, and moved to device.\n",
      "2025-05-10 18:55:28,183 [INFO] - Loading and adapting Distilled (S_d) (efficientnet_b0) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\EnsembleDistillation\\exports\\cal_aware_distilled_model.pth\n",
      "2025-05-10 18:55:28,283 [INFO] - Successfully loaded weights for Distilled (S_d) with 10-class head.\n",
      "2025-05-10 18:55:28,283 [INFO] - Replaced classifier of Distilled (S_d) for 10 classes (for STL-10).\n",
      "2025-05-10 18:55:28,351 [INFO] - Distilled (S_d) adapted, frozen, and moved to device.\n",
      "2025-05-10 18:55:28,351 [INFO] - Loading and adapting Mutual (S_m) (efficientnet_b0) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\exports\\mutual_learning_20250503_234230_final_student.pth\n",
      "2025-05-10 18:55:28,434 [INFO] - Successfully loaded weights for Mutual (S_m) with 10-class head.\n",
      "2025-05-10 18:55:28,450 [INFO] - Replaced classifier of Mutual (S_m) for 10 classes (for STL-10).\n",
      "2025-05-10 18:55:28,498 [INFO] - Mutual (S_m) adapted, frozen, and moved to device.\n",
      "2025-05-10 18:55:28,498 [INFO] - Loading and adapting MetaStudent (S_meta Recalibrated) (efficientnet_b1) from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MetaStudent_AKTP\\recalibration_stl10\\meta_student_recalibrated_stl10_best.pth\n",
      "2025-05-10 18:55:28,775 [INFO] - Successfully loaded weights for MetaStudent (S_meta Recalibrated) with 10-class head.\n",
      "2025-05-10 18:55:28,775 [INFO] - Replaced classifier of MetaStudent (S_meta Recalibrated) for 10 classes (for STL-10).\n",
      "2025-05-10 18:55:28,833 [INFO] - MetaStudent (S_meta Recalibrated) adapted, frozen, and moved to device.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:55:42,305 [INFO] - Results for Baseline_Sb_STL10 on STL-10: Acc=12.41%, ECE=0.0045, Loss=2.2923, F1=0.0720\n",
      "2025-05-10 18:55:42,306 [INFO] - GPU Memory: Current=81.65MB, Peak=705.34MB, Reserved=620.00MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:55:54,045 [INFO] - Results for Distilled_Sd_STL10 on STL-10: Acc=4.42%, ECE=0.0690, Loss=2.3348, F1=0.0231\n",
      "2025-05-10 18:55:54,056 [INFO] - GPU Memory: Current=81.65MB, Peak=705.34MB, Reserved=620.00MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:56:05,917 [INFO] - Results for Mutual_Sm_STL10 on STL-10: Acc=13.56%, ECE=0.0177, Loss=2.3206, F1=0.1263\n",
      "2025-05-10 18:56:05,920 [INFO] - GPU Memory: Current=81.65MB, Peak=705.34MB, Reserved=620.00MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:56:19,686 [INFO] - Results for MetaStudent_Smeta_Recalibrated_STL10 on STL-10: Acc=9.71%, ECE=0.0594, Loss=2.3319, F1=0.0794\n",
      "2025-05-10 18:56:19,687 [INFO] - GPU Memory: Current=81.65MB, Peak=705.34MB, Reserved=600.00MB\n",
      "2025-05-10 18:56:19,688 [INFO] - Final benchmark results saved to Results\\CALM_Stage3_STL10_Benchmark\\stage3_stl10_benchmark_results_20250510_185619.json\n",
      "2025-05-10 18:56:19,688 [INFO] - --- CALM Stage 3 Benchmarking on STL-10 Completed ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CALM Framework - Stage 3: Minimal Adaptation & Benchmarking on STL-10\n",
    "\n",
    "- Loads S_b (Baseline from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_d (Distilled Student from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_m (Mutual Student from Stage 1, CIFAR-10 trained)\n",
    "- Loads S_meta (Meta-Student from Stage 2.5, CIFAR-10 trained & recalibrated)\n",
    "- Adapts classifier heads of all models for STL-10 (10 classes), freezes backbones.\n",
    "- Evaluates all adapted models on the STL-10 test set.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, efficientnet_b1, EfficientNet_B1_Weights\n",
    "from torch.amp import autocast # Updated import for modern autocast API\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt # Not used if plotting functions are removed\n",
    "# import seaborn as sns           # Not used\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, log_loss, f1_score, precision_score, recall_score # ADDED f1, precision, recall\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# from itertools import cycle     # Not used\n",
    "import logging\n",
    "import gc\n",
    "import random # ADDED missing import\n",
    "import traceback # ADDED missing import\n",
    "from packaging import version\n",
    "\n",
    "# Add numpy.core.multiarray.scalar to torch's safe globals\n",
    "# This allows loading checkpoints containing this numpy type with weights_only=True.\n",
    "if hasattr(np, 'core') and hasattr(np.core.multiarray, 'scalar'):\n",
    "    torch.serialization.add_safe_globals([np.core.multiarray.scalar])\n",
    "\n",
    "# --- Setup Logging ---\n",
    "STAGE3_MODEL_NAME = \"CALM_Stage3_STL10_Benchmark\"\n",
    "RESULTS_PATH_BASE = \"Results\" \n",
    "STAGE3_RESULTS_PATH = os.path.join(RESULTS_PATH_BASE, STAGE3_MODEL_NAME)\n",
    "os.makedirs(STAGE3_RESULTS_PATH, exist_ok=True)\n",
    "# os.makedirs(os.path.join(STAGE3_RESULTS_PATH, \"plots\"), exist_ok=True) # Plots directory not strictly needed if plotting is removed\n",
    "\n",
    "log_file_stage3 = os.path.join(STAGE3_RESULTS_PATH, \"stage3_stl10_benchmark.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_stage3, encoding='utf-8'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"CALM_Stage3_Benchmark\")\n",
    "\n",
    "# --- Configuration for Stage 3 ---\n",
    "class ConfigStage3:\n",
    "    def __init__(self):\n",
    "        # Paths to pre-trained models from previous stages\n",
    "        # self.sb_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\ensemble_distillation\\20250419_185329\\baseline_student_ensemble_distillation.pth\"\n",
    "        self.sb_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\Baseline\\exports\\mutual_learning\\20250419_174414\\baseline_student_mutual_learning.pth\"\n",
    "        \n",
    "        self.sd_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\EnsembleDistillation\\exports\\cal_aware_distilled_model.pth\"\n",
    "        self.sm_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\exports\\mutual_learning_20250503_234230_final_student.pth\"\n",
    "        self.smeta_recalibrated_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MetaStudent_AKTP\\recalibration_stl10\\meta_student_recalibrated_stl10_best.pth\"\n",
    "\n",
    "        self.base_student_arch = \"efficientnet_b0\"\n",
    "        self.meta_student_arch = \"efficientnet_b1\"\n",
    "        \n",
    "        self.num_classes_stl10 = 10  # STL-10 has 10 classes\n",
    "        self.input_size_stl10 = 224   # Native STL-10 resolution\n",
    "\n",
    "        # Evaluation settings\n",
    "        self.batch_size = 64 \n",
    "        self.num_workers = 0 \n",
    "        self.pin_memory = True\n",
    "        self.use_amp = True \n",
    "        self.seed = 42        # Dataset paths\n",
    "        self.dataset_base_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Dataset\"\n",
    "        self.stl10_data_path = r\"C:\\Users\\Gading\\Downloads\\Research\\Dataset\\stl10_binary\"\n",
    "        \n",
    "        # Output directory for Stage 3 results\n",
    "        self.output_dir = STAGE3_RESULTS_PATH\n",
    "        \n",
    "        # STL-10 Normalization (ImageNet stats as a common default)\n",
    "        self.mean_stl10 = [0.485, 0.456, 0.406]\n",
    "        self.std_stl10 = [0.229, 0.224, 0.225]\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Utilities ---\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); os.environ[\"PYTHONHASHSEED\"] = str(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False \n",
    "    torch.backends.cudnn.benchmark = True \n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer): return int(obj)\n",
    "        elif isinstance(obj, np.floating): return float(obj)\n",
    "        elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        elif isinstance(obj, torch.device): return str(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# ADDED print_gpu_memory_stats function\n",
    "def print_gpu_memory_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        current_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        max_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        reserved_mem = torch.cuda.memory_reserved() / 1024**2\n",
    "        logger.info(f\"GPU Memory: Current={current_mem:.2f}MB, Peak={max_mem:.2f}MB, Reserved={reserved_mem:.2f}MB\")\n",
    "\n",
    "\n",
    "# --- Calibration Metrics ---\n",
    "class CalibrationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_ece(probs, labels, n_bins=15):\n",
    "        if isinstance(labels, torch.Tensor): labels = labels.cpu().numpy()\n",
    "        if isinstance(probs, torch.Tensor): probs = probs.cpu().numpy()\n",
    "\n",
    "        confidences = np.max(probs, axis=1)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        accuracies = (predictions == labels)\n",
    "        \n",
    "        bin_boundaries = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        ece = 0.0\n",
    "        total_samples = len(labels)\n",
    "        if total_samples == 0: return 0.0\n",
    "\n",
    "        for i in range(n_bins):\n",
    "            bin_lower = bin_boundaries[i]\n",
    "            bin_upper = bin_boundaries[i + 1]\n",
    "            if i == n_bins - 1: in_bin = (confidences >= bin_lower) & (confidences <= bin_upper)\n",
    "            else: in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "            bin_count = np.sum(in_bin)\n",
    "            \n",
    "            if bin_count > 0:\n",
    "                accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "                avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "                ece += (bin_count / total_samples) * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "        return ece\n",
    "\n",
    "# --- Data Preparation for STL-10 ---\n",
    "def get_stl10_test_loader(config):\n",
    "    logger.info(f\"Preparing STL-10 test dataloader, input size {config.input_size_stl10}x{config.input_size_stl10}\")\n",
    "    normalize = transforms.Normalize(mean=config.mean_stl10, std=config.std_stl10)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((config.input_size_stl10, config.input_size_stl10), antialias=True),\n",
    "        transforms.ToTensor(), \n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    # Use the direct path to stl10_binary\n",
    "    stl10_data_root = config.stl10_data_path\n",
    "    logger.info(f\"Using STL-10 dataset from: {stl10_data_root}\")\n",
    "    \n",
    "    try:\n",
    "        # Set download=False as we already have the dataset\n",
    "        test_dataset = datasets.STL10(root=os.path.dirname(stl10_data_root), split='test', download=False, transform=test_transform)\n",
    "    except Exception as e: \n",
    "        logger.error(f\"Failed to load STL-10 test set: {e}\"); raise\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, \n",
    "                             num_workers=config.num_workers, pin_memory=config.pin_memory)\n",
    "    logger.info(f\"STL-10 Test Dataset Size: {len(test_dataset)}\")\n",
    "    return test_loader\n",
    "\n",
    "# --- Model Loading and Adaptation ---\n",
    "def load_and_adapt_model(checkpoint_path, model_arch, original_num_classes, new_num_classes, device, model_name_log=\"Model\"):\n",
    "    logger.info(f\"Loading and adapting {model_name_log} ({model_arch}) from: {checkpoint_path}\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        logger.error(f\"Checkpoint not found for {model_name_log} at {checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    if model_arch == \"efficientnet_b0\":\n",
    "        model = efficientnet_b0(weights=None)\n",
    "        original_in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(original_in_features, original_num_classes)\n",
    "        )\n",
    "    elif model_arch == \"efficientnet_b1\":\n",
    "        model = efficientnet_b1(weights=None)\n",
    "        original_in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=True), \n",
    "            nn.Linear(original_in_features, original_num_classes)\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"Unsupported architecture for adaptation: {model_arch}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "        state_dict_key = 'model_state_dict' if 'model_state_dict' in checkpoint else \\\n",
    "                         'meta_student_state_dict' if 'meta_student_state_dict' in checkpoint else None\n",
    "        \n",
    "        model_state_to_load = checkpoint[state_dict_key] if state_dict_key else checkpoint\n",
    "\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(model_state_to_load, strict=True)\n",
    "        if missing_keys: logger.warning(f\"Missing keys loading {model_name_log} (original head): {missing_keys}\")\n",
    "        if unexpected_keys: logger.warning(f\"Unexpected keys loading {model_name_log} (original head): {unexpected_keys}\")\n",
    "        logger.info(f\"Successfully loaded weights for {model_name_log} with {original_num_classes}-class head.\")\n",
    "\n",
    "        if model_arch == \"efficientnet_b0\":\n",
    "            in_features_new = model.classifier[1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                 nn.Dropout(p=0.2, inplace=True), \n",
    "                 nn.Linear(in_features_new, new_num_classes)\n",
    "            )\n",
    "        elif model_arch == \"efficientnet_b1\":\n",
    "            in_features_new = model.classifier[1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                 nn.Dropout(p=0.3, inplace=True), \n",
    "                 nn.Linear(in_features_new, new_num_classes)\n",
    "            )\n",
    "        logger.info(f\"Replaced classifier of {model_name_log} for {new_num_classes} classes (for STL-10).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading or adapting {model_name_log}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "    model = model.to(device)\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    model.eval() \n",
    "    logger.info(f\"{model_name_log} adapted, frozen, and moved to device.\")\n",
    "    return model\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "@torch.no_grad()\n",
    "def evaluate_on_stl10(model, loader, device, config, model_name_log=\"Model\"):\n",
    "    model.eval()\n",
    "    all_probs_list = []\n",
    "    all_targets_list = []\n",
    "\n",
    "    criterion_ce_eval = nn.CrossEntropyLoss() \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, targets in tqdm(loader, desc=f\"Evaluating {model_name_log} on STL-10\", leave=False):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "        # Use torch.amp.autocast with device_type for modern API and to address FutureWarning\n",
    "        with autocast(device_type=device.type, enabled=config.use_amp and device.type == 'cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion_ce_eval(outputs, targets)\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(probs, 1)\n",
    "\n",
    "        all_probs_list.append(probs.cpu().numpy())\n",
    "        all_targets_list.append(targets.cpu().numpy())\n",
    "        total_correct += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "    if total_samples == 0: \n",
    "        logger.warning(f\"No samples processed for {model_name_log}.\")\n",
    "        return {'accuracy': 0, 'ece': float('inf'), 'loss': float('inf'), 'f1_score':0, 'precision':0, 'recall':0}\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = 100. * total_correct / total_samples\n",
    "\n",
    "    all_probs_np = np.concatenate(all_probs_list, axis=0)\n",
    "    all_targets_np = np.concatenate(all_targets_list, axis=0)\n",
    "\n",
    "    ece = CalibrationMetrics.compute_ece(all_probs_np, all_targets_np)\n",
    "    predictions_np = np.argmax(all_probs_np, axis=1)\n",
    "    f1 = f1_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "    precision = precision_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_targets_np, predictions_np, average='macro', zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy, \n",
    "        'ece': ece, \n",
    "        'loss': avg_loss,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    logger.info(f\"Results for {model_name_log} on STL-10: Acc={accuracy:.2f}%, ECE={ece:.4f}, Loss={avg_loss:.4f}, F1={f1:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "def main():\n",
    "    config = ConfigStage3()\n",
    "    set_seed(config.seed)\n",
    "    logger.info(f\"--- Starting CALM Stage 3: Minimal Adaptation & Benchmarking on STL-10 ---\")\n",
    "    logger.info(f\"Configuration:\\n{json.dumps(config.__dict__, indent=4, cls=NumpyEncoder)}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "\n",
    "    # Get STL-10 Test Loader\n",
    "    stl10_test_loader = get_stl10_test_loader(config)\n",
    "\n",
    "    # Load and Adapt Models\n",
    "    s_b = load_and_adapt_model(config.sb_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Baseline (S_b)\")\n",
    "    s_d = load_and_adapt_model(config.sd_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Distilled (S_d)\")\n",
    "    s_m = load_and_adapt_model(config.sm_path, config.base_student_arch, 10, config.num_classes_stl10, config.device, \"Mutual (S_m)\")\n",
    "    s_meta = load_and_adapt_model(config.smeta_recalibrated_path, config.meta_student_arch, 10, config.num_classes_stl10, config.device, \"MetaStudent (S_meta Recalibrated)\")\n",
    "\n",
    "    models_to_evaluate = {\n",
    "        \"Baseline_Sb_STL10\": s_b,\n",
    "        \"Distilled_Sd_STL10\": s_d,\n",
    "        \"Mutual_Sm_STL10\": s_m,\n",
    "        \"MetaStudent_Smeta_Recalibrated_STL10\": s_meta\n",
    "    }\n",
    "\n",
    "    final_benchmark_results = {}\n",
    "\n",
    "    for name_log, model_instance in models_to_evaluate.items():\n",
    "        if model_instance is None:\n",
    "            logger.warning(f\"Skipping evaluation for {name_log} as model loading/adaptation failed.\")\n",
    "            final_benchmark_results[name_log] = \"Loading/Adaptation Failed\"\n",
    "            continue\n",
    "        \n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
    "        metrics = evaluate_on_stl10(model_instance, stl10_test_loader, config.device, config, model_name_log=name_log)\n",
    "        final_benchmark_results[name_log] = metrics\n",
    "        if torch.cuda.is_available(): print_gpu_memory_stats() # Log memory after each eval\n",
    "\n",
    "    # Save final comparative results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_results_filename = f\"stage3_stl10_benchmark_results_{timestamp}.json\"\n",
    "    final_results_path = os.path.join(config.output_dir, final_results_filename)\n",
    "    try:\n",
    "        with open(final_results_path, 'w') as f:\n",
    "            json.dump(final_benchmark_results, f, indent=4, cls=NumpyEncoder)\n",
    "        logger.info(f\"Final benchmark results saved to {final_results_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save final benchmark results: {e}\")\n",
    "\n",
    "    logger.info(\"--- CALM Stage 3 Benchmarking on STL-10 Completed ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
