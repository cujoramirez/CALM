{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:47:33,262 [INFO] - Using device: cuda\n",
      "2025-05-09 01:47:33,265 [INFO] - GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "2025-05-09 01:47:33,265 [INFO] - Available memory: 6.44 GB\n",
      "2025-05-09 01:47:33,265 [INFO] - Starting ensemble distillation evaluation...\n",
      "2025-05-09 01:47:33,265 [INFO] - Loading student model from: C:\\Users\\Gading\\Downloads\\Research\\Models\\EnsembleDistillation\\exports\\cal_aware_distilled_model.pth\n",
      "2025-05-09 01:47:33,269 [INFO] - Creating student model architecture...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ensemble Distillation Evaluation Pipeline\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:47:33,519 [INFO] - Student model state loaded from 'model_state_dict'\n",
      "2025-05-09 01:47:33,519 [INFO] - Teacher weights loaded from checkpoint: {'vit': 0.38825134116073234, 'efficientnet': 0.13210255220711029, 'inception': 0.08131136123086424, 'mobilenet': 0.13238247173004686, 'resnet': 0.1320022366872692, 'densenet': 0.1339500369839771}\n",
      "2025-05-09 01:47:33,519 [INFO] - Teacher temperatures loaded from checkpoint: {'vit': 2.8279638290405273, 'efficientnet': 3.539057493209839, 'inception': 4.367103576660156, 'mobilenet': 3.524693489074707, 'resnet': 3.4917454719543457, 'densenet': 3.4410717487335205}\n",
      "2025-05-09 01:47:33,519 [INFO] - Previous test metrics found in checkpoint:\n",
      "2025-05-09 01:47:33,519 [INFO] -   - loss: 0.1288561333967432\n",
      "2025-05-09 01:47:33,519 [INFO] -   - accuracy: 97.3\n",
      "2025-05-09 01:47:33,519 [INFO] -   - f1_score: 0.9729687854640469\n",
      "2025-05-09 01:47:33,519 [INFO] -   - precision: 0.9729771492586057\n",
      "2025-05-09 01:47:33,519 [INFO] -   - recall: 0.9730000000000001\n",
      "2025-05-09 01:47:33,526 [INFO] -   - ece: 0.01865815930068493\n",
      "2025-05-09 01:47:33,550 [INFO] - Student model loaded successfully and set to evaluation mode\n",
      "2025-05-09 01:47:33,552 [INFO] - Loading teacher models for comparison...\n",
      "2025-05-09 01:47:33,553 [INFO] - Loading vit teacher model...\n",
      "2025-05-09 01:47:33,553 [INFO] - Creating vit model architecture...\n",
      "2025-05-09 01:47:35,387 [INFO] - Teacher model vit loaded successfully\n",
      "2025-05-09 01:47:35,389 [INFO] - Loading efficientnet teacher model...\n",
      "2025-05-09 01:47:35,389 [INFO] - Creating efficientnet model architecture...\n",
      "2025-05-09 01:47:35,733 [INFO] - Teacher model efficientnet loaded successfully\n",
      "2025-05-09 01:47:35,733 [INFO] - Loading inception teacher model...\n",
      "2025-05-09 01:47:35,733 [INFO] - Creating inception model architecture...\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "2025-05-09 01:47:36,546 [INFO] - Teacher model inception loaded successfully\n",
      "2025-05-09 01:47:36,547 [INFO] - Loading mobilenet teacher model...\n",
      "2025-05-09 01:47:36,547 [INFO] - Creating mobilenet model architecture...\n",
      "2025-05-09 01:47:36,867 [INFO] - Teacher model mobilenet loaded successfully\n",
      "2025-05-09 01:47:36,867 [INFO] - Loading resnet teacher model...\n",
      "2025-05-09 01:47:36,867 [INFO] - Creating resnet model architecture...\n",
      "2025-05-09 01:47:37,448 [INFO] - Teacher model resnet loaded successfully\n",
      "2025-05-09 01:47:37,449 [INFO] - Loading densenet teacher model...\n",
      "2025-05-09 01:47:37,449 [INFO] - Creating densenet model architecture...\n",
      "2025-05-09 01:47:38,032 [INFO] - Teacher model densenet loaded successfully\n",
      "2025-05-09 01:47:38,032 [INFO] - Loaded 6 teacher models for comparison\n",
      "2025-05-09 01:47:38,032 [INFO] - Evaluating student model...\n",
      "2025-05-09 01:47:38,040 [INFO] - Preparing test dataset for student model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:47:38,660 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:47:38,660 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:47:38,661 [INFO] - Running inference for student model...\n",
      "2025-05-09 01:47:38,825 [INFO] - GPU cache cleared: 594.58MB → 594.58MB (freed 0.00MB)\n",
      "Evaluating student: 100%|██████████| 1250/1250 [00:32<00:00, 38.92it/s]\n",
      "2025-05-09 01:48:10,945 [INFO] - Inference complete on 10000 samples for student\n",
      "2025-05-09 01:48:10,947 [INFO] - Analyzing student model performance...\n",
      "2025-05-09 01:48:10,948 [INFO] - [student] Test Accuracy: 96.32%\n",
      "2025-05-09 01:48:10,959 [INFO] - [student] F1 Score (macro): 96.32%\n",
      "2025-05-09 01:48:10,960 [INFO] - [student] Precision (macro): 96.39%\n",
      "2025-05-09 01:48:10,960 [INFO] - [student] Recall (macro): 96.32%\n",
      "2025-05-09 01:48:10,966 [INFO] - [student] Expected Calibration Error (ECE): 0.0256\n",
      "2025-05-09 01:48:10,967 [INFO] - [student] Maximum Calibration Error (MCE): 0.3979\n",
      "2025-05-09 01:48:10,968 [INFO] - [student] Average Calibration Error (ACE): 0.1525\n",
      "2025-05-09 01:48:10,969 [INFO] - [student] Root Mean Squared Cal. Error (RMSCE): 0.0566\n",
      "2025-05-09 01:48:11,815 [INFO] - \n",
      "[student] Classification Report:\n",
      "2025-05-09 01:48:11,816 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.950     0.989     0.969      1000\n",
      "  automobile      0.948     0.990     0.969      1000\n",
      "        bird      0.961     0.966     0.964      1000\n",
      "         cat      0.898     0.949     0.923      1000\n",
      "        deer      0.987     0.953     0.969      1000\n",
      "         dog      0.967     0.908     0.937      1000\n",
      "        frog      0.971     0.988     0.979      1000\n",
      "       horse      0.988     0.975     0.981      1000\n",
      "        ship      0.987     0.967     0.977      1000\n",
      "       truck      0.983     0.947     0.965      1000\n",
      "\n",
      "    accuracy                          0.963     10000\n",
      "   macro avg      0.964     0.963     0.963     10000\n",
      "weighted avg      0.964     0.963     0.963     10000\n",
      "\n",
      "2025-05-09 01:48:12,959 [INFO] - [student] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\student\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating prediction visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1363: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction visualizations saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation/prediction_examples.png\n",
      "[INFO] Generating GradCAM visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding class samples:   0%|          | 25/10000 [00:00<00:12, 817.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating GradCAM for class 'airplane'\n",
      "[INFO] Generating GradCAM for class 'automobile'\n",
      "[INFO] Generating GradCAM for class 'bird'\n",
      "[INFO] Generating GradCAM for class 'cat'\n",
      "[INFO] Generating GradCAM for class 'deer'\n",
      "[INFO] Generating GradCAM for class 'dog'\n",
      "[INFO] Generating GradCAM for class 'frog'\n",
      "[INFO] Generating GradCAM for class 'horse'\n",
      "[INFO] Generating GradCAM for class 'ship'\n",
      "[INFO] Generating GradCAM for class 'truck'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1564: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  fig.subplots_adjust(right=0.95, top=0.92, bottom=0.05, wspace=0.3, hspace=0.4)\n",
      "2025-05-09 01:48:23,337 [INFO] - Evaluating teacher model: vit...\n",
      "2025-05-09 01:48:23,337 [INFO] - Preparing test dataset for vit model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GradCAM visualizations saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation/gradcam_visualization.png\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:48:23,868 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:48:23,868 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:48:23,868 [INFO] - Running inference for vit model...\n",
      "2025-05-09 01:48:24,073 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating vit: 100%|██████████| 1250/1250 [01:05<00:00, 19.03it/s]\n",
      "2025-05-09 01:49:29,755 [INFO] - Inference complete on 10000 samples for vit\n",
      "2025-05-09 01:49:29,756 [INFO] - Analyzing vit model performance...\n",
      "2025-05-09 01:49:29,758 [INFO] - [vit] Test Accuracy: 92.47%\n",
      "2025-05-09 01:49:29,767 [INFO] - [vit] F1 Score (macro): 92.45%\n",
      "2025-05-09 01:49:29,768 [INFO] - [vit] Precision (macro): 92.51%\n",
      "2025-05-09 01:49:29,768 [INFO] - [vit] Recall (macro): 92.47%\n",
      "2025-05-09 01:49:29,773 [INFO] - [vit] Expected Calibration Error (ECE): 0.0243\n",
      "2025-05-09 01:49:29,774 [INFO] - [vit] Maximum Calibration Error (MCE): 0.1025\n",
      "2025-05-09 01:49:29,774 [INFO] - [vit] Average Calibration Error (ACE): 0.0662\n",
      "2025-05-09 01:49:29,775 [INFO] - [vit] Root Mean Squared Cal. Error (RMSCE): 0.0367\n",
      "2025-05-09 01:49:30,592 [INFO] - \n",
      "[vit] Classification Report:\n",
      "2025-05-09 01:49:30,592 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.930     0.945     0.938      1000\n",
      "  automobile      0.918     0.979     0.947      1000\n",
      "        bird      0.920     0.907     0.913      1000\n",
      "         cat      0.860     0.832     0.846      1000\n",
      "        deer      0.940     0.918     0.929      1000\n",
      "         dog      0.870     0.882     0.876      1000\n",
      "        frog      0.923     0.977     0.949      1000\n",
      "       horse      0.947     0.955     0.951      1000\n",
      "        ship      0.971     0.944     0.957      1000\n",
      "       truck      0.973     0.908     0.939      1000\n",
      "\n",
      "    accuracy                          0.925     10000\n",
      "   macro avg      0.925     0.925     0.925     10000\n",
      "weighted avg      0.925     0.925     0.925     10000\n",
      "\n",
      "2025-05-09 01:49:31,728 [INFO] - [vit] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\vit\n",
      "2025-05-09 01:49:31,937 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "2025-05-09 01:49:31,938 [INFO] - Evaluating teacher model: efficientnet...\n",
      "2025-05-09 01:49:31,939 [INFO] - Preparing test dataset for efficientnet model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:49:32,479 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:49:32,479 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:49:32,481 [INFO] - Running inference for efficientnet model...\n",
      "2025-05-09 01:49:32,687 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating efficientnet: 100%|██████████| 1250/1250 [00:31<00:00, 39.62it/s]\n",
      "2025-05-09 01:50:04,240 [INFO] - Inference complete on 10000 samples for efficientnet\n",
      "2025-05-09 01:50:04,242 [INFO] - Analyzing efficientnet model performance...\n",
      "2025-05-09 01:50:04,243 [INFO] - [efficientnet] Test Accuracy: 95.23%\n",
      "2025-05-09 01:50:04,251 [INFO] - [efficientnet] F1 Score (macro): 95.24%\n",
      "2025-05-09 01:50:04,251 [INFO] - [efficientnet] Precision (macro): 95.29%\n",
      "2025-05-09 01:50:04,252 [INFO] - [efficientnet] Recall (macro): 95.23%\n",
      "2025-05-09 01:50:04,256 [INFO] - [efficientnet] Expected Calibration Error (ECE): 0.0223\n",
      "2025-05-09 01:50:04,257 [INFO] - [efficientnet] Maximum Calibration Error (MCE): 0.2143\n",
      "2025-05-09 01:50:04,257 [INFO] - [efficientnet] Average Calibration Error (ACE): 0.1083\n",
      "2025-05-09 01:50:04,258 [INFO] - [efficientnet] Root Mean Squared Cal. Error (RMSCE): 0.0484\n",
      "2025-05-09 01:50:05,056 [INFO] - \n",
      "[efficientnet] Classification Report:\n",
      "2025-05-09 01:50:05,057 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.940     0.979     0.959      1000\n",
      "  automobile      0.964     0.977     0.970      1000\n",
      "        bird      0.961     0.927     0.944      1000\n",
      "         cat      0.876     0.927     0.901      1000\n",
      "        deer      0.975     0.945     0.960      1000\n",
      "         dog      0.933     0.903     0.918      1000\n",
      "        frog      0.963     0.981     0.972      1000\n",
      "       horse      0.971     0.976     0.974      1000\n",
      "        ship      0.977     0.946     0.961      1000\n",
      "       truck      0.970     0.962     0.966      1000\n",
      "\n",
      "    accuracy                          0.952     10000\n",
      "   macro avg      0.953     0.952     0.952     10000\n",
      "weighted avg      0.953     0.952     0.952     10000\n",
      "\n",
      "2025-05-09 01:50:06,177 [INFO] - [efficientnet] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\efficientnet\n",
      "2025-05-09 01:50:06,395 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "2025-05-09 01:50:06,396 [INFO] - Evaluating teacher model: inception...\n",
      "2025-05-09 01:50:06,396 [INFO] - Preparing test dataset for inception model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:50:06,962 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:50:06,962 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:50:06,964 [INFO] - Running inference for inception model...\n",
      "2025-05-09 01:50:07,181 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating inception: 100%|██████████| 1250/1250 [00:41<00:00, 29.80it/s]\n",
      "2025-05-09 01:50:49,128 [INFO] - Inference complete on 10000 samples for inception\n",
      "2025-05-09 01:50:49,129 [INFO] - Analyzing inception model performance...\n",
      "2025-05-09 01:50:49,132 [INFO] - [inception] Test Accuracy: 10.00%\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-05-09 01:50:49,144 [INFO] - [inception] F1 Score (macro): 5.66%\n",
      "2025-05-09 01:50:49,144 [INFO] - [inception] Precision (macro): 4.84%\n",
      "2025-05-09 01:50:49,145 [INFO] - [inception] Recall (macro): 10.00%\n",
      "2025-05-09 01:50:49,150 [INFO] - [inception] Expected Calibration Error (ECE): 0.0420\n",
      "2025-05-09 01:50:49,151 [INFO] - [inception] Maximum Calibration Error (MCE): 0.2130\n",
      "2025-05-09 01:50:49,151 [INFO] - [inception] Average Calibration Error (ACE): 0.0931\n",
      "2025-05-09 01:50:49,152 [INFO] - [inception] Root Mean Squared Cal. Error (RMSCE): 0.0537\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025-05-09 01:50:50,005 [INFO] - \n",
      "[inception] Classification Report:\n",
      "2025-05-09 01:50:50,005 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.017     0.007     0.010      1000\n",
      "  automobile      0.000     0.000     0.000      1000\n",
      "        bird      0.092     0.540     0.158      1000\n",
      "         cat      0.000     0.000     0.000      1000\n",
      "        deer      0.000     0.000     0.000      1000\n",
      "         dog      0.145     0.244     0.182      1000\n",
      "        frog      0.003     0.001     0.001      1000\n",
      "       horse      0.194     0.191     0.193      1000\n",
      "        ship      0.000     0.000     0.000      1000\n",
      "       truck      0.033     0.017     0.022      1000\n",
      "\n",
      "    accuracy                          0.100     10000\n",
      "   macro avg      0.048     0.100     0.057     10000\n",
      "weighted avg      0.048     0.100     0.057     10000\n",
      "\n",
      "2025-05-09 01:50:51,164 [INFO] - [inception] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\inception\n",
      "2025-05-09 01:50:51,392 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "2025-05-09 01:50:51,393 [INFO] - Evaluating teacher model: mobilenet...\n",
      "2025-05-09 01:50:51,394 [INFO] - Preparing test dataset for mobilenet model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:50:51,973 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:50:51,975 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:50:51,977 [INFO] - Running inference for mobilenet model...\n",
      "2025-05-09 01:50:52,207 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating mobilenet: 100%|██████████| 1250/1250 [00:25<00:00, 48.97it/s]\n",
      "2025-05-09 01:51:17,736 [INFO] - Inference complete on 10000 samples for mobilenet\n",
      "2025-05-09 01:51:17,737 [INFO] - Analyzing mobilenet model performance...\n",
      "2025-05-09 01:51:17,738 [INFO] - [mobilenet] Test Accuracy: 95.60%\n",
      "2025-05-09 01:51:17,747 [INFO] - [mobilenet] F1 Score (macro): 95.58%\n",
      "2025-05-09 01:51:17,748 [INFO] - [mobilenet] Precision (macro): 95.59%\n",
      "2025-05-09 01:51:17,748 [INFO] - [mobilenet] Recall (macro): 95.60%\n",
      "2025-05-09 01:51:17,752 [INFO] - [mobilenet] Expected Calibration Error (ECE): 0.0238\n",
      "2025-05-09 01:51:17,754 [INFO] - [mobilenet] Maximum Calibration Error (MCE): 0.2961\n",
      "2025-05-09 01:51:17,754 [INFO] - [mobilenet] Average Calibration Error (ACE): 0.1419\n",
      "2025-05-09 01:51:17,755 [INFO] - [mobilenet] Root Mean Squared Cal. Error (RMSCE): 0.0559\n",
      "2025-05-09 01:51:18,563 [INFO] - \n",
      "[mobilenet] Classification Report:\n",
      "2025-05-09 01:51:18,564 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.958     0.967     0.963      1000\n",
      "  automobile      0.972     0.984     0.978      1000\n",
      "        bird      0.946     0.955     0.950      1000\n",
      "         cat      0.933     0.878     0.905      1000\n",
      "        deer      0.953     0.961     0.957      1000\n",
      "         dog      0.926     0.911     0.918      1000\n",
      "        frog      0.947     0.985     0.966      1000\n",
      "       horse      0.965     0.974     0.970      1000\n",
      "        ship      0.985     0.975     0.980      1000\n",
      "       truck      0.973     0.970     0.971      1000\n",
      "\n",
      "    accuracy                          0.956     10000\n",
      "   macro avg      0.956     0.956     0.956     10000\n",
      "weighted avg      0.956     0.956     0.956     10000\n",
      "\n",
      "2025-05-09 01:51:19,681 [INFO] - [mobilenet] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\mobilenet\n",
      "2025-05-09 01:51:19,927 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "2025-05-09 01:51:19,928 [INFO] - Evaluating teacher model: resnet...\n",
      "2025-05-09 01:51:19,928 [INFO] - Preparing test dataset for resnet model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:51:20,508 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:51:20,509 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:51:20,511 [INFO] - Running inference for resnet model...\n",
      "2025-05-09 01:51:20,746 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating resnet: 100%|██████████| 1250/1250 [00:25<00:00, 49.44it/s]\n",
      "2025-05-09 01:51:46,038 [INFO] - Inference complete on 10000 samples for resnet\n",
      "2025-05-09 01:51:46,039 [INFO] - Analyzing resnet model performance...\n",
      "2025-05-09 01:51:46,039 [INFO] - [resnet] Test Accuracy: 95.35%\n",
      "2025-05-09 01:51:46,046 [INFO] - [resnet] F1 Score (macro): 95.34%\n",
      "2025-05-09 01:51:46,051 [INFO] - [resnet] Precision (macro): 95.35%\n",
      "2025-05-09 01:51:46,051 [INFO] - [resnet] Recall (macro): 95.35%\n",
      "2025-05-09 01:51:46,055 [INFO] - [resnet] Expected Calibration Error (ECE): 0.0232\n",
      "2025-05-09 01:51:46,055 [INFO] - [resnet] Maximum Calibration Error (MCE): 0.3157\n",
      "2025-05-09 01:51:46,055 [INFO] - [resnet] Average Calibration Error (ACE): 0.1271\n",
      "2025-05-09 01:51:46,058 [INFO] - [resnet] Root Mean Squared Cal. Error (RMSCE): 0.0497\n",
      "2025-05-09 01:51:46,899 [INFO] - \n",
      "[resnet] Classification Report:\n",
      "2025-05-09 01:51:46,899 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.946     0.968     0.957      1000\n",
      "  automobile      0.974     0.974     0.974      1000\n",
      "        bird      0.920     0.963     0.941      1000\n",
      "         cat      0.909     0.883     0.896      1000\n",
      "        deer      0.958     0.953     0.955      1000\n",
      "         dog      0.940     0.910     0.925      1000\n",
      "        frog      0.967     0.979     0.973      1000\n",
      "       horse      0.972     0.972     0.972      1000\n",
      "        ship      0.979     0.970     0.974      1000\n",
      "       truck      0.970     0.963     0.966      1000\n",
      "\n",
      "    accuracy                          0.954     10000\n",
      "   macro avg      0.954     0.954     0.953     10000\n",
      "weighted avg      0.954     0.954     0.953     10000\n",
      "\n",
      "2025-05-09 01:51:48,101 [INFO] - [resnet] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\resnet\n",
      "2025-05-09 01:51:48,351 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "2025-05-09 01:51:48,351 [INFO] - Evaluating teacher model: densenet...\n",
      "2025-05-09 01:51:48,351 [INFO] - Preparing test dataset for densenet model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:51:48,932 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:51:48,933 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:51:48,935 [INFO] - Running inference for densenet model...\n",
      "2025-05-09 01:51:49,185 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating densenet: 100%|██████████| 1250/1250 [00:45<00:00, 27.58it/s]\n",
      "2025-05-09 01:52:34,510 [INFO] - Inference complete on 10000 samples for densenet\n",
      "2025-05-09 01:52:34,510 [INFO] - Analyzing densenet model performance...\n",
      "2025-05-09 01:52:34,510 [INFO] - [densenet] Test Accuracy: 96.76%\n",
      "2025-05-09 01:52:34,523 [INFO] - [densenet] F1 Score (macro): 96.76%\n",
      "2025-05-09 01:52:34,523 [INFO] - [densenet] Precision (macro): 96.78%\n",
      "2025-05-09 01:52:34,523 [INFO] - [densenet] Recall (macro): 96.76%\n",
      "2025-05-09 01:52:34,529 [INFO] - [densenet] Expected Calibration Error (ECE): 0.0189\n",
      "2025-05-09 01:52:34,530 [INFO] - [densenet] Maximum Calibration Error (MCE): 0.3801\n",
      "2025-05-09 01:52:34,530 [INFO] - [densenet] Average Calibration Error (ACE): 0.1733\n",
      "2025-05-09 01:52:34,531 [INFO] - [densenet] Root Mean Squared Cal. Error (RMSCE): 0.0549\n",
      "2025-05-09 01:52:35,306 [INFO] - \n",
      "[densenet] Classification Report:\n",
      "2025-05-09 01:52:35,306 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.977     0.970     0.973      1000\n",
      "  automobile      0.972     0.989     0.980      1000\n",
      "        bird      0.974     0.955     0.965      1000\n",
      "         cat      0.937     0.928     0.933      1000\n",
      "        deer      0.940     0.987     0.963      1000\n",
      "         dog      0.935     0.940     0.938      1000\n",
      "        frog      0.988     0.986     0.987      1000\n",
      "       horse      0.993     0.969     0.981      1000\n",
      "        ship      0.981     0.983     0.982      1000\n",
      "       truck      0.981     0.969     0.975      1000\n",
      "\n",
      "    accuracy                          0.968     10000\n",
      "   macro avg      0.968     0.968     0.968     10000\n",
      "weighted avg      0.968     0.968     0.968     10000\n",
      "\n",
      "2025-05-09 01:52:36,394 [INFO] - [densenet] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\densenet\n",
      "2025-05-09 01:52:36,646 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "2025-05-09 01:52:36,646 [INFO] - Evaluating teacher ensemble...\n",
      "2025-05-09 01:52:36,646 [INFO] - Preparing test dataset for student model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:52:37,205 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:52:37,205 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:52:37,205 [INFO] - Running inference for teacher ensemble...\n",
      "2025-05-09 01:52:37,452 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating teacher ensemble: 100%|██████████| 1250/1250 [02:42<00:00,  7.69it/s]\n",
      "2025-05-09 01:55:20,052 [INFO] - Ensemble inference complete on 10000 samples\n",
      "2025-05-09 01:55:20,055 [INFO] - Analyzing ensemble model performance...\n",
      "2025-05-09 01:55:20,056 [INFO] - [ensemble] Test Accuracy: 97.43%\n",
      "2025-05-09 01:55:20,066 [INFO] - [ensemble] F1 Score (macro): 97.43%\n",
      "2025-05-09 01:55:20,067 [INFO] - [ensemble] Precision (macro): 97.43%\n",
      "2025-05-09 01:55:20,067 [INFO] - [ensemble] Recall (macro): 97.43%\n",
      "2025-05-09 01:55:20,071 [INFO] - [ensemble] Expected Calibration Error (ECE): 0.3016\n",
      "2025-05-09 01:55:20,071 [INFO] - [ensemble] Maximum Calibration Error (MCE): 0.4681\n",
      "2025-05-09 01:55:20,071 [INFO] - [ensemble] Average Calibration Error (ACE): 0.3274\n",
      "2025-05-09 01:55:20,073 [INFO] - [ensemble] Root Mean Squared Cal. Error (RMSCE): 0.3162\n",
      "2025-05-09 01:55:20,961 [INFO] - \n",
      "[ensemble] Classification Report:\n",
      "2025-05-09 01:55:20,962 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.967     0.987     0.977      1000\n",
      "  automobile      0.972     0.988     0.980      1000\n",
      "        bird      0.982     0.968     0.975      1000\n",
      "         cat      0.949     0.941     0.945      1000\n",
      "        deer      0.976     0.978     0.977      1000\n",
      "         dog      0.955     0.950     0.952      1000\n",
      "        frog      0.979     0.996     0.988      1000\n",
      "       horse      0.989     0.984     0.986      1000\n",
      "        ship      0.989     0.977     0.983      1000\n",
      "       truck      0.986     0.974     0.980      1000\n",
      "\n",
      "    accuracy                          0.974     10000\n",
      "   macro avg      0.974     0.974     0.974     10000\n",
      "weighted avg      0.974     0.974     0.974     10000\n",
      "\n",
      "2025-05-09 01:55:22,240 [INFO] - [ensemble] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\\ensemble\n",
      "2025-05-09 01:55:22,241 [INFO] - Generating model comparisons...\n",
      "2025-05-09 01:55:22,242 [INFO] - Generating model comparison visualizations...\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:981: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1001: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1054: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1093: UserWarning: linestyle is redundantly defined by the 'linestyle' keyword argument and the fmt string \"o-\" (-> linestyle='-'). The keyword argument will take precedence.\n",
      "  ax.plot(angles, values, 'o-', linewidth=linewidth, linestyle=linestyle,\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1108: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "2025-05-09 01:55:25,547 [INFO] - Model comparison visualizations saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\n",
      "2025-05-09 01:55:25,549 [INFO] - Generating combined calibration curve comparison...\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1194: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "2025-05-09 01:55:26,592 [INFO] - Calibration curve comparison saved successfully\n",
      "2025-05-09 01:55:26,593 [INFO] - Generating teacher contribution visualization...\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1235: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1253: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "2025-05-09 01:55:27,357 [INFO] - Teacher contribution visualization saved successfully\n",
      "2025-05-09 01:55:27,358 [INFO] - Analyzing ensemble knowledge distribution...\n",
      "2025-05-09 01:55:27,358 [INFO] - Preparing test dataset for student model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:55:27,941 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:55:27,942 [INFO] - Getting predictions for student...\n",
      "2025-05-09 01:55:28,517 [INFO] - Getting predictions for vit...\n",
      "2025-05-09 01:55:29,167 [INFO] - Getting predictions for efficientnet...\n",
      "2025-05-09 01:55:29,486 [INFO] - Getting predictions for inception...\n",
      "2025-05-09 01:55:29,940 [INFO] - Getting predictions for mobilenet...\n",
      "2025-05-09 01:55:30,237 [INFO] - Getting predictions for resnet...\n",
      "2025-05-09 01:55:30,486 [INFO] - Getting predictions for densenet...\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1836: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(bottom=0.2)\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1868: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(bottom=0.2)\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1903: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(bottom=0.2)\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:1932: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(bottom=0.15, left=0.15)\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:2009: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(hspace=0.4, wspace=0.2, top=0.95, bottom=0.05)  # Adjust spacing without tight_layout\n",
      "2025-05-09 01:55:35,285 [INFO] - Generating t-SNE visualization of model decision making patterns...\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:2093: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(bottom=0.1, left=0.1, right=0.9, top=0.9)\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:2147: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  plt.subplots_adjust(bottom=0.2)\n",
      "2025-05-09 01:55:37,021 [INFO] - Ensemble knowledge analysis completed and saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\n",
      "2025-05-09 01:55:37,025 [INFO] - Analyzing ensemble calibration characteristics...\n",
      "2025-05-09 01:55:37,025 [INFO] - Preparing test dataset for student model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 01:55:37,589 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-09 01:55:37,589 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-09 01:55:37,591 [INFO] - Computing calibration for student...\n",
      "2025-05-09 01:55:37,591 [INFO] - Running inference for student model...\n",
      "2025-05-09 01:55:37,923 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating student: 100%|██████████| 1250/1250 [00:31<00:00, 39.23it/s]\n",
      "2025-05-09 01:56:09,793 [INFO] - Inference complete on 10000 samples for student\n",
      "2025-05-09 01:56:09,797 [INFO] - Computing calibration for vit...\n",
      "2025-05-09 01:56:09,797 [INFO] - Running inference for vit model...\n",
      "2025-05-09 01:56:10,107 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating vit: 100%|██████████| 1250/1250 [01:07<00:00, 18.44it/s]\n",
      "2025-05-09 01:57:17,911 [INFO] - Inference complete on 10000 samples for vit\n",
      "2025-05-09 01:57:17,917 [INFO] - Computing calibration for efficientnet...\n",
      "2025-05-09 01:57:17,918 [INFO] - Running inference for efficientnet model...\n",
      "2025-05-09 01:57:18,242 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating efficientnet: 100%|██████████| 1250/1250 [00:32<00:00, 38.18it/s]\n",
      "2025-05-09 01:57:50,987 [INFO] - Inference complete on 10000 samples for efficientnet\n",
      "2025-05-09 01:57:50,992 [INFO] - Computing calibration for inception...\n",
      "2025-05-09 01:57:50,993 [INFO] - Running inference for inception model...\n",
      "2025-05-09 01:57:51,304 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating inception: 100%|██████████| 1250/1250 [00:35<00:00, 34.72it/s]\n",
      "2025-05-09 01:58:27,309 [INFO] - Inference complete on 10000 samples for inception\n",
      "2025-05-09 01:58:27,314 [INFO] - Computing calibration for mobilenet...\n",
      "2025-05-09 01:58:27,315 [INFO] - Running inference for mobilenet model...\n",
      "2025-05-09 01:58:27,628 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating mobilenet: 100%|██████████| 1250/1250 [00:24<00:00, 50.58it/s]\n",
      "2025-05-09 01:58:52,344 [INFO] - Inference complete on 10000 samples for mobilenet\n",
      "2025-05-09 01:58:52,351 [INFO] - Computing calibration for resnet...\n",
      "2025-05-09 01:58:52,352 [INFO] - Running inference for resnet model...\n",
      "2025-05-09 01:58:52,665 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating resnet: 100%|██████████| 1250/1250 [00:24<00:00, 51.65it/s]\n",
      "2025-05-09 01:59:16,869 [INFO] - Inference complete on 10000 samples for resnet\n",
      "2025-05-09 01:59:16,874 [INFO] - Computing calibration for densenet...\n",
      "2025-05-09 01:59:16,875 [INFO] - Running inference for densenet model...\n",
      "2025-05-09 01:59:17,182 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating densenet: 100%|██████████| 1250/1250 [00:46<00:00, 27.15it/s]\n",
      "2025-05-09 02:00:03,234 [INFO] - Inference complete on 10000 samples for densenet\n",
      "2025-05-09 02:00:03,241 [INFO] - Computing calibration for teacher ensemble...\n",
      "2025-05-09 02:00:03,242 [INFO] - Running inference for teacher ensemble...\n",
      "2025-05-09 02:00:03,555 [INFO] - GPU cache cleared: 627.16MB → 627.16MB (freed 0.00MB)\n",
      "Evaluating teacher ensemble: 100%|██████████| 1250/1250 [02:53<00:00,  7.19it/s]\n",
      "2025-05-09 02:02:57,504 [INFO] - Ensemble inference complete on 10000 samples\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:2289: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:2328: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:2366: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_29368\\4215796343.py:2398: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "2025-05-09 02:03:00,051 [INFO] - Ensemble calibration analysis completed and saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\n",
      "2025-05-09 02:03:00,054 [INFO] - ==================================================\n",
      "2025-05-09 02:03:00,054 [INFO] - Ensemble distillation evaluation completed successfully!\n",
      "2025-05-09 02:03:00,055 [INFO] - All results saved to 'C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation' directory\n",
      "2025-05-09 02:03:00,055 [INFO] - ==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Evaluation Complete! Results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\evaluation\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import logging\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "# Utility: Recursively convert numpy types to native Python types for JSON serialization\n",
    "def to_serializable(obj):\n",
    "    import numpy as np\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(to_serializable(v) for v in obj)\n",
    "    elif hasattr(obj, 'tolist'):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.generic,)):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Import specific model classes for proper model loading\n",
    "from torchvision.models import (\n",
    "    vit_b_16, \n",
    "    efficientnet_b0, \n",
    "    inception_v3, \n",
    "    mobilenet_v3_large, \n",
    "    resnet50, \n",
    "    densenet121\n",
    ")\n",
    "\n",
    "# Set the style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set environment variables for better performance\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Optimize CPU threading\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # Limit memory fragmentation\n",
    "\n",
    "# Setup logging\n",
    "log_file = os.path.join(r\"C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\logs\", \"ensemble_distillation_test.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "####################################\n",
    "# 1. Configuration Class\n",
    "####################################\n",
    "class EnsembleDistillationEvalConfig:\n",
    "    def __init__(self):\n",
    "        # Base paths\n",
    "        self.base_path = r\"C:\\Users\\Gading\\Downloads\\Research\"\n",
    "        \n",
    "        # Dataset path\n",
    "        self.dataset_path = os.path.join(self.base_path, \"Dataset\", \"CIFAR-10\")\n",
    "        \n",
    "        # Model paths - distilled student model and all teacher models\n",
    "        self.models_base_path = os.path.join(self.base_path, \"Models\")\n",
    "        self.student_model_path = os.path.join(self.models_base_path, \"EnsembleDistillation\", \"exports\", \"cal_aware_distilled_model.pth\")\n",
    "        \n",
    "        # Teacher model paths (for comparison)\n",
    "        self.teacher_model_paths = {\n",
    "            'vit': os.path.join(self.models_base_path, \"ViT\", \"checkpoints\", \"vit_b16_teacher_20250507_234740_best.pth\"),\n",
    "            'efficientnet': os.path.join(self.models_base_path, \"EfficientNetB0\", \"checkpoints\", \"efficientnet_b0_teacher_20250508_103413_best.pth\"),\n",
    "            'inception': os.path.join(self.models_base_path, \"InceptionV3\", \"checkpoints\", \"inception_v3_teacher_20250508_072838_best.pth\"),\n",
    "            'mobilenet': os.path.join(self.models_base_path, \"MobileNetV3\", \"checkpoints\", \"mobilenetv3_20250508_053015_best.pth\"),\n",
    "            'resnet': os.path.join(self.models_base_path, \"ResNet50\", \"checkpoints\", \"resnet50_teacher_20250508_022222_best.pth\"),\n",
    "            'densenet': os.path.join(self.models_base_path, \"DenseNet121\", \"checkpoints\", \"densenet121_teacher_20250508_114100_best.pth\")\n",
    "        }\n",
    "        \n",
    "        # Output directory for evaluation results\n",
    "        self.output_dir = os.path.join(self.base_path, \"Results\", \"EnsembleDistillation\", \"evaluation\")\n",
    "        \n",
    "        # Hardware settings - optimized for stability\n",
    "        self.batch_size = 8  # Reduced for stability\n",
    "        self.num_workers = 0  # Start with 0 workers to avoid hanging\n",
    "        self.use_amp = True   # Use mixed precision for faster evaluation\n",
    "        self.pin_memory = True\n",
    "        \n",
    "        # Evaluation options\n",
    "        self.compare_with_teachers = True  # Compare student with teachers\n",
    "        self.evaluate_teacher_ensemble = True  # Evaluate the ensemble of teachers\n",
    "        self.n_bins_calibration = 15  # Number of bins for calibration metrics\n",
    "        \n",
    "        # CIFAR-10 classes\n",
    "        self.classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "        \n",
    "        # ImageNet normalization (used by pretrained models)\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        # Model-specific input sizes\n",
    "        self.model_input_sizes = {\n",
    "            'vit': 224,\n",
    "            'efficientnet': 224,\n",
    "            'inception': 299,  # InceptionV3 requires 299x299 input\n",
    "            'mobilenet': 224,\n",
    "            'resnet': 224,\n",
    "            'densenet': 224,\n",
    "            'student': 224\n",
    "        }\n",
    "        \n",
    "        # Plots configuration\n",
    "        self.plot_dpi = 300\n",
    "        self.plot_format = 'png'  # Use 'pdf' for publication-quality\n",
    "        self.ieee_style = True  # Use IEEE conference/journal style guidelines\n",
    "        \n",
    "        # Ensemble distillation specific metrics\n",
    "        self.calibration_metrics = ['ece', 'mce', 'ace', 'rmsce']  # Expected, Maximum, Average, Root Mean Square Calibration Errors\n",
    "        self.knowledge_transfer_analysis = True  # Analyze how knowledge was transferred\n",
    "        self.soft_target_temp = 4.0  # Temperature used in distillation (for visualization)\n",
    "        \n",
    "        # Teacher weights (if available in checkpoint, will be overwritten)\n",
    "        self.teacher_weights = {\n",
    "            'vit': 1.0,\n",
    "            'efficientnet': 1.0,\n",
    "            'inception': 0.5,\n",
    "            'mobilenet': 1.0,\n",
    "            'resnet': 1.0,\n",
    "            'densenet': 1.0\n",
    "        }\n",
    "\n",
    "    def get_input_size(self, model_name):\n",
    "        \"\"\"Get model-specific input size\"\"\"\n",
    "        if model_name in self.model_input_sizes:\n",
    "            return self.model_input_sizes[model_name]\n",
    "        return 224  # Default size\n",
    "\n",
    "\n",
    "####################################\n",
    "# 2. Utilities\n",
    "####################################\n",
    "def setup_environment():\n",
    "    \"\"\"Setup environment and output directory\"\"\"\n",
    "    # Create output directory\n",
    "    config = EnsembleDistillationEvalConfig()\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Show GPU info if available\n",
    "    if device.type == 'cuda':\n",
    "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    return config, device\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache to free up memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        before_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()  # Explicit garbage collection\n",
    "        after_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        logger.info(f\"GPU cache cleared: {before_mem:.2f}MB → {after_mem:.2f}MB (freed {before_mem-after_mem:.2f}MB)\")\n",
    "\n",
    "def set_ieee_style():\n",
    "    \"\"\"Set matplotlib styling for IEEE paper quality figures\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Times', 'Times New Roman', 'DejaVu Serif'],\n",
    "        'font.size': 10,\n",
    "        'axes.titlesize': 11,\n",
    "        'axes.labelsize': 10,\n",
    "        'xtick.labelsize': 9,\n",
    "        'ytick.labelsize': 9,\n",
    "        'legend.fontsize': 9,\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 300,\n",
    "        'savefig.bbox': 'tight',\n",
    "        'savefig.pad_inches': 0.05,\n",
    "        'figure.figsize': (8, 6),\n",
    "        'figure.constrained_layout.use': True,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'lines.markersize': 5,\n",
    "        'lines.linewidth': 1.5,\n",
    "    })\n",
    "\n",
    "\n",
    "####################################\n",
    "# 3. Dataset and DataLoader\n",
    "####################################\n",
    "def get_transform(config, model_name):\n",
    "    \"\"\"Get model-specific transforms for CIFAR-10 test dataset\"\"\"\n",
    "    input_size = config.get_input_size(model_name)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=config.mean, std=config.std),\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def get_test_dataset(config, model_name='student'):\n",
    "    \"\"\"Create a CIFAR-10 test dataset with model-specific transformations\"\"\"\n",
    "    logger.info(f\"Preparing test dataset for {model_name} model...\")\n",
    "    \n",
    "    transform = get_transform(config, model_name)\n",
    "    \n",
    "    # Load the dataset\n",
    "    try:\n",
    "        test_dataset = datasets.CIFAR10(\n",
    "            root=config.dataset_path,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        )\n",
    "        logger.info(f\"Test dataset loaded with {len(test_dataset)} samples\")\n",
    "        return test_dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_original_images(config, indices):\n",
    "    \"\"\"Get original 32x32 images for display purposes\"\"\"\n",
    "    # Load dataset without transformations\n",
    "    orig_dataset = datasets.CIFAR10(\n",
    "        root=config.dataset_path,\n",
    "        train=False,\n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    originals = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        img, label = orig_dataset.data[idx], orig_dataset.targets[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        originals.append(img_tensor)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return originals, labels\n",
    "\n",
    "def create_data_loader(dataset, config):\n",
    "    \"\"\"Create a DataLoader with optimized settings\"\"\"\n",
    "    logger.info(f\"Creating DataLoader with batch size {config.batch_size}...\")\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory,\n",
    "        persistent_workers=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "####################################\n",
    "# 4. Model Loading\n",
    "####################################\n",
    "class InceptionV3Wrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for InceptionV3 that safely handles auxiliary outputs for small inputs.\n",
    "    This prevents the \"Kernel size can't be greater than actual input size\" error.\n",
    "    \"\"\"\n",
    "    def __init__(self, inception_model):\n",
    "        super(InceptionV3Wrapper, self).__init__()\n",
    "        self.inception = inception_model\n",
    "        # Directly access the internal model components we need\n",
    "        self.Conv2d_1a_3x3 = inception_model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = inception_model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = inception_model.Conv2d_2b_3x3\n",
    "        self.maxpool1 = inception_model.maxpool1\n",
    "        self.Conv2d_3b_1x1 = inception_model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = inception_model.Conv2d_4a_3x3\n",
    "        self.maxpool2 = inception_model.maxpool2\n",
    "        self.Mixed_5b = inception_model.Mixed_5b\n",
    "        self.Mixed_5c = inception_model.Mixed_5c\n",
    "        self.Mixed_5d = inception_model.Mixed_5d\n",
    "        self.Mixed_6a = inception_model.Mixed_6a\n",
    "        self.Mixed_6b = inception_model.Mixed_6b\n",
    "        self.Mixed_6c = inception_model.Mixed_6c\n",
    "        self.Mixed_6d = inception_model.Mixed_6d\n",
    "        self.Mixed_6e = inception_model.Mixed_6e\n",
    "        self.Mixed_7a = inception_model.Mixed_7a\n",
    "        self.Mixed_7b = inception_model.Mixed_7b\n",
    "        self.Mixed_7c = inception_model.Mixed_7c\n",
    "        self.avgpool = inception_model.avgpool\n",
    "        self.dropout = inception_model.dropout\n",
    "        self.fc = inception_model.fc\n",
    "        \n",
    "        # Important: mark that this is a wrapper\n",
    "        self.is_wrapper = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get the batch size for reshaping later\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Basic stem\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.Mixed_5b(x)\n",
    "        x = self.Mixed_5c(x)\n",
    "        x = self.Mixed_5d(x)\n",
    "        x = self.Mixed_6a(x)\n",
    "        x = self.Mixed_6b(x)\n",
    "        x = self.Mixed_6c(x)\n",
    "        x = self.Mixed_6d(x)\n",
    "        x = self.Mixed_6e(x)\n",
    "        \n",
    "        # No auxiliary classifier usage - skip those layers that cause issues\n",
    "        \n",
    "        x = self.Mixed_7a(x)\n",
    "        x = self.Mixed_7b(x)\n",
    "        x = self.Mixed_7c(x)\n",
    "        \n",
    "        # Final pooling and prediction\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def load_state_dict(self, state_dict, strict=False):\n",
    "        \"\"\"Custom state_dict loading to handle the wrapper structure\"\"\"\n",
    "        # Filter out AuxLogits keys since we're not using them\n",
    "        state_dict = {k: v for k, v in state_dict.items() if not k.startswith('AuxLogits.')}\n",
    "        \n",
    "        # Load the state dict directly to the inception model\n",
    "        self.inception.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        # Now update our direct references\n",
    "        self.Conv2d_1a_3x3 = self.inception.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = self.inception.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = self.inception.Conv2d_2b_3x3\n",
    "        self.maxpool1 = self.inception.maxpool1\n",
    "        self.Conv2d_3b_1x1 = self.inception.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = self.inception.Conv2d_4a_3x3\n",
    "        self.maxpool2 = self.inception.maxpool2\n",
    "        self.Mixed_5b = self.inception.Mixed_5b\n",
    "        self.Mixed_5c = self.inception.Mixed_5c\n",
    "        self.Mixed_5d = self.inception.Mixed_5d\n",
    "        self.Mixed_6a = self.inception.Mixed_6a\n",
    "        self.Mixed_6b = self.inception.Mixed_6b\n",
    "        self.Mixed_6c = self.inception.Mixed_6c\n",
    "        self.Mixed_6d = self.inception.Mixed_6d\n",
    "        self.Mixed_6e = self.inception.Mixed_6e\n",
    "        self.Mixed_7a = self.inception.Mixed_7a\n",
    "        self.Mixed_7b = self.inception.Mixed_7b\n",
    "        self.Mixed_7c = self.inception.Mixed_7c\n",
    "        self.avgpool = self.inception.avgpool\n",
    "        self.dropout = self.inception.dropout\n",
    "        self.fc = self.inception.fc\n",
    "        \n",
    "        return self\n",
    "\n",
    "def create_model_architecture(model_name, num_classes=10):\n",
    "    \"\"\"Create a model architecture based on the model name\"\"\"\n",
    "    logger.info(f\"Creating {model_name} model architecture...\")\n",
    "    \n",
    "    if model_name == 'vit':\n",
    "        model = vit_b_16(weights=None)\n",
    "        if hasattr(model, 'heads'):\n",
    "            input_dim = model.heads.head.in_features\n",
    "            model.heads.head = torch.nn.Linear(input_dim, num_classes)\n",
    "        else:\n",
    "            input_dim = model.head.in_features\n",
    "            model.head = torch.nn.Linear(input_dim, num_classes)\n",
    "            \n",
    "    elif model_name == 'efficientnet' or model_name == 'student':\n",
    "        model = efficientnet_b0(weights=None)\n",
    "        if hasattr(model, 'classifier'):\n",
    "            in_features = model.classifier[1].in_features\n",
    "            model.classifier[1] = torch.nn.Linear(in_features, num_classes)\n",
    "            \n",
    "    elif model_name == 'inception':\n",
    "        base_inception = inception_v3(weights=None)\n",
    "        base_inception.fc = torch.nn.Linear(base_inception.fc.in_features, num_classes)\n",
    "        model = InceptionV3Wrapper(base_inception)\n",
    "        \n",
    "    elif model_name == 'mobilenet':\n",
    "        model = mobilenet_v3_large(weights=None)\n",
    "        model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'resnet':\n",
    "        model = resnet50(weights=None)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'densenet':\n",
    "        model = densenet121(weights=None)\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_student_model(config, device):\n",
    "    \"\"\"Load the ensemble distilled student model from checkpoint\"\"\"\n",
    "    logger.info(f\"Loading student model from: {config.student_model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Create model architecture\n",
    "        model = create_model_architecture('student')\n",
    "        \n",
    "        # Add numpy.core.multiarray.scalar to safe globals for loading\n",
    "        from torch.serialization import add_safe_globals\n",
    "        import numpy.core.multiarray\n",
    "        add_safe_globals([numpy.core.multiarray.scalar])\n",
    "        \n",
    "        # Load checkpoint with weights_only=False for compatibility\n",
    "        checkpoint = torch.load(\n",
    "            config.student_model_path, \n",
    "            map_location=device,\n",
    "            weights_only=False  # Set to False to avoid UnpicklingError\n",
    "        )\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            logger.info(f\"Student model state loaded from 'model_state_dict'\")\n",
    "            \n",
    "            # Load teacher weights if available\n",
    "            if 'teacher_weights' in checkpoint:\n",
    "                config.teacher_weights = checkpoint['teacher_weights']\n",
    "                logger.info(f\"Teacher weights loaded from checkpoint: {config.teacher_weights}\")\n",
    "            \n",
    "            # Load teacher temperatures if available\n",
    "            if 'teacher_temperatures' in checkpoint:\n",
    "                config.teacher_temperatures = checkpoint['teacher_temperatures']\n",
    "                logger.info(f\"Teacher temperatures loaded from checkpoint: {config.teacher_temperatures}\")\n",
    "                \n",
    "            # Print additional metadata if available\n",
    "            if 'test_metrics' in checkpoint:\n",
    "                logger.info(f\"Previous test metrics found in checkpoint:\")\n",
    "                for k, v in checkpoint['test_metrics'].items():\n",
    "                    logger.info(f\"  - {k}: {v}\")\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            logger.info(f\"Student model state loaded directly from checkpoint\")\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        logger.info(f\"Student model loaded successfully and set to evaluation mode\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load student model: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def load_teacher_models(config, device):\n",
    "    \"\"\"Load all teacher models for comparison and ensemble evaluation\"\"\"\n",
    "    if not config.compare_with_teachers:\n",
    "        logger.info(\"Skipping teacher model loading as comparison is disabled\")\n",
    "        return {}\n",
    "    \n",
    "    logger.info(\"Loading teacher models for comparison...\")\n",
    "    teachers = {}\n",
    "    \n",
    "    for name, path in config.teacher_model_paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            logger.warning(f\"Teacher model path not found: {path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            logger.info(f\"Loading {name} teacher model...\")\n",
    "            model = create_model_architecture(name)\n",
    "            \n",
    "            # Load checkpoint with weights_only=True to avoid FutureWarning\n",
    "            checkpoint = torch.load(path, map_location=device, weights_only=True)\n",
    "            \n",
    "            # Handle different checkpoint formats\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "                \n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            logger.info(f\"Teacher model {name} loaded successfully\")\n",
    "            \n",
    "            teachers[name] = model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load teacher model {name}: {str(e)}\")\n",
    "            \n",
    "    logger.info(f\"Loaded {len(teachers)} teacher models for comparison\")\n",
    "    return teachers\n",
    "\n",
    "def create_teacher_ensemble(teachers, weights=None):\n",
    "    \"\"\"Create an ensemble prediction function using the teacher models and weights\"\"\"\n",
    "    # Default to equal weights if not provided\n",
    "    if weights is None:\n",
    "        weights = {name: 1.0/len(teachers) for name in teachers.keys()}\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    weight_sum = sum(weights.values())\n",
    "    normalized_weights = {k: v/weight_sum for k, v in weights.items()}\n",
    "    \n",
    "    def ensemble_predict(x, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Make a weighted ensemble prediction\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            temperature: Temperature for softening probabilities\n",
    "            \n",
    "        Returns:\n",
    "            Weighted average of teacher predictions\n",
    "        \"\"\"\n",
    "        all_logits = []\n",
    "        active_weights = []\n",
    "        active_names = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for name, model in teachers.items():\n",
    "                if name in normalized_weights:\n",
    "                    weight = normalized_weights[name]\n",
    "                    if weight > 0:\n",
    "                        # Get model outputs\n",
    "                        outputs = model(x)\n",
    "                        \n",
    "                        # Handle inception output format\n",
    "                        if name == 'inception' and isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0]\n",
    "                            \n",
    "                        # Apply temperature scaling\n",
    "                        scaled_logits = outputs / temperature\n",
    "                        \n",
    "                        # Store logits and weight\n",
    "                        all_logits.append(scaled_logits)\n",
    "                        active_weights.append(weight)\n",
    "                        active_names.append(name)\n",
    "        \n",
    "        if not all_logits:\n",
    "            return None\n",
    "            \n",
    "        # Convert weights to tensor and normalize\n",
    "        weights_tensor = torch.tensor(active_weights, device=x.device)\n",
    "        weights_tensor = weights_tensor / weights_tensor.sum()\n",
    "        \n",
    "        # Apply softmax to each model's logits\n",
    "        all_probs = [F.softmax(logits, dim=1) for logits in all_logits]\n",
    "        \n",
    "        # Weighted sum of probabilities\n",
    "        weighted_probs = torch.zeros_like(all_probs[0])\n",
    "        for i, probs in enumerate(all_probs):\n",
    "            weighted_probs += probs * weights_tensor[i]\n",
    "            \n",
    "        # Convert back to logits (optional, depends on usage)\n",
    "        weighted_logits = torch.log(weighted_probs + 1e-8)\n",
    "        \n",
    "        return weighted_logits, weighted_probs, active_names\n",
    "        \n",
    "    return ensemble_predict\n",
    "\n",
    "\n",
    "####################################\n",
    "# 5. Inference\n",
    "####################################\n",
    "def run_inference(model, loader, config, device, model_name=\"student\"):\n",
    "    \"\"\"Run inference on the test set\"\"\"\n",
    "    logger.info(f\"Running inference for {model_name} model...\")\n",
    "    \n",
    "    # Store predictions, targets and probabilities\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    clear_gpu_cache()\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=f\"Evaluating {model_name}\"):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Use mixed precision if available and enabled\n",
    "            if config.use_amp and device.type == 'cuda':\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    # Handle inception output format\n",
    "                    if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                # Handle inception output format\n",
    "                if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(probs, dim=1)\n",
    "            \n",
    "            # Store results (on CPU to save GPU memory)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "            # Free memory\n",
    "            del images, outputs, probs, preds\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    \n",
    "    logger.info(f\"Inference complete on {len(all_targets)} samples for {model_name}\")\n",
    "    return np.array(all_targets), np.array(all_preds), all_probs\n",
    "\n",
    "def run_ensemble_inference(ensemble_predict, loader, config, device):\n",
    "    \"\"\"Run inference with the teacher ensemble\"\"\"\n",
    "    logger.info(f\"Running inference for teacher ensemble...\")\n",
    "    \n",
    "    # Store predictions, targets and probabilities\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    teacher_outputs = []\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    clear_gpu_cache()\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=\"Evaluating teacher ensemble\"):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Use mixed precision if available and enabled\n",
    "            if config.use_amp and device.type == 'cuda':\n",
    "                with autocast(device_type='cuda'):\n",
    "                    # Get ensemble prediction\n",
    "                    _, ensemble_probs, _ = ensemble_predict(images, temperature=config.soft_target_temp)\n",
    "            else:\n",
    "                # Get ensemble prediction\n",
    "                _, ensemble_probs, _ = ensemble_predict(images, temperature=config.soft_target_temp)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(ensemble_probs, dim=1)\n",
    "            \n",
    "            # Store results (on CPU to save GPU memory)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.append(ensemble_probs.cpu().numpy())\n",
    "            \n",
    "            # Free memory\n",
    "            del images, ensemble_probs, preds\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    \n",
    "    logger.info(f\"Ensemble inference complete on {len(all_targets)} samples\")\n",
    "    return np.array(all_targets), np.array(all_preds), all_probs\n",
    "\n",
    "\n",
    "####################################\n",
    "# 6. Evaluation Metrics\n",
    "####################################\n",
    "def compute_ece(probs, targets, n_bins=15):\n",
    "    \"\"\"Compute Expected Calibration Error (ECE)\"\"\"\n",
    "    # Convert targets to numpy array if it's not already\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.numpy()\n",
    "    \n",
    "    # Get the predicted class and its confidence\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = (predictions == targets).astype(np.float32)\n",
    "    \n",
    "    # Sort by confidence\n",
    "    sorted_indices = np.argsort(confidences)\n",
    "    sorted_confidences = confidences[sorted_indices]\n",
    "    sorted_accuracies = accuracies[sorted_indices]\n",
    "    \n",
    "    # Create bins\n",
    "    bin_size = 1.0 / n_bins\n",
    "    bins = np.linspace(0, 1.0, n_bins+1)\n",
    "    ece = 0.0\n",
    "    \n",
    "    bin_confidences = []\n",
    "    bin_accuracies = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        # Determine bin boundaries\n",
    "        bin_start = bins[i]\n",
    "        bin_end = bins[i+1]\n",
    "        \n",
    "        # Find samples in bin\n",
    "        in_bin = (sorted_confidences >= bin_start) & (sorted_confidences < bin_end)\n",
    "        bin_count = np.sum(in_bin)\n",
    "        bin_counts.append(bin_count)\n",
    "        \n",
    "        if bin_count > 0:\n",
    "            bin_conf = np.mean(sorted_confidences[in_bin])\n",
    "            bin_acc = np.mean(sorted_accuracies[in_bin])\n",
    "            bin_confidences.append(bin_conf)\n",
    "            bin_accuracies.append(bin_acc)\n",
    "            # Add weighted absolute difference to ECE\n",
    "            ece += (bin_count / len(confidences)) * np.abs(bin_acc - bin_conf)\n",
    "        else:\n",
    "            bin_confidences.append((bin_start + bin_end) / 2)\n",
    "            bin_accuracies.append(0)\n",
    "    \n",
    "    return ece, bin_confidences, bin_accuracies, bin_counts\n",
    "\n",
    "def compute_extended_calibration_metrics(probs, targets, n_bins=15):\n",
    "    \"\"\"\n",
    "    Compute comprehensive calibration metrics for ensemble distillation evaluation:\n",
    "    - ECE: Expected Calibration Error (weighted average of |accuracy-confidence|)\n",
    "    - MCE: Maximum Calibration Error (maximum deviation between accuracy and confidence)\n",
    "    - ACE: Average Calibration Error (simple average of |accuracy-confidence|)\n",
    "    - RMSCE: Root Mean Squared Calibration Error (L2 norm of calibration errors)\n",
    "    \"\"\"\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.numpy()\n",
    "    \n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = (predictions == targets).astype(np.float32)\n",
    "    \n",
    "    # Create bins\n",
    "    bins = np.linspace(0, 1.0, n_bins+1)\n",
    "    bin_errors = []\n",
    "    bin_weights = []\n",
    "    \n",
    "    # Calculate per-bin metrics\n",
    "    for i in range(n_bins):\n",
    "        bin_start = bins[i]\n",
    "        bin_end = bins[i+1]\n",
    "        \n",
    "        in_bin = (confidences >= bin_start) & (confidences < bin_end)\n",
    "        bin_count = np.sum(in_bin)\n",
    "        \n",
    "        if bin_count > 0:\n",
    "            bin_conf = np.mean(confidences[in_bin])\n",
    "            bin_acc = np.mean(accuracies[in_bin])\n",
    "            bin_error = np.abs(bin_acc - bin_conf)\n",
    "            \n",
    "            bin_errors.append(bin_error)\n",
    "            bin_weights.append(bin_count / len(confidences))\n",
    "        else:\n",
    "            bin_errors.append(0.0)\n",
    "            bin_weights.append(0.0)\n",
    "    \n",
    "    # Calculate ECE (Expected Calibration Error)\n",
    "    ece = np.sum(np.array(bin_errors) * np.array(bin_weights))\n",
    "    \n",
    "    # Calculate MCE (Maximum Calibration Error)\n",
    "    mce = np.max(bin_errors) if bin_errors else 0.0\n",
    "    \n",
    "    # Calculate ACE (Average Calibration Error)\n",
    "    non_empty_bins = [i for i, w in enumerate(bin_weights) if w > 0]\n",
    "    ace = np.mean([bin_errors[i] for i in non_empty_bins]) if non_empty_bins else 0.0\n",
    "    \n",
    "    # Calculate RMSCE (Root Mean Squared Calibration Error)\n",
    "    rmsce = np.sqrt(np.sum(np.array(bin_weights) * np.array(bin_errors) ** 2))\n",
    "    \n",
    "    return {\n",
    "        'ece': ece,\n",
    "        'mce': mce,\n",
    "        'ace': ace,\n",
    "        'rmsce': rmsce,\n",
    "        'bin_errors': bin_errors,\n",
    "        'bin_weights': bin_weights\n",
    "    }\n",
    "\n",
    "def analyze_results(y_true, y_pred, y_probs, class_names, config, model_name=\"student\"):\n",
    "    \"\"\"Generate and save evaluation metrics for a single model\"\"\"\n",
    "    logger.info(f\"Analyzing {model_name} model performance...\")\n",
    "    \n",
    "    # Create output directory for this model\n",
    "    model_output_dir = os.path.join(config.output_dir, model_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Calculate and print accuracy\n",
    "    accuracy = np.mean(y_true == y_pred) * 100\n",
    "    logger.info(f\"[{model_name}] Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # 2. Calculate F1 score, precision, and recall\n",
    "    f1 = f1_score(y_true, y_pred, average='macro') * 100\n",
    "    precision = precision_score(y_true, y_pred, average='macro') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='macro') * 100\n",
    "    logger.info(f\"[{model_name}] F1 Score (macro): {f1:.2f}%\")\n",
    "    logger.info(f\"[{model_name}] Precision (macro): {precision:.2f}%\")\n",
    "    logger.info(f\"[{model_name}] Recall (macro): {recall:.2f}%\")\n",
    "    \n",
    "    # 3. Calculate Extended Calibration Metrics\n",
    "    cal_metrics = compute_extended_calibration_metrics(y_probs, y_true, n_bins=config.n_bins_calibration)\n",
    "    logger.info(f\"[{model_name}] Expected Calibration Error (ECE): {cal_metrics['ece']:.4f}\")\n",
    "    logger.info(f\"[{model_name}] Maximum Calibration Error (MCE): {cal_metrics['mce']:.4f}\")\n",
    "    logger.info(f\"[{model_name}] Average Calibration Error (ACE): {cal_metrics['ace']:.4f}\")\n",
    "    logger.info(f\"[{model_name}] Root Mean Squared Cal. Error (RMSCE): {cal_metrics['rmsce']:.4f}\")\n",
    "    \n",
    "    # 4. Generate confusion matrix - Set IEEE style for plots\n",
    "    if config.ieee_style:\n",
    "        set_ieee_style()\n",
    "        \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix - {model_name} (Accuracy: {accuracy:.2f}%)\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.savefig(f\"{model_output_dir}/confusion_matrix.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Generate classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, digits=3)\n",
    "    logger.info(f\"\\n[{model_name}] Classification Report:\")\n",
    "    logger.info(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open(f\"{model_output_dir}/classification_report.txt\", \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Test Accuracy: {accuracy:.2f}%\\n\")\n",
    "        f.write(f\"F1 Score (macro): {f1:.2f}%\\n\")\n",
    "        f.write(f\"Precision (macro): {precision:.2f}%\\n\")\n",
    "        f.write(f\"Recall (macro): {recall:.2f}%\\n\")\n",
    "        f.write(f\"Expected Calibration Error: {cal_metrics['ece']:.4f}\\n\")\n",
    "        f.write(f\"Maximum Calibration Error: {cal_metrics['mce']:.4f}\\n\")\n",
    "        f.write(f\"Average Calibration Error: {cal_metrics['ace']:.4f}\\n\")\n",
    "        f.write(f\"Root Mean Squared Cal. Error: {cal_metrics['rmsce']:.4f}\\n\\n\")\n",
    "        f.write(report)\n",
    "    \n",
    "    # 6. Per-class accuracy\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_acc = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=list(class_names), y=class_acc)\n",
    "    \n",
    "    # Add value labels on top of bars for IEEE paper quality\n",
    "    for i, v in enumerate(class_acc):\n",
    "        ax.text(i, v + 1, f\"{v:.1f}%\", ha='center', fontsize=9)\n",
    "        \n",
    "    plt.title(f\"{model_name}: Per-Class Accuracy\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.ylim(0, 105)  # Add space for labels\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(f\"{model_output_dir}/per_class_accuracy.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Plot calibration reliability diagram\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Get calibration data\n",
    "    ece, bin_confs, bin_accs, bin_counts = compute_ece(y_probs, y_true, n_bins=config.n_bins_calibration)\n",
    "    \n",
    "    # Plot bins with their accuracies\n",
    "    bin_edges = np.linspace(0, 1, config.n_bins_calibration + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    bin_counts_norm = np.array(bin_counts) / sum(bin_counts)\n",
    "    \n",
    "    plt.bar(bin_centers, bin_accs, width=1/config.n_bins_calibration, alpha=0.3, label='Accuracy in bin')\n",
    "    \n",
    "    # Create a twin axis plot with appropriate manual layout\n",
    "    twin_ax = plt.twinx()\n",
    "    twin_ax.bar(bin_centers, bin_counts_norm, width=1/config.n_bins_calibration, alpha=0.2, color='g', label='Proportion of samples')\n",
    "    twin_ax.set_ylabel('Proportion of Samples')\n",
    "    \n",
    "    # Connect actual calibration points\n",
    "    plt.plot(bin_confs, bin_accs, 'ro-', label=f'Actual Calibration (ECE={ece:.4f})')\n",
    "    \n",
    "    plt.title(f'{model_name} - Calibration Reliability Diagram')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(f\"{model_output_dir}/calibration_curve.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Save all metrics as a dictionary\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': float(accuracy),  # Convert to Python float\n",
    "        'f1_score': float(f1),        # Convert to Python float\n",
    "        'precision': float(precision), # Convert to Python float\n",
    "        'recall': float(recall),       # Convert to Python float\n",
    "        'ece': float(cal_metrics['ece']),  # Convert to Python float\n",
    "        'mce': float(cal_metrics['mce']),  # Convert to Python float\n",
    "        'ace': float(cal_metrics['ace']),  # Convert to Python float\n",
    "        'rmsce': float(cal_metrics['rmsce']),  # Convert to Python float\n",
    "        'per_class_accuracy': [float(acc) for acc in class_acc.tolist()]  # Convert all values to Python float\n",
    "    }\n",
    "    \n",
    "    # Save metrics as JSON\n",
    "    with open(f\"{model_output_dir}/metrics.json\", \"w\") as f:\n",
    "        json.dump(to_serializable(metrics), f, indent=4)\n",
    "    \n",
    "    logger.info(f\"[{model_name}] Evaluation results saved to {model_output_dir}\")\n",
    "    return metrics\n",
    "\n",
    "def compare_models(all_metrics, config):\n",
    "    \"\"\"Create comparison visualizations between student and teachers\"\"\"\n",
    "    logger.info(\"Generating model comparison visualizations...\")\n",
    "    \n",
    "    if len(all_metrics) <= 1:\n",
    "        logger.info(\"Not enough models to compare.\")\n",
    "        return\n",
    "    \n",
    "    # Set IEEE style for plots\n",
    "    if config.ieee_style:\n",
    "        set_ieee_style()\n",
    "    \n",
    "    # Extract model names and metrics\n",
    "    model_names = [metrics['model_name'] for metrics in all_metrics]\n",
    "    accuracies = [metrics['accuracy'] for metrics in all_metrics]\n",
    "    f1_scores = [metrics['f1_score'] for metrics in all_metrics]\n",
    "    precisions = [metrics['precision'] for metrics in all_metrics]\n",
    "    recalls = [metrics['recall'] for metrics in all_metrics]\n",
    "    eces = [metrics['ece'] for metrics in all_metrics]\n",
    "    \n",
    "    # Advanced calibration metrics\n",
    "    mces = [metrics['mce'] if 'mce' in metrics else 0 for metrics in all_metrics]\n",
    "    aces = [metrics['ace'] if 'ace' in metrics else 0 for metrics in all_metrics]\n",
    "    rmsces = [metrics['rmsce'] if 'rmsce' in metrics else 0 for metrics in all_metrics]\n",
    "    \n",
    "    # Set colors - make student model stand out\n",
    "    colors = ['#d62728' if name == 'student' or name == 'ensemble' else '#1f77b4' for name in model_names]\n",
    "    student_color = '#d62728'  # Red\n",
    "    teacher_color = '#1f77b4'  # Blue\n",
    "    ensemble_color = '#2ca02c'  # Green\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    ax = plt.subplot(111)\n",
    "    bars = ax.bar(model_names, accuracies, color=colors)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f\"{height:.2f}%\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title('Accuracy Comparison: Distilled Student vs. Teachers')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, max(accuracies) + 5)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/accuracy_comparison.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. F1, Precision, Recall comparison\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    bars1 = ax.bar(x - width, f1_scores, width, label='F1 Score', alpha=0.7)\n",
    "    bars2 = ax.bar(x, precisions, width, label='Precision', alpha=0.7)\n",
    "    bars3 = ax.bar(x + width, recalls, width, label='Recall', alpha=0.7)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.set_ylabel('Score (%)')\n",
    "    ax.set_title('F1, Precision, and Recall Comparison')\n",
    "    ax.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/f1_precision_recall_comparison.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Calibration metrics comparison (lower is better)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create subplots for different calibration metrics\n",
    "    plt.subplot(2, 2, 1)\n",
    "    bars = plt.bar(model_names, eces, color=colors)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "                f\"{height:.4f}\", ha='center', va='bottom', fontsize=9)\n",
    "    plt.title('Expected Calibration Error (ECE)')\n",
    "    plt.ylabel('Error (lower is better)')\n",
    "    plt.ylim(0, max(eces) * 1.2)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    bars = plt.bar(model_names, mces, color=colors)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "                f\"{height:.4f}\", ha='center', va='bottom', fontsize=9)\n",
    "    plt.title('Maximum Calibration Error (MCE)')\n",
    "    plt.ylabel('Error (lower is better)')\n",
    "    plt.ylim(0, max(mces) * 1.2)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    bars = plt.bar(model_names, aces, color=colors)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "                f\"{height:.4f}\", ha='center', va='bottom', fontsize=9)\n",
    "    plt.title('Average Calibration Error (ACE)')\n",
    "    plt.ylabel('Error (lower is better)')\n",
    "    plt.ylim(0, max(aces) * 1.2)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    bars = plt.bar(model_names, rmsces, color=colors)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "                f\"{height:.4f}\", ha='center', va='bottom', fontsize=9)\n",
    "    plt.title('Root Mean Squared Calibration Error (RMSCE)')\n",
    "    plt.ylabel('Error (lower is better)')\n",
    "    plt.ylim(0, max(rmsces) * 1.2)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Calibration Metrics Comparison (Lower is Better)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f\"{config.output_dir}/calibration_metrics_comparison.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Combined radar chart for all metrics - normalized to 0-1 scale\n",
    "    # Prepare metrics for radar chart (normalize everything to 0-1 range)\n",
    "    metrics_names = ['Accuracy', 'F1 Score', 'Precision', 'Recall', \n",
    "                    'Calibration (1-ECE)', 'Calibration (1-MCE)']\n",
    "    \n",
    "    # Normalize accuracy metrics to 0-1\n",
    "    norm_accuracies = [acc/100 for acc in accuracies]\n",
    "    norm_f1s = [f1/100 for f1 in f1_scores]\n",
    "    norm_precisions = [prec/100 for prec in precisions]\n",
    "    norm_recalls = [rec/100 for rec in recalls]\n",
    "    \n",
    "    # Invert calibration metrics (so higher is better)\n",
    "    norm_eces = [1 - min(ece, 1.0) for ece in eces]  # Cap at 1.0 to avoid negative values\n",
    "    norm_mces = [1 - min(mce, 1.0) for mce in mces]\n",
    "    \n",
    "    # Create the radar chart\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Set up the radar chart parameters\n",
    "    angles = np.linspace(0, 2*np.pi, len(metrics_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    \n",
    "    # Plot each model's metrics\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        values = [norm_accuracies[i], norm_f1s[i], norm_precisions[i], \n",
    "                norm_recalls[i], norm_eces[i], norm_mces[i]]\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        color = student_color if model_name == 'student' else (ensemble_color if model_name == 'ensemble' else teacher_color)\n",
    "        linestyle = '-' if model_name == 'student' or model_name == 'ensemble' else '--'\n",
    "        linewidth = 2.5 if model_name == 'student' or model_name == 'ensemble' else 1.5\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=linewidth, linestyle=linestyle, \n",
    "               label=model_name, color=color)\n",
    "        ax.fill(angles, values, alpha=0.1, color=color)\n",
    "    \n",
    "    # Set chart properties\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add legend and title\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Performance Metrics Comparison\", size=15, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/radar_chart_comparison.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Calibration curve comparison (for the most important models)\n",
    "    # This will be done in a separate function for better focus on calibration visualization\n",
    "    \n",
    "    # Save comparison metrics as JSON\n",
    "    comparison = {\n",
    "        'models': model_names,\n",
    "        'accuracy': accuracies,\n",
    "        'f1_score': f1_scores,\n",
    "        'precision': precisions,\n",
    "        'recall': recalls,\n",
    "        'ece': eces,\n",
    "        'mce': mces, \n",
    "        'ace': aces,\n",
    "        'rmsce': rmsces\n",
    "    }\n",
    "    \n",
    "    with open(f\"{config.output_dir}/model_comparison.json\", \"w\") as f:\n",
    "        json.dump(to_serializable(comparison), f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Model comparison visualizations saved to {config.output_dir}\")\n",
    "    \n",
    "def plot_multiple_calibration_curves(all_probs, all_targets, model_names, config):\n",
    "    \"\"\"Plot calibration curves for multiple models in one figure\"\"\"\n",
    "    logger.info(\"Generating combined calibration curve comparison...\")\n",
    "    \n",
    "    if config.ieee_style:\n",
    "        set_ieee_style()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Define colors and styles\n",
    "    student_color = '#d62728'  # Red\n",
    "    teacher_color = '#1f77b4'  # Blue\n",
    "    ensemble_color = '#2ca02c'  # Green\n",
    "    \n",
    "    n_bins = config.n_bins_calibration\n",
    "    \n",
    "    # Plot calibration curve for each model\n",
    "    for i, (probs, targets, name) in enumerate(zip(all_probs, all_targets, model_names)):\n",
    "        # Calculate calibration data\n",
    "        ece, bin_confs, bin_accs, bin_counts = compute_ece(probs, targets, n_bins=n_bins)\n",
    "        \n",
    "        # Select color and style based on model type\n",
    "        if name == 'student':\n",
    "            color = student_color\n",
    "            linestyle = '-'\n",
    "            linewidth = 2.5\n",
    "            marker = 'o'\n",
    "            markersize = 6\n",
    "        elif name == 'ensemble':\n",
    "            color = ensemble_color\n",
    "            linestyle = '-'\n",
    "            linewidth = 2.5\n",
    "            marker = 's'  # square marker for ensemble\n",
    "            markersize = 6\n",
    "        else:\n",
    "            color = teacher_color\n",
    "            linestyle = '--'\n",
    "            linewidth = 1.5\n",
    "            marker = '.'\n",
    "            markersize = 5\n",
    "        \n",
    "        # Plot calibration points\n",
    "        plt.plot(bin_confs, bin_accs, marker=marker, linestyle=linestyle, \n",
    "                linewidth=linewidth, markersize=markersize,\n",
    "                label=f'{name} (ECE={ece:.4f})', color=color)\n",
    "    \n",
    "    # Add legend, labels, and grid\n",
    "    plt.legend(loc='lower right', fontsize=9)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Calibration Reliability Comparison', fontsize=14)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Add axes for the diagonal line\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/calibration_curves_comparison.png\", dpi=config.plot_dpi)\n",
    "    plt.savefig(f\"{config.output_dir}/calibration_curves_comparison.pdf\", format='pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(\"Calibration curve comparison saved successfully\")\n",
    "\n",
    "def visualize_teacher_contributions(config):\n",
    "    \"\"\"Visualize teacher contributions/weights in the ensemble distillation process\"\"\"\n",
    "    if not hasattr(config, 'teacher_weights') or not config.teacher_weights:\n",
    "        logger.warning(\"No teacher weights available for visualization\")\n",
    "        return\n",
    "        \n",
    "    logger.info(\"Generating teacher contribution visualization...\")\n",
    "    \n",
    "    if config.ieee_style:\n",
    "        set_ieee_style()\n",
    "    \n",
    "    # Extract teacher names and weights\n",
    "    teacher_names = list(config.teacher_weights.keys())\n",
    "    weights = list(config.teacher_weights.values())\n",
    "    \n",
    "    # Normalize weights to sum to 1 for better visualization\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [w/total_weight for w in weights]\n",
    "    \n",
    "    # 1. Bar chart of teacher weights\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(teacher_names, normalized_weights, color='#1f77b4')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f\"{height:.3f}\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title('Normalized Teacher Weights in Ensemble Distillation')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.ylim(0, max(normalized_weights) * 1.2)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/teacher_weights_bar.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Pie chart of teacher contributions\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        normalized_weights, \n",
    "        labels=teacher_names,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        wedgeprops={'edgecolor': 'w', 'linewidth': 1},\n",
    "        textprops={'fontsize': 10}\n",
    "    )\n",
    "    \n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    plt.axis('equal')\n",
    "    plt.title('Teacher Model Contributions in Ensemble Distillation')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/teacher_weights_pie.png\", dpi=config.plot_dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Heatmap of teacher weights with model metrics\n",
    "    # This would be more informative with more metadata, but we'll create a simple version\n",
    "    logger.info(\"Teacher contribution visualization saved successfully\")\n",
    "\n",
    "\n",
    "####################################\n",
    "# 7. Visualization Helpers\n",
    "####################################\n",
    "def visualize_predictions(model, test_dataset, config, device, num_examples=5):\n",
    "    \"\"\"Visualize random predictions with original CIFAR-10 images\"\"\"\n",
    "    print(\"[INFO] Generating prediction visualizations...\")\n",
    "    \n",
    "    # Use a professional style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "        'font.size': 9,\n",
    "        'axes.titlesize': 10,\n",
    "        'axes.labelsize': 9\n",
    "    })\n",
    "    \n",
    "    # Define colors for correct and incorrect predictions\n",
    "    correct_color = '#1f77b4'  # Professional blue\n",
    "    incorrect_color = '#d62728'  # Professional red\n",
    "    \n",
    "    # Select random indices\n",
    "    indices = np.random.choice(len(test_dataset), size=num_examples*len(config.classes), replace=False)\n",
    "    \n",
    "    # Get original images and labels\n",
    "    originals, true_labels = get_original_images(config, indices)\n",
    "    \n",
    "    # Prepare a batch of transformed images for the model\n",
    "    batch_images = torch.stack([test_dataset[idx][0] for idx in indices]).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if config.use_amp and device.type == 'cuda':\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(batch_images)\n",
    "        else:\n",
    "            outputs = model(batch_images)\n",
    "    \n",
    "    # Get prediction probabilities and classes\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    pred_scores, pred_labels = torch.max(probs, dim=1)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    pred_labels = pred_labels.cpu().numpy()\n",
    "    pred_scores = pred_scores.cpu().numpy()\n",
    "    \n",
    "    # Plot results - create a figure with better proportions\n",
    "    fig, axes = plt.subplots(len(config.classes), num_examples, figsize=(num_examples*2.5, len(config.classes)*2))\n",
    "    fig.suptitle(\"CIFAR-10 Prediction Examples (EfficientNetB0 Distilled)\", fontsize=14, y=0.98)\n",
    "    \n",
    "    # Group samples by true class\n",
    "    class_indices = {i: [] for i in range(len(config.classes))}\n",
    "    for i, label in enumerate(true_labels):\n",
    "        if len(class_indices[label]) < num_examples:\n",
    "            class_indices[label].append(i)\n",
    "    \n",
    "    for class_idx in range(len(config.classes)):\n",
    "        for example_idx in range(num_examples):\n",
    "            ax = axes[class_idx, example_idx]\n",
    "            \n",
    "            # Check if we have enough examples for this class\n",
    "            if example_idx < len(class_indices[class_idx]):\n",
    "                i = class_indices[class_idx][example_idx]\n",
    "                \n",
    "                # Plot image with a border\n",
    "                img = originals[i].permute(1, 2, 0).numpy()\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                # Add prediction info with better formatting\n",
    "                true_label = true_labels[i]\n",
    "                pred_label = pred_labels[i]\n",
    "                color = correct_color if true_label == pred_label else incorrect_color\n",
    "                \n",
    "                # Create a clean title with proper formatting\n",
    "                ax.set_title(f\"True: {config.classes[true_label]}\\nPred: {config.classes[pred_label]}\\nConf: {pred_scores[i]:.3f}\", \n",
    "                            color=color, fontsize=9, pad=3)\n",
    "                \n",
    "                # Add a professional border\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor(color)\n",
    "                    spine.set_linewidth(1.5)\n",
    "            else:\n",
    "                # If not enough examples, hide the empty subplot\n",
    "                ax.set_visible(False)\n",
    "            \n",
    "            # Remove ticks for all subplots (whether they have content or not)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    # Add row labels on the left\n",
    "    for class_idx in range(len(config.classes)):\n",
    "        if axes[class_idx, 0].get_visible():  # Only add label if the first subplot in row is visible\n",
    "            axes[class_idx, 0].set_ylabel(config.classes[class_idx], fontsize=10, \n",
    "                                        rotation=90, labelpad=10, va='center')\n",
    "    \n",
    "    # Add a footer with model information\n",
    "    plt.figtext(0.5, 0.01, \n",
    "               f\"EfficientNetB0 Distilled Model evaluation on CIFAR-10 test set\", \n",
    "               ha=\"center\", fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, bottom=0.05)\n",
    "    plt.savefig(f\"{config.output_dir}/prediction_examples.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"[INFO] Prediction visualizations saved to {config.output_dir}/prediction_examples.png\")\n",
    "\n",
    "\n",
    "####################################\n",
    "# 8. GradCAM Implementation\n",
    "####################################\n",
    "class GradCAM:\n",
    "    \"\"\"Gradient-weighted Class Activation Mapping\"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        \n",
    "        # Register hooks\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "    \n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        # Forward pass\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Get prediction if target class not specified\n",
    "        if target_class is None:\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_tensor)\n",
    "                target_class = output.argmax(dim=1)\n",
    "        \n",
    "        # Forward pass with gradients\n",
    "        output = self.model(input_tensor)\n",
    "        loss = output[:, target_class].sum()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "        \n",
    "        # Generate CAM\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # Upsample CAM to input size\n",
    "        cam = torch.nn.functional.interpolate(\n",
    "            cam, \n",
    "            size=input_tensor.shape[2:], \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Normalize CAM\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        return cam\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "def visualize_gradcam(model, test_dataset, config, device):\n",
    "    \"\"\"Create GradCAM visualizations for each class with improved scientific appearance\"\"\"\n",
    "    print(\"[INFO] Generating GradCAM visualizations...\")\n",
    "    \n",
    "    # Set scientific plotting style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "        'font.size': 10,\n",
    "        'axes.titlesize': 11,\n",
    "        'axes.labelsize': 10\n",
    "    })\n",
    "    \n",
    "    # Find one sample per class\n",
    "    samples_by_class = {c: None for c in range(len(config.classes))}\n",
    "    indices_by_class = {c: None for c in range(len(config.classes))}\n",
    "    \n",
    "    for idx in tqdm(range(len(test_dataset)), desc=\"Finding class samples\"):\n",
    "        _, label = test_dataset[idx]\n",
    "        if samples_by_class[label] is None:\n",
    "            samples_by_class[label] = test_dataset[idx][0].unsqueeze(0)\n",
    "            indices_by_class[label] = idx\n",
    "        if all(v is not None for v in samples_by_class.values()):\n",
    "            break\n",
    "    \n",
    "    # Initialize GradCAM with the appropriate layer for EfficientNetB0\n",
    "    # For EfficientNetB0, we target the last feature block\n",
    "    target_layer = model.features[-1]\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Use a scientific colormap\n",
    "    cmap = 'inferno'  # Scientific colormap that works well for heatmaps\n",
    "    \n",
    "    # Create a figure with increased top margin\n",
    "    fig = plt.figure(figsize=(15, 14))  # Increased height to add space for title\n",
    "    \n",
    "    # Create GridSpec with adjusted height ratios to leave room for the title\n",
    "    # Make the last column narrower for the colorbar (0.8 vs 1.0 for image columns)\n",
    "    gs = fig.add_gridspec(5, 6, height_ratios=[0.5, 1, 1, 1, 1], \n",
    "                         width_ratios=[1, 1, 1, 1, 1, 0.05])  # Narrower colorbar column\n",
    "    \n",
    "    # Add title - with improved readability and model detection\n",
    "    model_name = \"Ensemble Distilled\"\n",
    "    # Try to determine the model type from its class name\n",
    "    model_class_name = model.__class__.__name__.lower()\n",
    "    if 'efficient' in model_class_name:\n",
    "        model_name = \"EfficientNetB0 Distilled\"\n",
    "    elif 'mobile' in model_class_name:\n",
    "        model_name = \"MobileNetV3 Distilled\"\n",
    "    elif 'resnet' in model_class_name:\n",
    "        model_name = \"ResNet50 Distilled\"\n",
    "    elif 'inception' in model_class_name:\n",
    "        model_name = \"InceptionV3 Distilled\"\n",
    "    elif 'dense' in model_class_name:\n",
    "        model_name = \"DenseNet121 Distilled\"\n",
    "    elif 'vit' in model_class_name:\n",
    "        model_name = \"ViT Distilled\"\n",
    "    \n",
    "    # Set the title with improved styling and position\n",
    "    fig.suptitle(f\"GradCAM Visualizations for CIFAR-10 Classes\\n{model_name}\", \n",
    "                fontsize=16, fontweight='bold', y=0.95)  # Moved title up by setting y=0.95\n",
    "    \n",
    "    # Create a mapping for grid with proper organization - adjust for the spacing row\n",
    "    class_to_position = {\n",
    "        0: (1, 0),  # airplane - shifted down one row\n",
    "        1: (1, 1),  # automobile\n",
    "        2: (1, 2),  # bird\n",
    "        3: (1, 3),  # cat\n",
    "        4: (1, 4),  # deer\n",
    "        5: (3, 0),  # dog - shifted down one row\n",
    "        6: (3, 1),  # frog\n",
    "        7: (3, 2),  # horse\n",
    "        8: (3, 3),  # ship\n",
    "        9: (3, 4),  # truck\n",
    "    }\n",
    "    \n",
    "    # Variable to store the last heatmap for colorbar reference\n",
    "    last_heatmap = None\n",
    "    \n",
    "    for class_idx in range(len(config.classes)):\n",
    "        print(f\"[INFO] Generating GradCAM for class '{config.classes[class_idx]}'\")\n",
    "        \n",
    "        # Get the sample\n",
    "        input_tensor = samples_by_class[class_idx].to(device)\n",
    "        \n",
    "        # Generate CAM\n",
    "        cam = grad_cam.generate_cam(input_tensor, target_class=class_idx)\n",
    "        cam = cam.cpu().numpy()[0, 0]\n",
    "        \n",
    "        # Get original image\n",
    "        orig_imgs, _ = get_original_images(config, [indices_by_class[class_idx]])\n",
    "        orig_img = orig_imgs[0].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Upsample original image to match model input size (224x224)\n",
    "        img_upsampled = transforms.Resize(config.get_input_size('student'))(orig_imgs[0])\n",
    "        img_upsampled = img_upsampled.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Get row, col position\n",
    "        row, col = class_to_position[class_idx]\n",
    "        \n",
    "        # Plot original image\n",
    "        ax_orig = fig.add_subplot(gs[row, col])\n",
    "        ax_orig.imshow(img_upsampled)\n",
    "        ax_orig.set_title(f\"{config.classes[class_idx]} (Original)\", fontsize=11)\n",
    "        ax_orig.set_xticks([])\n",
    "        ax_orig.set_yticks([])\n",
    "        \n",
    "        # Plot heatmap overlay\n",
    "        ax_overlay = fig.add_subplot(gs[row+1, col])\n",
    "        ax_overlay.imshow(img_upsampled)\n",
    "        last_heatmap = ax_overlay.imshow(cam, cmap=cmap, alpha=0.6)\n",
    "        ax_overlay.set_title(f\"{config.classes[class_idx]} (GradCAM)\", fontsize=11)\n",
    "        ax_overlay.set_xticks([])\n",
    "        ax_overlay.set_yticks([])\n",
    "    \n",
    "    # Add a colorbar for the heatmap - use a specific position that won't conflict\n",
    "    # Make it thinner to match the reference image\n",
    "    cax = fig.add_subplot(gs[:, 5])  # Use the last column for colorbar\n",
    "    cbar = fig.colorbar(last_heatmap, cax=cax)\n",
    "    cbar.set_label('Activation Strength', fontsize=10)\n",
    "    \n",
    "    # Add a footer with model information\n",
    "    fig.text(0.5, 0.02, \n",
    "                \"GradCAM visualizations show regions the model focuses on when classifying each category\",\n",
    "                ha=\"center\", fontsize=10, style='italic')\n",
    "    \n",
    "    # Adjust spacing - don't use tight_layout here\n",
    "    fig.subplots_adjust(right=0.95, top=0.92, bottom=0.05, wspace=0.3, hspace=0.4)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(f\"{config.output_dir}/gradcam_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Clean up\n",
    "    grad_cam.remove_hooks()\n",
    "    \n",
    "    print(f\"[INFO] GradCAM visualizations saved to {config.output_dir}/gradcam_visualization.png\")\n",
    "\n",
    "\n",
    "####################################\n",
    "# 9. Main Evaluation Function\n",
    "####################################\n",
    "def main():\n",
    "    \"\"\"Main evaluation pipeline for ensemble distillation\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Ensemble Distillation Evaluation Pipeline\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Setup environment\n",
    "    config, device = setup_environment()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting ensemble distillation evaluation...\")\n",
    "        \n",
    "        # 2. Load the student model\n",
    "        student_model = load_student_model(config, device)\n",
    "        if student_model is None:\n",
    "            logger.error(\"Failed to load student model. Check the file path.\")\n",
    "            return 1\n",
    "        \n",
    "        # 3. Load all teacher models if comparison is requested\n",
    "        teacher_models = {}\n",
    "        if config.compare_with_teachers:\n",
    "            teacher_models = load_teacher_models(config, device)\n",
    "            \n",
    "            if not teacher_models:\n",
    "                logger.warning(\"No teacher models could be loaded for comparison. Continuing with student only.\")\n",
    "        \n",
    "        # Combine all models\n",
    "        all_models = {'student': student_model}\n",
    "        all_models.update(teacher_models)\n",
    "        \n",
    "        # 4. Create ensemble prediction function if needed\n",
    "        ensemble_predict = None\n",
    "        if config.evaluate_teacher_ensemble and len(teacher_models) > 0:\n",
    "            ensemble_predict = create_teacher_ensemble(teacher_models, config.teacher_weights)\n",
    "        \n",
    "        # Store all metrics for comparison\n",
    "        all_metrics = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        model_names = []\n",
    "        \n",
    "        # 5. Evaluate the student model\n",
    "        logger.info(\"Evaluating student model...\")\n",
    "        test_dataset = get_test_dataset(config, 'student')\n",
    "        test_loader = create_data_loader(test_dataset, config)\n",
    "        \n",
    "        targets, predictions, probabilities = run_inference(student_model, test_loader, config, device)\n",
    "        metrics = analyze_results(targets, predictions, probabilities, config.classes, config)\n",
    "        all_metrics.append(metrics)\n",
    "        all_targets.append(targets)\n",
    "        all_probs.append(probabilities)\n",
    "        model_names.append('student')\n",
    "        \n",
    "        # 6. Visualize predictions and GradCAM for student\n",
    "        visualize_predictions(student_model, test_dataset, config, device)\n",
    "        visualize_gradcam(student_model, test_dataset, config, device)\n",
    "        \n",
    "        # 7. Evaluate each teacher model if requested\n",
    "        if config.compare_with_teachers:\n",
    "            for name, model in teacher_models.items():\n",
    "                logger.info(f\"Evaluating teacher model: {name}...\")\n",
    "                \n",
    "                teacher_dataset = get_test_dataset(config, name)\n",
    "                teacher_loader = create_data_loader(teacher_dataset, config)\n",
    "                \n",
    "                targets, predictions, probabilities = run_inference(model, teacher_loader, config, device, name)\n",
    "                metrics = analyze_results(targets, predictions, probabilities, config.classes, config, name)\n",
    "                all_metrics.append(metrics)\n",
    "                all_targets.append(targets)\n",
    "                all_probs.append(probabilities)\n",
    "                model_names.append(name)\n",
    "                \n",
    "                # Clear GPU cache between models\n",
    "                clear_gpu_cache()\n",
    "        \n",
    "        # 8. Evaluate the teacher ensemble if requested\n",
    "        if config.evaluate_teacher_ensemble and ensemble_predict is not None:\n",
    "            logger.info(\"Evaluating teacher ensemble...\")\n",
    "            \n",
    "            ensemble_dataset = get_test_dataset(config, 'student')  # Use student's transform\n",
    "            ensemble_loader = create_data_loader(ensemble_dataset, config)\n",
    "            \n",
    "            targets, predictions, probabilities = run_ensemble_inference(\n",
    "                ensemble_predict, ensemble_loader, config, device\n",
    "            )\n",
    "            metrics = analyze_results(targets, predictions, probabilities, config.classes, config, \"ensemble\")\n",
    "            all_metrics.append(metrics)\n",
    "            all_targets.append(targets)\n",
    "            all_probs.append(probabilities)\n",
    "            model_names.append('ensemble')\n",
    "        \n",
    "        # 9. Compare models with visualizations\n",
    "        if len(all_metrics) > 1:\n",
    "            logger.info(\"Generating model comparisons...\")\n",
    "            compare_models(all_metrics, config)\n",
    "            \n",
    "            # 10. Plot combined calibration curves\n",
    "            plot_multiple_calibration_curves(all_probs, all_targets, model_names, config)\n",
    "        \n",
    "        # 11. Visualize teacher contributions in ensemble distillation\n",
    "        if hasattr(config, 'teacher_weights') and config.teacher_weights:\n",
    "            visualize_teacher_contributions(config)\n",
    "        \n",
    "        # 12. Analyze ensemble knowledge transfer\n",
    "        if len(all_models) > 1:\n",
    "            analyze_teacher_ensemble_knowledge(all_models, config, device)\n",
    "            analyze_ensemble_calibration(all_models, config, device)\n",
    "        \n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"Ensemble distillation evaluation completed successfully!\")\n",
    "        logger.info(f\"All results saved to '{config.output_dir}' directory\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Evaluation Complete! Results saved to {config.output_dir}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        logger.error(f\"An error occurred during evaluation: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        logger.error(\"Try adjusting the batch size or device settings if out of memory\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "####################################\n",
    "# 10. Ensemble Knowledge Analysis\n",
    "####################################\n",
    "def analyze_teacher_ensemble_knowledge(all_models, config, device):\n",
    "    \"\"\"\n",
    "    Analyze how the ensemble distillation knowledge is distributed and\n",
    "    visualize the knowledge overlap between teachers and student with enhanced visuals.\n",
    "    \"\"\"\n",
    "    logger.info(\"Analyzing ensemble knowledge distribution...\")\n",
    "    \n",
    "    # Set improved visualization styling\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Times', 'Times New Roman', 'DejaVu Serif'],\n",
    "        'font.size': 10,\n",
    "        'axes.titlesize': 12,\n",
    "        'axes.labelsize': 11,\n",
    "        'xtick.labelsize': 9,\n",
    "        'ytick.labelsize': 9,\n",
    "        'legend.fontsize': 9,\n",
    "        'figure.figsize': (10, 6),\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "    })\n",
    "    \n",
    "    # Create a test dataset for analysis\n",
    "    test_dataset = get_test_dataset(config, 'student')\n",
    "    \n",
    "    # Check if we have at least the student model and one teacher\n",
    "    if 'student' not in all_models or len(all_models) < 2:\n",
    "        logger.warning(\"Need at least student and one teacher for ensemble knowledge analysis\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples for analysis\n",
    "    num_samples = 100\n",
    "    sample_indices = np.random.choice(len(test_dataset), size=num_samples, replace=False)\n",
    "    \n",
    "    # Store model predictions\n",
    "    all_preds = {}\n",
    "    all_probs = {}\n",
    "    all_entropies = {}\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for model_name, model in all_models.items():\n",
    "        logger.info(f\"Getting predictions for {model_name}...\")\n",
    "        \n",
    "        preds = []\n",
    "        probs = []\n",
    "        entropies = []\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = config.batch_size\n",
    "        \n",
    "        for i in range(0, len(sample_indices), batch_size):\n",
    "            batch_indices = sample_indices[i:i+batch_size]\n",
    "            batch_inputs = torch.stack([test_dataset[idx][0] for idx in batch_indices]).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                    outputs = model(batch_inputs)\n",
    "                    \n",
    "                    # Handle inception output format\n",
    "                    if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            batch_probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, batch_preds = torch.max(batch_probs, dim=1)\n",
    "            \n",
    "            # Calculate entropy (uncertainty)\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "            batch_entropies = -(batch_probs * log_probs).sum(dim=1)\n",
    "            \n",
    "            # Store results\n",
    "            preds.extend(batch_preds.cpu().numpy())\n",
    "            probs.append(batch_probs.cpu().numpy())\n",
    "            entropies.extend(batch_entropies.cpu().numpy())\n",
    "        \n",
    "        # Convert lists to arrays\n",
    "        all_preds[model_name] = np.array(preds)\n",
    "        all_probs[model_name] = np.concatenate(probs, axis=0)\n",
    "        all_entropies[model_name] = np.array(entropies)\n",
    "    \n",
    "    # Calculate agreement ratios between student and each teacher\n",
    "    student_preds = all_preds['student']\n",
    "    agreement_ratios = {}\n",
    "    \n",
    "    for model_name, preds in all_preds.items():\n",
    "        if model_name != 'student':\n",
    "            agreement = np.mean(preds == student_preds) * 100\n",
    "            agreement_ratios[model_name] = agreement\n",
    "    \n",
    "    # Get the ground truth for these samples\n",
    "    ground_truth = np.array([test_dataset[idx][1] for idx in sample_indices])\n",
    "    \n",
    "    # Calculate agreement with ground truth\n",
    "    ground_truth_agreement = {}\n",
    "    for model_name, preds in all_preds.items():\n",
    "        ground_truth_agreement[model_name] = np.mean(preds == ground_truth) * 100\n",
    "    \n",
    "    # 1. Plot agreement between student and each teacher using Seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Sort teachers by agreement\n",
    "    sorted_teachers = sorted(agreement_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "    teacher_names = [t[0] for t in sorted_teachers]\n",
    "    agreement_values = [t[1] for t in sorted_teachers]\n",
    "    \n",
    "    # Create DataFrame for Seaborn\n",
    "    agreement_df = pd.DataFrame({\n",
    "        'Teacher': teacher_names,\n",
    "        'Agreement (%)': agreement_values\n",
    "    })\n",
    "    \n",
    "    # Use Seaborn's better color palette with explicit hue parameter\n",
    "    ax = sns.barplot(x='Teacher', y='Agreement (%)', data=agreement_df, \n",
    "                    hue='Teacher', palette=sns.color_palette(\"viridis\", len(teacher_names)), legend=False)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(agreement_values):\n",
    "        ax.text(i, v + 1, f\"{v:.2f}%\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.title('Agreement Ratio: Student vs. Teachers', fontsize=13, fontweight='bold')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Adjust layout safely\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.savefig(f\"{config.output_dir}/student_teacher_agreement.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Plot agreement with ground truth using Seaborn\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    model_names = list(ground_truth_agreement.keys())\n",
    "    accuracy_values = list(ground_truth_agreement.values())\n",
    "    \n",
    "    # Create DataFrame with a column for coloring\n",
    "    accuracy_df = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Accuracy (%)': accuracy_values,\n",
    "        'Type': ['Student' if name == 'student' else 'Teacher' for name in model_names]\n",
    "    })\n",
    "    \n",
    "    # Create bar plot with Seaborn using hue for coloring\n",
    "    ax = sns.barplot(x='Model', y='Accuracy (%)', hue='Type', data=accuracy_df,\n",
    "                    palette={'Student': '#d62728', 'Teacher': '#1f77b4'})\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(accuracy_values):\n",
    "        ax.text(i, v + 1, f\"{v:.2f}%\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.title('Accuracy on Sample Set', fontsize=13, fontweight='bold')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='')\n",
    "    \n",
    "    # Adjust layout safely\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.savefig(f\"{config.output_dir}/ground_truth_agreement.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Plot prediction uncertainty (entropy) distribution with Seaborn\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate average entropy for each model\n",
    "    avg_entropies = {model: np.mean(entropy) for model, entropy in all_entropies.items()}\n",
    "    \n",
    "    # Sort models by average entropy\n",
    "    sorted_models = sorted(avg_entropies.items(), key=lambda x: x[1])\n",
    "    model_names = [m[0] for m in sorted_models]\n",
    "    entropy_values = [m[1] for m in sorted_models]\n",
    "    \n",
    "    # Create DataFrame with a column for coloring\n",
    "    entropy_df = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Entropy': entropy_values,\n",
    "        'Type': ['Student' if name == 'student' else 'Teacher' for name in model_names]\n",
    "    })\n",
    "    \n",
    "    # Create bar plot with Seaborn using hue for coloring\n",
    "    ax = sns.barplot(x='Model', y='Entropy', hue='Type', data=entropy_df,\n",
    "                   palette={'Student': '#d62728', 'Teacher': '#1f77b4'})\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(entropy_values):\n",
    "        ax.text(i, v + 0.03, f\"{v:.2f}\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.title('Average Prediction Uncertainty (Entropy)', fontsize=13, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='')\n",
    "    \n",
    "    # Adjust layout safely\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.savefig(f\"{config.output_dir}/prediction_uncertainty.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Create a prediction overlap matrix with Seaborn\n",
    "    overlap_matrix = np.zeros((len(all_models), len(all_models)))\n",
    "    model_names = list(all_models.keys())\n",
    "    \n",
    "    for i, model1 in enumerate(model_names):\n",
    "        for j, model2 in enumerate(model_names):\n",
    "            if i <= j:  # Only compute upper triangle\n",
    "                overlap = np.mean(all_preds[model1] == all_preds[model2]) * 100\n",
    "                overlap_matrix[i, j] = overlap\n",
    "                overlap_matrix[j, i] = overlap  # Mirror\n",
    "    \n",
    "    # Create a DataFrame for better Seaborn integration\n",
    "    overlap_df = pd.DataFrame(overlap_matrix, index=model_names, columns=model_names)\n",
    "    \n",
    "    # Plot heatmap with improved Seaborn styling\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap with a colorbar that's properly handled\n",
    "    ax = sns.heatmap(overlap_df, annot=True, fmt='.1f', cmap='viridis',\n",
    "                     cbar_kws={'label': 'Prediction Agreement (%)'})\n",
    "    \n",
    "    # Add title with better styling\n",
    "    plt.title('Prediction Overlap Between Models (%)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Don't use tight_layout() with colorbar\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.15)\n",
    "    plt.savefig(f\"{config.output_dir}/prediction_overlap_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Analyze decision boundary areas - where models disagree\n",
    "    disagreement_indices = []\n",
    "    \n",
    "    # Find samples where there's significant disagreement\n",
    "    for i in range(len(sample_indices)):\n",
    "        # Count unique predictions for this sample\n",
    "        unique_preds = set(all_preds[model][i] for model in all_models)\n",
    "        \n",
    "        # If there are multiple predictions (disagreement)\n",
    "        if len(unique_preds) > 1:\n",
    "            disagreement_indices.append(i)\n",
    "    \n",
    "    # If there are disagreements, visualize some examples\n",
    "    if disagreement_indices:\n",
    "        # Pick a few disagreement cases to visualize\n",
    "        num_examples = min(5, len(disagreement_indices))\n",
    "        selected_indices = np.random.choice(disagreement_indices, num_examples, replace=False)\n",
    "        \n",
    "        # Create figure with subplots directly (avoiding GridSpec layout issues)\n",
    "        fig, axes = plt.subplots(num_examples, 2, figsize=(12, num_examples * 2.5), \n",
    "                                 gridspec_kw={'width_ratios': [1, 2]})\n",
    "        \n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            # Get the original image\n",
    "            orig_img, true_label = test_dataset[sample_indices[idx]]\n",
    "            orig_img = orig_img.permute(1, 2, 0).cpu().numpy()\n",
    "            \n",
    "            # Add normalization values back to make image more viewable\n",
    "            mean = np.array(config.mean).reshape(1, 1, 3)\n",
    "            std = np.array(config.std).reshape(1, 1, 3)\n",
    "            orig_img = orig_img * std + mean\n",
    "            orig_img = np.clip(orig_img, 0, 1)\n",
    "            \n",
    "            # Create image subplot\n",
    "            ax_img = axes[i, 0]\n",
    "            ax_img.imshow(orig_img)\n",
    "            ax_img.set_title(f\"True: {config.classes[true_label]}\", fontsize=11, fontweight='bold')\n",
    "            ax_img.set_xticks([])\n",
    "            ax_img.set_yticks([])\n",
    "            \n",
    "            # Create a bar chart of model predictions and confidences\n",
    "            ax_pred = axes[i, 1]\n",
    "            \n",
    "            # Collect predictions and confidences\n",
    "            pred_classes = []\n",
    "            pred_confs = []\n",
    "            pred_colors = []\n",
    "            \n",
    "            for model_name in all_models:\n",
    "                pred = all_preds[model_name][idx]\n",
    "                conf = np.max(all_probs[model_name][idx]) * 100\n",
    "                pred_classes.append(f\"{model_name}: {config.classes[pred]}\")\n",
    "                pred_confs.append(conf)\n",
    "                \n",
    "                # Use red for student, blue for teachers\n",
    "                color = '#d62728' if model_name == 'student' else '#1f77b4'\n",
    "                pred_colors.append(color)\n",
    "            \n",
    "            # Create horizontal bar chart\n",
    "            y_pos = np.arange(len(pred_classes))\n",
    "            ax_pred.barh(y_pos, pred_confs, color=pred_colors, alpha=0.7)\n",
    "            \n",
    "            # Add confidence values\n",
    "            for j, v in enumerate(pred_confs):\n",
    "                ax_pred.text(v + 1, j, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "            \n",
    "            ax_pred.set_yticks(y_pos)\n",
    "            ax_pred.set_yticklabels(pred_classes)\n",
    "            ax_pred.set_xlabel('Confidence (%)')\n",
    "            ax_pred.set_xlim(0, 105)  # Leave room for labels\n",
    "            ax_pred.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Model Disagreement Examples', fontsize=14, fontweight='bold')\n",
    "        plt.subplots_adjust(hspace=0.4, wspace=0.2, top=0.95, bottom=0.05)  # Adjust spacing without tight_layout\n",
    "        plt.savefig(f\"{config.output_dir}/disagreement_examples.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 6. Create a new visualization: Decision making similarity with t-SNE\n",
    "    logger.info(\"Generating t-SNE visualization of model decision making patterns...\")\n",
    "    \n",
    "    # Extract logits for t-SNE analysis\n",
    "    model_logits = []\n",
    "    model_names_for_tsne = []\n",
    "    \n",
    "    for model_name, probs in all_probs.items():\n",
    "        # Get top predicted classes and confidences for each sample\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        confs = np.max(probs, axis=1)\n",
    "        \n",
    "        # Create a feature vector: [class_1, conf_1, class_2, conf_2, ...]\n",
    "        # This captures both what the model predicted and how confident it was\n",
    "        feature_vector = np.column_stack((preds, confs))\n",
    "        \n",
    "        model_logits.append(feature_vector.flatten())\n",
    "        model_names_for_tsne.append(model_name)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    model_logits = np.array(model_logits)\n",
    "    \n",
    "    # Create and fit t-SNE\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE\n",
    "        \n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(3, len(model_names_for_tsne)-1))\n",
    "        logits_embedded = tsne.fit_transform(model_logits)\n",
    "        \n",
    "        # Create mapping of model types for coloring\n",
    "        model_types = []\n",
    "        markers = []\n",
    "        sizes = []\n",
    "        \n",
    "        for name in model_names_for_tsne:\n",
    "            if name == 'student':\n",
    "                model_types.append('Student')\n",
    "                markers.append('*')       # Star\n",
    "                sizes.append(200)         # Larger\n",
    "            elif name == 'ensemble':\n",
    "                model_types.append('Ensemble')\n",
    "                markers.append('s')       # Square\n",
    "                sizes.append(150)         # Medium large\n",
    "            else:\n",
    "                model_types.append('Teacher')\n",
    "                markers.append('o')       # Circle\n",
    "                sizes.append(100)         # Standard\n",
    "        \n",
    "        # Create DataFrame for plotting\n",
    "        tsne_df = pd.DataFrame({\n",
    "            'x': logits_embedded[:, 0],\n",
    "            'y': logits_embedded[:, 1],\n",
    "            'Model': model_names_for_tsne,\n",
    "            'Type': model_types,\n",
    "            'Marker': markers,\n",
    "            'Size': sizes\n",
    "        })\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Plot each model type separately\n",
    "        for model_type, marker, size in zip(['Student', 'Ensemble', 'Teacher'], ['*', 's', 'o'], [200, 150, 100]):\n",
    "            subset = tsne_df[tsne_df['Type'] == model_type]\n",
    "            if len(subset) > 0:  # Only plot if this type exists\n",
    "                plt.scatter(\n",
    "                    subset['x'], subset['y'],\n",
    "                    s=size,\n",
    "                    marker=marker,\n",
    "                    label=f\"{model_type} Model{'s' if len(subset) > 1 and model_type == 'Teacher' else ''}\",\n",
    "                    edgecolors='black'\n",
    "                )\n",
    "        \n",
    "        plt.title('Model Decision Making Similarity (t-SNE)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "        plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "        plt.legend(loc='best', fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # Adjust layout safely without tight_layout\n",
    "        plt.subplots_adjust(bottom=0.1, left=0.1, right=0.9, top=0.9)\n",
    "        plt.savefig(f\"{config.output_dir}/model_decision_tsne.png\", dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not create t-SNE visualization: {str(e)}\")\n",
    "    \n",
    "    # 7. Create a violin plot of prediction confidences (new visualization)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare data for the violin plot in a better format\n",
    "    confidence_data = []\n",
    "    \n",
    "    for model_name, probs in all_probs.items():\n",
    "        # Extract max confidence per prediction\n",
    "        confidences = np.max(probs, axis=1) * 100\n",
    "        model_type = 'Student' if model_name == 'student' else ('Ensemble' if model_name == 'ensemble' else 'Teacher')\n",
    "        \n",
    "        # Add each data point as a row\n",
    "        for conf in confidences:\n",
    "            confidence_data.append({\n",
    "                'Model': model_name,\n",
    "                'Type': model_type,\n",
    "                'Confidence (%)': conf\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    confidence_df = pd.DataFrame(confidence_data)\n",
    "    \n",
    "    # Create violin plot with explicit color palette\n",
    "    color_palette = {\n",
    "        'Student': '#d62728',  # Red\n",
    "        'Ensemble': '#2ca02c',  # Green\n",
    "        'Teacher': '#1f77b4'   # Blue\n",
    "    }\n",
    "    \n",
    "    # Create the violin plot\n",
    "    sns.violinplot(\n",
    "        x='Model',\n",
    "        y='Confidence (%)',\n",
    "        hue='Type',\n",
    "        data=confidence_df,\n",
    "        palette=color_palette,\n",
    "        inner='quartile',\n",
    "        cut=0,\n",
    "        split=False\n",
    "    )\n",
    "    \n",
    "    plt.title('Distribution of Prediction Confidences', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.legend(title='')\n",
    "    \n",
    "    # Adjust layout safely\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.savefig(f\"{config.output_dir}/confidence_distribution_violin.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save analysis results\n",
    "    analysis_results = {\n",
    "        'student_teacher_agreement': agreement_ratios,\n",
    "        'ground_truth_agreement': ground_truth_agreement,\n",
    "        'average_entropies': avg_entropies,\n",
    "        'disagreement_ratio': len(disagreement_indices) / len(sample_indices) * 100\n",
    "    }\n",
    "    \n",
    "    with open(f\"{config.output_dir}/ensemble_knowledge_analysis.json\", 'w') as f:\n",
    "        json.dump(to_serializable(analysis_results), f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Ensemble knowledge analysis completed and saved to {config.output_dir}\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "####################################\n",
    "# 11. Ensemble Calibration Analysis\n",
    "####################################\n",
    "def analyze_ensemble_calibration(all_models, config, device):\n",
    "    \"\"\"\n",
    "    Analyze the calibration of the student model compared to teachers and the ensemble.\n",
    "    Focus on how ensemble distillation affects calibration.\n",
    "    \"\"\"\n",
    "    logger.info(\"Analyzing ensemble calibration characteristics...\")\n",
    "    \n",
    "    # Check if we have the student model and teachers\n",
    "    if 'student' not in all_models or len(all_models) < 2:\n",
    "        logger.warning(\"Need student and at least one teacher for ensemble calibration analysis\")\n",
    "        return\n",
    "    \n",
    "    # Create a test dataset and loader for analysis\n",
    "    test_dataset = get_test_dataset(config, 'student')\n",
    "    test_loader = create_data_loader(test_dataset, config)\n",
    "    \n",
    "    # Store calibration results\n",
    "    calibration_results = {}\n",
    "    all_probs = {}\n",
    "    all_targets = {}\n",
    "    \n",
    "    # Run inference for each model\n",
    "    for model_name, model in all_models.items():\n",
    "        logger.info(f\"Computing calibration for {model_name}...\")\n",
    "        \n",
    "        targets, preds, probs = run_inference(model, test_loader, config, device, model_name)\n",
    "        \n",
    "        # Calculate multiple calibration metrics\n",
    "        ece, bin_confs, bin_accs, bin_counts = compute_ece(probs, targets, n_bins=config.n_bins_calibration)\n",
    "        \n",
    "        # Store results\n",
    "        calibration_results[model_name] = {\n",
    "            'ece': ece,\n",
    "            'bin_confidences': bin_confs,\n",
    "            'bin_accuracies': bin_accs,\n",
    "            'bin_counts': bin_counts,\n",
    "        }\n",
    "        \n",
    "        all_probs[model_name] = probs\n",
    "        all_targets[model_name] = targets\n",
    "    \n",
    "    # Also create ensemble prediction if teachers are available\n",
    "    teacher_models = {name: model for name, model in all_models.items() \n",
    "                     if name != 'student' and name in config.teacher_weights}\n",
    "    \n",
    "    if len(teacher_models) > 0 and hasattr(config, 'teacher_weights'):\n",
    "        logger.info(\"Computing calibration for teacher ensemble...\")\n",
    "        \n",
    "        # Create ensemble prediction function\n",
    "        ensemble_predict = create_teacher_ensemble(teacher_models, config.teacher_weights)\n",
    "        \n",
    "        # Get ensemble predictions\n",
    "        ensemble_targets, ensemble_preds, ensemble_probs = run_ensemble_inference(\n",
    "            ensemble_predict, test_loader, config, device\n",
    "        )\n",
    "        \n",
    "        # Calculate calibration metrics for ensemble\n",
    "        ece, bin_confs, bin_accs, bin_counts = compute_ece(\n",
    "            ensemble_probs, ensemble_targets, n_bins=config.n_bins_calibration\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        calibration_results['ensemble'] = {\n",
    "            'ece': ece,\n",
    "            'bin_confidences': bin_confs,\n",
    "            'bin_accuracies': bin_accs,\n",
    "            'bin_counts': bin_counts,\n",
    "        }\n",
    "        \n",
    "        all_probs['ensemble'] = ensemble_probs\n",
    "        all_targets['ensemble'] = ensemble_targets\n",
    "    \n",
    "    # 1. Create a combined calibration curve plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Plot calibration curve for each model\n",
    "    for model_name, results in calibration_results.items():\n",
    "        bin_confs = results['bin_confidences']\n",
    "        bin_accs = results['bin_accuracies']\n",
    "        ece = results['ece']\n",
    "        \n",
    "        if model_name == 'student':\n",
    "            color = '#d62728'  # Red\n",
    "            linestyle = '-'\n",
    "            linewidth = 2\n",
    "            marker = 'o'\n",
    "            markersize = 7\n",
    "        elif model_name == 'ensemble':\n",
    "            color = '#2ca02c'  # Green\n",
    "            linestyle = '-'\n",
    "            linewidth = 2\n",
    "            marker = 's'\n",
    "            markersize = 7\n",
    "        else:\n",
    "            color = '#1f77b4'  # Blue\n",
    "            linestyle = '--'\n",
    "            linewidth = 1\n",
    "            marker = '.'\n",
    "            markersize = 5\n",
    "        \n",
    "        plt.plot(bin_confs, bin_accs, marker=marker, linestyle=linestyle, \n",
    "                 linewidth=linewidth, markersize=markersize,\n",
    "                 label=f'{model_name} (ECE={ece:.4f})', color=color)\n",
    "    \n",
    "    # Add legend, labels, and grid\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Calibration Reliability Comparison')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Add axes for the diagonal line\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/combined_calibration_curves.png\", dpi=300)\n",
    "    plt.savefig(f\"{config.output_dir}/combined_calibration_curves.pdf\", format='pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Calculate confidence histogram for each model\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for model_name, probs in all_probs.items():\n",
    "        # Get maximum probability (confidence) for each prediction\n",
    "        confidences = np.max(probs, axis=1)\n",
    "        \n",
    "        if model_name == 'student':\n",
    "            color = '#d62728'  # Red\n",
    "            alpha = 0.7\n",
    "            linestyle = '-'\n",
    "            linewidth = 2\n",
    "        elif model_name == 'ensemble':\n",
    "            color = '#2ca02c'  # Green\n",
    "            alpha = 0.7\n",
    "            linestyle = '-'\n",
    "            linewidth = 2\n",
    "        else:\n",
    "            # Use a lighter blue with low alpha for teachers\n",
    "            color = '#1f77b4'  # Blue\n",
    "            alpha = 0.2\n",
    "            linestyle = '--'\n",
    "            linewidth = 1\n",
    "        \n",
    "        # Create histogram\n",
    "        plt.hist(confidences, bins=20, alpha=alpha, label=model_name, \n",
    "                 color=color, histtype='step', linewidth=linewidth,\n",
    "                 density=True, linestyle=linestyle)\n",
    "    \n",
    "    plt.title('Confidence Distribution')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/confidence_distribution.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Calculate calibration error by confidence level\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for model_name, results in calibration_results.items():\n",
    "        bin_confs = np.array(results['bin_confidences'])\n",
    "        bin_accs = np.array(results['bin_accuracies'])\n",
    "        \n",
    "        # Calculate absolute calibration error at each bin\n",
    "        cal_errors = np.abs(bin_confs - bin_accs)\n",
    "        \n",
    "        if model_name == 'student':\n",
    "            color = '#d62728'  # Red\n",
    "            linestyle = '-'\n",
    "            linewidth = 2\n",
    "            marker = 'o'\n",
    "        elif model_name == 'ensemble':\n",
    "            color = '#2ca02c'  # Green\n",
    "            linestyle = '-'\n",
    "            linewidth = 2\n",
    "            marker = 's'\n",
    "        else:\n",
    "            color = '#1f77b4'  # Blue\n",
    "            linestyle = '--'\n",
    "            linewidth = 1\n",
    "            marker = '.'\n",
    "        \n",
    "        plt.plot(bin_confs, cal_errors, marker=marker, linestyle=linestyle, \n",
    "                 linewidth=linewidth, label=model_name, color=color)\n",
    "    \n",
    "    plt.title('Calibration Error by Confidence Level')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('|Accuracy - Confidence|')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/calibration_error_by_confidence.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Bar chart of ECE comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    model_names = list(calibration_results.keys())\n",
    "    ece_values = [results['ece'] for results in calibration_results.values()]\n",
    "    \n",
    "    # Use red for student, green for ensemble, blue for teachers\n",
    "    colors = []\n",
    "    for name in model_names:\n",
    "        if name == 'student':\n",
    "            colors.append('#d62728')  # Red\n",
    "        elif name == 'ensemble':\n",
    "            colors.append('#2ca02c')  # Green\n",
    "        else:\n",
    "            colors.append('#1f77b4')  # Blue\n",
    "    \n",
    "    bars = plt.bar(model_names, ece_values, color=colors)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "                f\"{height:.4f}\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title('Expected Calibration Error Comparison')\n",
    "    plt.ylabel('ECE (lower is better)')\n",
    "    plt.ylim(0, max(ece_values) * 1.2)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/ece_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save calibration analysis results\n",
    "    simplified_results = {model: {'ece': results['ece']} \n",
    "                         for model, results in calibration_results.items()}\n",
    "    \n",
    "    with open(f\"{config.output_dir}/calibration_analysis.json\", 'w') as f:\n",
    "        json.dump(to_serializable(simplified_results), f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Ensemble calibration analysis completed and saved to {config.output_dir}\")\n",
    "    \n",
    "    return calibration_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
