{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2d216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 22:14:25,250 [INFO] - Using device: cuda\n",
      "2025-04-17 22:14:25,273 [INFO] - GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "2025-04-17 22:14:25,274 [INFO] - GPU Memory: 6.00 GB\n",
      "2025-04-17 22:14:25,274 [INFO] - CUDA Version: 12.4\n",
      "2025-04-17 22:14:25,275 [INFO] - cuDNN benchmark mode enabled\n",
      "2025-04-17 22:14:25,288 [INFO] - Configuration: {\n",
      "    \"seed\": 42,\n",
      "    \"model_name\": \"ensemble_distillation\",\n",
      "    \"dataset\": \"CIFAR-10\",\n",
      "    \"use_amp\": true,\n",
      "    \"memory_efficient_attention\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"batch_size\": 64,\n",
      "    \"gradient_accumulation_steps\": 8,\n",
      "    \"find_batch_size\": false,\n",
      "    \"gpu_memory_fraction\": 0.75,\n",
      "    \"input_size\": 32,\n",
      "    \"model_input_size\": 224,\n",
      "    \"num_workers\": 4,\n",
      "    \"val_split\": 0.1,\n",
      "    \"dataset_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\",\n",
      "    \"clear_cache_every_n_epochs\": 1,\n",
      "    \"pretrained\": true,\n",
      "    \"num_classes\": 10,\n",
      "    \"teacher_models\": [\n",
      "        \"vit\",\n",
      "        \"efficientnet\",\n",
      "        \"inception\",\n",
      "        \"mobilenet\",\n",
      "        \"resnet\",\n",
      "        \"densenet\"\n",
      "    ],\n",
      "    \"teacher_finetune_epochs\": 5,\n",
      "    \"freeze_teacher_backbones\": true,\n",
      "    \"teacher_model_paths\": {\n",
      "        \"vit\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\ViT\\\\checkpoints\\\\vit_b16_teacher_20250321_053628_best.pth\",\n",
      "        \"efficientnet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\EfficientNetB0\\\\checkpoints\\\\efficientnet_b0_teacher_20250325_132652_best.pth\",\n",
      "        \"inception\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\InceptionV3\\\\checkpoints\\\\inception_v3_teacher_20250321_153825_best.pth\",\n",
      "        \"mobilenet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MobileNetV3\\\\checkpoints\\\\mobilenetv3_20250326_035725_best.pth\",\n",
      "        \"resnet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\ResNet50\\\\checkpoints\\\\resnet50_teacher_20250322_225032_best.pth\",\n",
      "        \"densenet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\DenseNet121\\\\checkpoints\\\\densenet121_teacher_20250325_160534_best.pth\"\n",
      "    },\n",
      "    \"use_pretrained_teachers\": true,\n",
      "    \"teacher_accuracies\": {\n",
      "        \"densenet\": 95.07,\n",
      "        \"efficientnet\": 94.94,\n",
      "        \"inception\": 74.3,\n",
      "        \"mobilenet\": 94.98,\n",
      "        \"resnet\": 94.08,\n",
      "        \"vit\": 93.89\n",
      "    },\n",
      "    \"teacher_init_weights\": {\n",
      "        \"densenet\": 1.0,\n",
      "        \"efficientnet\": 1.0,\n",
      "        \"inception\": 0.5,\n",
      "        \"mobilenet\": 1.0,\n",
      "        \"resnet\": 1.0,\n",
      "        \"vit\": 1.0\n",
      "    },\n",
      "    \"use_adaptive_temperature\": true,\n",
      "    \"teacher_temperatures\": {\n",
      "        \"densenet\": 4.0,\n",
      "        \"efficientnet\": 4.0,\n",
      "        \"inception\": 5.0,\n",
      "        \"mobilenet\": 4.0,\n",
      "        \"resnet\": 4.0,\n",
      "        \"vit\": 4.0\n",
      "    },\n",
      "    \"learn_temperatures\": true,\n",
      "    \"use_teacher_gating\": true,\n",
      "    \"gating_threshold\": 0.2,\n",
      "    \"dynamic_weight_update\": true,\n",
      "    \"weighting_scheme\": \"adaptive\",\n",
      "    \"weight_update_interval\": 5,\n",
      "    \"soft_target_temp\": 4.0,\n",
      "    \"epochs\": 50,\n",
      "    \"lr\": 0.001,\n",
      "    \"weight_decay\": 1e-05,\n",
      "    \"early_stop_patience\": 10,\n",
      "    \"alpha\": 0.7,\n",
      "    \"feature_loss_weight\": 0.3,\n",
      "    \"cal_weight\": 0.1,\n",
      "    \"use_curriculum\": true,\n",
      "    \"curriculum_ramp_epochs\": 30,\n",
      "    \"checkpoint_dir\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\EnsembleDistillation\\\\checkpoints\",\n",
      "    \"results_dir\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Results\\\\EnsembleDistillation\",\n",
      "    \"export_dir\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\EnsembleDistillation\\\\exports\",\n",
      "    \"per_teacher_calibration\": true,\n",
      "    \"weight_by_calibration\": true\n",
      "}\n",
      "2025-04-17 22:14:25,296 [INFO] - Random seed set to 42\n",
      "2025-04-17 22:14:25,297 [INFO] - GPU Memory: Current=0.00MB, Peak=0.00MB, Reserved=0.00MB\n",
      "2025-04-17 22:14:25,297 [INFO] - Preparing data loaders...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 22:14:27,202 [INFO] - Training samples: 45000\n",
      "2025-04-17 22:14:27,209 [INFO] - Validation samples: 5000\n",
      "2025-04-17 22:14:27,209 [INFO] - Test samples: 10000\n",
      "2025-04-17 22:14:27,210 [INFO] - Loading pre-trained teacher models...\n",
      "2025-04-17 22:14:27,210 [INFO] - Loading ViT-B16 model...\n",
      "2025-04-17 22:14:45,538 [INFO] - Loading EfficientNetB0 model...\n",
      "2025-04-17 22:14:46,028 [INFO] - Loading InceptionV3 model...\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "2025-04-17 22:14:46,460 [INFO] - Loading MobileNetV3 model...\n",
      "2025-04-17 22:14:46,536 [INFO] - Loading ResNet50 model...\n",
      "2025-04-17 22:14:46,764 [INFO] - Loading DenseNet121 model...\n",
      "2025-04-17 22:14:46,856 [INFO] - Loading pre-trained weights for vit from C:\\Users\\Gading\\Downloads\\Research\\Models\\ViT\\checkpoints\\vit_b16_teacher_20250321_053628_best.pth\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_14208\\1253639460.py:464: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "2025-04-17 22:14:48,217 [INFO] - Successfully loaded pre-trained weights for vit\n",
      "2025-04-17 22:14:48,218 [INFO] - Loading pre-trained weights for efficientnet from C:\\Users\\Gading\\Downloads\\Research\\Models\\EfficientNetB0\\checkpoints\\efficientnet_b0_teacher_20250325_132652_best.pth\n",
      "2025-04-17 22:14:48,491 [INFO] - Successfully loaded pre-trained weights for efficientnet\n",
      "2025-04-17 22:14:48,493 [INFO] - Loading pre-trained weights for inception from C:\\Users\\Gading\\Downloads\\Research\\Models\\InceptionV3\\checkpoints\\inception_v3_teacher_20250321_153825_best.pth\n",
      "2025-04-17 22:14:49,000 [INFO] - Successfully loaded pre-trained weights for inception\n",
      "2025-04-17 22:14:49,000 [INFO] - Loading pre-trained weights for mobilenet from C:\\Users\\Gading\\Downloads\\Research\\Models\\MobileNetV3\\checkpoints\\mobilenetv3_20250326_035725_best.pth\n",
      "2025-04-17 22:14:49,202 [INFO] - Successfully loaded pre-trained weights for mobilenet\n",
      "2025-04-17 22:14:49,202 [INFO] - Loading pre-trained weights for resnet from C:\\Users\\Gading\\Downloads\\Research\\Models\\ResNet50\\checkpoints\\resnet50_teacher_20250322_225032_best.pth\n",
      "2025-04-17 22:14:49,736 [INFO] - Successfully loaded pre-trained weights for resnet\n",
      "2025-04-17 22:14:49,741 [INFO] - Loading pre-trained weights for densenet from C:\\Users\\Gading\\Downloads\\Research\\Models\\DenseNet121\\checkpoints\\densenet121_teacher_20250325_160534_best.pth\n",
      "2025-04-17 22:14:50,383 [INFO] - Successfully loaded pre-trained weights for densenet\n",
      "2025-04-17 22:14:50,437 [INFO] - Model vit loaded and moved to cuda\n",
      "2025-04-17 22:14:50,438 [INFO] - Model vit set to evaluation mode\n",
      "2025-04-17 22:14:50,456 [INFO] - Model efficientnet loaded and moved to cuda\n",
      "2025-04-17 22:14:50,459 [INFO] - Model efficientnet set to evaluation mode\n",
      "2025-04-17 22:14:50,511 [INFO] - Model inception loaded and moved to cuda\n",
      "2025-04-17 22:14:50,514 [INFO] - Model inception set to evaluation mode\n",
      "2025-04-17 22:14:50,530 [INFO] - Model mobilenet loaded and moved to cuda\n",
      "2025-04-17 22:14:50,532 [INFO] - Model mobilenet set to evaluation mode\n",
      "2025-04-17 22:14:50,557 [INFO] - Model resnet loaded and moved to cuda\n",
      "2025-04-17 22:14:50,559 [INFO] - Model resnet set to evaluation mode\n",
      "2025-04-17 22:14:50,611 [INFO] - Model densenet loaded and moved to cuda\n",
      "2025-04-17 22:14:50,611 [INFO] - Model densenet set to evaluation mode\n",
      "2025-04-17 22:14:50,618 [INFO] - Skipping teacher fine-tuning as pre-trained models are being used\n",
      "2025-04-17 22:14:50,618 [INFO] - Creating student model...\n",
      "2025-04-17 22:14:50,618 [INFO] - Creating scaled EfficientNet-B0 student model...\n",
      "2025-04-17 22:14:50,754 [INFO] - Student model created with 4.02M parameters\n",
      "2025-04-17 22:14:50,776 [INFO] - Training student with calibration-aware ensemble distillation...\n",
      "2025-04-17 22:14:50,777 [INFO] - Training student model with ensemble distillation...\n",
      "2025-04-17 22:14:50,778 [INFO] - Teacher weighting initialized with scheme: adaptive\n",
      "2025-04-17 22:14:50,778 [INFO] - Initial teacher weights: {'vit': 0.18181818181818182, 'efficientnet': 0.18181818181818182, 'inception': 0.09090909090909091, 'mobilenet': 0.18181818181818182, 'resnet': 0.18181818181818182, 'densenet': 0.18181818181818182}\n",
      "2025-04-17 22:14:50,778 [INFO] - Initial teacher temperatures: {'vit': 4.0, 'efficientnet': 4.0, 'inception': 5.0, 'mobilenet': 4.0, 'resnet': 4.0, 'densenet': 4.0}\n",
      "2025-04-17 22:14:50,781 [INFO] - Hook registered for encoder.ln\n",
      "2025-04-17 22:14:50,781 [INFO] - Feature extractor registered for vit at layer encoder.ln\n",
      "2025-04-17 22:15:07,128 [INFO] - Feature shape for vit: torch.Size([1, 197, 768])\n",
      "2025-04-17 22:15:07,129 [INFO] - Hook registered for features.8\n",
      "2025-04-17 22:15:07,129 [INFO] - Feature extractor registered for efficientnet at layer features.8\n",
      "2025-04-17 22:15:09,408 [INFO] - Feature shape for efficientnet: torch.Size([1, 1280, 7, 7])\n",
      "2025-04-17 22:15:09,409 [INFO] - Hook registered for Mixed_7c\n",
      "2025-04-17 22:15:09,409 [INFO] - Feature extractor registered for inception at layer Mixed_7c\n",
      "2025-04-17 22:15:10,029 [INFO] - Feature shape for inception: torch.Size([1, 2048, 5, 5])\n",
      "2025-04-17 22:15:10,029 [INFO] - Hook registered for features\n",
      "2025-04-17 22:15:10,031 [INFO] - Feature extractor registered for mobilenet at layer features\n",
      "2025-04-17 22:15:10,198 [INFO] - Feature shape for mobilenet: torch.Size([1, 960, 7, 7])\n",
      "2025-04-17 22:15:10,198 [INFO] - Hook registered for layer4\n",
      "2025-04-17 22:15:10,198 [INFO] - Feature extractor registered for resnet at layer layer4\n",
      "2025-04-17 22:15:10,391 [INFO] - Feature shape for resnet: torch.Size([1, 2048, 7, 7])\n",
      "2025-04-17 22:15:10,391 [INFO] - Hook registered for features\n",
      "2025-04-17 22:15:10,391 [INFO] - Feature extractor registered for densenet at layer features\n",
      "2025-04-17 22:15:10,763 [INFO] - Feature shape for densenet: torch.Size([1, 1024, 7, 7])\n",
      "2025-04-17 22:15:10,764 [INFO] - Hook registered for features.8\n",
      "2025-04-17 22:15:10,765 [INFO] - Feature extractor registered for student model\n",
      "2025-04-17 22:15:10,821 [INFO] - Feature shape for student: torch.Size([1, 1280, 7, 7])\n",
      "2025-04-17 22:15:10,866 [INFO] - HFI module initialized with 6 teachers\n",
      "2025-04-17 22:15:10,867 [INFO] - Target student feature shape: torch.Size([1, 1280, 7, 7])\n",
      "2025-04-17 22:15:10,921 [INFO] - Adding teacher temperatures to optimizer parameters\n",
      "2025-04-17 22:15:10,923 [INFO] - Configuration saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\ensemble_distillation_20250417_221510_config.json\n",
      "2025-04-17 22:15:10,925 [INFO] - GPU Memory: Current=646.47MB, Peak=1031.56MB, Reserved=698.00MB\n",
      "2025-04-17 22:15:10,926 [INFO] - Epoch 1/50\n",
      "2025-04-17 22:15:11,113 [INFO] - GPU cache cleared: 646.47MB → 646.47MB (freed 0.00MB)\n",
      "2025-04-17 22:15:11,115 [INFO] - Current calibration weight: 0.0033\n",
      "2025-04-17 22:15:11,115 [INFO] - Updated teacher weights: {'vit': 0.17156379052004533, 'efficientnet': 0.1734824397909586, 'inception': 0.1357672769798633, 'mobilenet': 0.1735555311917553, 'resnet': 0.17191097467382963, 'densenet': 0.17371998684354784}\n",
      "2025-04-17 22:15:11,116 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:15:11,126 [INFO] - Current teacher temperatures: {'vit': 5.0, 'efficientnet': 5.0, 'inception': 6.0, 'mobilenet': 5.0, 'resnet': 5.0, 'densenet': 5.0}\n",
      "2025-04-17 22:15:11,126 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:15:11,146 [INFO] - HFI attention weights: {'vit': 0.1666666716337204, 'efficientnet': 0.1666666716337204, 'inception': 0.1666666716337204, 'mobilenet': 0.1666666716337204, 'resnet': 0.1666666716337204, 'densenet': 0.1666666716337204}\n",
      "Training (cal_weight=0.0033): 100%|██████████| 704/704 [07:46<00:00,  1.51it/s, loss=5.01, acc=83.8%, ce=0.60, kl=0.41, feat=0.61, cal=0.10] \n",
      "Validating: 100%|██████████| 79/79 [00:23<00:00,  3.35it/s]\n",
      "2025-04-17 22:23:21,797 [INFO] - Epoch 1 Results - Time: 490.87s, LR: 0.000999\n",
      "2025-04-17 22:23:21,800 [INFO] - Train - Loss: 5.0060, Acc: 83.79%\n",
      "2025-04-17 22:23:21,801 [INFO] -   CE: 0.6013, KL: 0.4145, Feat: 0.6101, Cal: 0.1044\n",
      "2025-04-17 22:23:21,802 [INFO] - Val - Loss: 0.1957, Acc: 94.86%, F1: 0.9481, ECE: 0.0291\n",
      "2025-04-17 22:23:22,394 [INFO] - New best model saved (Val Loss: 0.1957)\n",
      "2025-04-17 22:23:22,710 [INFO] - New best model saved (Val Acc: 94.86%)\n",
      "2025-04-17 22:23:22,715 [INFO] - GPU Memory: Current=841.35MB, Peak=4296.33MB, Reserved=1636.00MB\n",
      "2025-04-17 22:23:22,716 [INFO] - Epoch 2/50\n",
      "2025-04-17 22:23:23,686 [INFO] - GPU cache cleared: 841.35MB → 841.35MB (freed 0.00MB)\n",
      "2025-04-17 22:23:23,687 [INFO] - Current calibration weight: 0.0067\n",
      "2025-04-17 22:23:23,688 [INFO] - Updated teacher weights: {'vit': 0.4192149368262984, 'efficientnet': 0.13313449830171956, 'inception': 0.03151863562019519, 'mobilenet': 0.15118301290756728, 'resnet': 0.1320697443140333, 'densenet': 0.13287917203018634}\n",
      "2025-04-17 22:23:23,689 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:23:23,689 [INFO] - Current teacher temperatures: {'vit': 4.950312614440918, 'efficientnet': 4.942068099975586, 'inception': 5.979669570922852, 'mobilenet': 4.9479169845581055, 'resnet': 4.94888973236084, 'densenet': 4.94724702835083}\n",
      "2025-04-17 22:23:23,690 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:23:23,691 [INFO] - HFI attention weights: {'vit': 0.1729714274406433, 'efficientnet': 0.16518862545490265, 'inception': 0.16258357465267181, 'mobilenet': 0.16951130330562592, 'resnet': 0.161488875746727, 'densenet': 0.16825620830059052}\n",
      "Training (cal_weight=0.0067): 100%|██████████| 704/704 [07:29<00:00,  1.57it/s, loss=2.26, acc=94.5%, ce=0.20, kl=0.19, feat=0.23, cal=0.04]\n",
      "Validating: 100%|██████████| 79/79 [00:03<00:00, 22.84it/s]\n",
      "2025-04-17 22:30:56,641 [INFO] - Epoch 2 Results - Time: 453.93s, LR: 0.000996\n",
      "2025-04-17 22:30:56,642 [INFO] - Train - Loss: 2.2637, Acc: 94.48%\n",
      "2025-04-17 22:30:56,643 [INFO] -   CE: 0.1960, KL: 0.1907, Feat: 0.2302, Cal: 0.0413\n",
      "2025-04-17 22:30:56,644 [INFO] - Val - Loss: 0.1368, Acc: 96.28%, F1: 0.9625, ECE: 0.0211\n",
      "2025-04-17 22:30:57,153 [INFO] - New best model saved (Val Loss: 0.1368)\n",
      "2025-04-17 22:30:57,411 [INFO] - New best model saved (Val Acc: 96.28%)\n",
      "2025-04-17 22:30:57,412 [INFO] - GPU Memory: Current=841.69MB, Peak=4296.33MB, Reserved=4726.00MB\n",
      "2025-04-17 22:30:57,412 [INFO] - Epoch 3/50\n",
      "2025-04-17 22:30:57,689 [INFO] - GPU cache cleared: 841.69MB → 841.69MB (freed 0.00MB)\n",
      "2025-04-17 22:30:57,689 [INFO] - Current calibration weight: 0.0100\n",
      "2025-04-17 22:30:57,690 [INFO] - Updated teacher weights: {'vit': 0.23639033727118205, 'efficientnet': 0.2771508481757986, 'inception': 0.017180285402043267, 'mobilenet': 0.11835967668828784, 'resnet': 0.07338850554807723, 'densenet': 0.27753034691461104}\n",
      "2025-04-17 22:30:57,691 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:30:57,693 [INFO] - Current teacher temperatures: {'vit': 4.863611221313477, 'efficientnet': 4.884550094604492, 'inception': 6.017629146575928, 'mobilenet': 4.897582054138184, 'resnet': 4.896173477172852, 'densenet': 4.896034240722656}\n",
      "2025-04-17 22:30:57,693 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:30:57,694 [INFO] - HFI attention weights: {'vit': 0.17630232870578766, 'efficientnet': 0.1659817397594452, 'inception': 0.1640816032886505, 'mobilenet': 0.16425815224647522, 'resnet': 0.1614045947790146, 'densenet': 0.16797158122062683}\n",
      "Training (cal_weight=0.0100): 100%|██████████| 704/704 [07:11<00:00,  1.63it/s, loss=1.99, acc=96.3%, ce=0.12, kl=0.17, feat=0.15, cal=0.03]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 26.77it/s]\n",
      "2025-04-17 22:38:12,184 [INFO] - Epoch 3 Results - Time: 434.77s, LR: 0.000991\n",
      "2025-04-17 22:38:12,185 [INFO] - Train - Loss: 1.9912, Acc: 96.28%\n",
      "2025-04-17 22:38:12,186 [INFO] -   CE: 0.1216, KL: 0.1706, Feat: 0.1461, Cal: 0.0272\n",
      "2025-04-17 22:38:12,187 [INFO] - Val - Loss: 0.1123, Acc: 96.94%, F1: 0.9692, ECE: 0.0176\n",
      "2025-04-17 22:38:12,762 [INFO] - New best model saved (Val Loss: 0.1123)\n",
      "2025-04-17 22:38:13,046 [INFO] - New best model saved (Val Acc: 96.94%)\n",
      "2025-04-17 22:38:13,048 [INFO] - GPU Memory: Current=841.72MB, Peak=4296.33MB, Reserved=4678.00MB\n",
      "2025-04-17 22:38:13,048 [INFO] - Epoch 4/50\n",
      "2025-04-17 22:38:13,255 [INFO] - GPU cache cleared: 841.72MB → 841.72MB (freed 0.00MB)\n",
      "2025-04-17 22:38:13,256 [INFO] - Current calibration weight: 0.0133\n",
      "2025-04-17 22:38:13,257 [INFO] - Updated teacher weights: {'vit': 0.10153112780519193, 'efficientnet': 0.29027511616226526, 'inception': 0.018137249386946643, 'mobilenet': 0.07471989929056183, 'resnet': 0.2876457017963547, 'densenet': 0.22769090555867968}\n",
      "2025-04-17 22:38:13,258 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:38:13,259 [INFO] - Current teacher temperatures: {'vit': 4.772839546203613, 'efficientnet': 4.820683479309082, 'inception': 6.074615478515625, 'mobilenet': 4.84004020690918, 'resnet': 4.837343215942383, 'densenet': 4.837935447692871}\n",
      "2025-04-17 22:38:13,259 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:38:13,260 [INFO] - HFI attention weights: {'vit': 0.17963702976703644, 'efficientnet': 0.16731785237789154, 'inception': 0.16448763012886047, 'mobilenet': 0.16052912175655365, 'resnet': 0.16046790778636932, 'densenet': 0.16756047308444977}\n",
      "Training (cal_weight=0.0133): 100%|██████████| 704/704 [07:11<00:00,  1.63it/s, loss=1.84, acc=97.1%, ce=0.09, kl=0.16, feat=0.11, cal=0.02]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 27.79it/s]\n",
      "2025-04-17 22:45:27,555 [INFO] - Epoch 4 Results - Time: 434.51s, LR: 0.000984\n",
      "2025-04-17 22:45:27,555 [INFO] - Train - Loss: 1.8417, Acc: 97.08%\n",
      "2025-04-17 22:45:27,557 [INFO] -   CE: 0.0898, KL: 0.1591, Feat: 0.1099, Cal: 0.0208\n",
      "2025-04-17 22:45:27,557 [INFO] - Val - Loss: 0.1213, Acc: 96.68%, F1: 0.9665, ECE: 0.0181\n",
      "2025-04-17 22:45:27,830 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-17 22:45:27,830 [INFO] - Epoch 5/50\n",
      "2025-04-17 22:45:28,050 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-17 22:45:28,050 [INFO] - Current calibration weight: 0.0167\n",
      "2025-04-17 22:45:28,050 [INFO] - Updated teacher weights: {'vit': 0.20073646747959592, 'efficientnet': 0.20298136353725463, 'inception': 0.018766731248487035, 'mobilenet': 0.18619252578073095, 'resnet': 0.20114268676621985, 'densenet': 0.19018022518771163}\n",
      "2025-04-17 22:45:28,050 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:45:28,050 [INFO] - Current teacher temperatures: {'vit': 4.680971145629883, 'efficientnet': 4.753368377685547, 'inception': 6.145500183105469, 'mobilenet': 4.7783613204956055, 'resnet': 4.774232864379883, 'densenet': 4.775443077087402}\n",
      "2025-04-17 22:45:28,050 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:45:28,050 [INFO] - HFI attention weights: {'vit': 0.18255461752414703, 'efficientnet': 0.16848224401474, 'inception': 0.16456671059131622, 'mobilenet': 0.15801148116588593, 'resnet': 0.1597244143486023, 'densenet': 0.16666053235530853}\n",
      "Training (cal_weight=0.0167): 100%|██████████| 704/704 [06:47<00:00,  1.73it/s, loss=1.71, acc=97.8%, ce=0.07, kl=0.15, feat=0.09, cal=0.02]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.85it/s]\n",
      "2025-04-17 22:52:18,288 [INFO] - Epoch 5 Results - Time: 410.46s, LR: 0.000976\n",
      "2025-04-17 22:52:18,289 [INFO] - Train - Loss: 1.7137, Acc: 97.80%\n",
      "2025-04-17 22:52:18,290 [INFO] -   CE: 0.0666, KL: 0.1489, Feat: 0.0869, Cal: 0.0159\n",
      "2025-04-17 22:52:18,290 [INFO] - Val - Loss: 0.1110, Acc: 96.88%, F1: 0.9686, ECE: 0.0169\n",
      "2025-04-17 22:52:18,761 [INFO] - New best model saved (Val Loss: 0.1110)\n",
      "2025-04-17 22:52:18,761 [INFO] - GPU Memory: Current=841.66MB, Peak=4296.33MB, Reserved=4676.00MB\n",
      "2025-04-17 22:52:18,761 [INFO] - Epoch 6/50\n",
      "2025-04-17 22:52:18,959 [INFO] - GPU cache cleared: 841.66MB → 841.66MB (freed 0.00MB)\n",
      "2025-04-17 22:52:18,959 [INFO] - Current calibration weight: 0.0200\n",
      "2025-04-17 22:52:18,959 [INFO] - Updated teacher weights: {'vit': 0.17701277243288127, 'efficientnet': 0.20296255267759972, 'inception': 0.012612097553449882, 'mobilenet': 0.20304806460204786, 'resnet': 0.20112404630196523, 'densenet': 0.20324046643205607}\n",
      "2025-04-17 22:52:18,959 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:52:18,959 [INFO] - Current teacher temperatures: {'vit': 4.58743143081665, 'efficientnet': 4.684953689575195, 'inception': 6.2214250564575195, 'mobilenet': 4.714205741882324, 'resnet': 4.708282947540283, 'densenet': 4.708698272705078}\n",
      "2025-04-17 22:52:18,959 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:52:18,959 [INFO] - HFI attention weights: {'vit': 0.18509696424007416, 'efficientnet': 0.16947679221630096, 'inception': 0.16485421359539032, 'mobilenet': 0.15596526861190796, 'resnet': 0.1589653640985489, 'densenet': 0.16564138233661652}\n",
      "Training (cal_weight=0.0200): 100%|██████████| 704/704 [06:40<00:00,  1.76it/s, loss=1.63, acc=98.3%, ce=0.05, kl=0.14, feat=0.07, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.90it/s]\n",
      "2025-04-17 22:59:02,253 [INFO] - Epoch 6 Results - Time: 403.49s, LR: 0.000965\n",
      "2025-04-17 22:59:02,254 [INFO] - Train - Loss: 1.6346, Acc: 98.27%\n",
      "2025-04-17 22:59:02,255 [INFO] -   CE: 0.0506, KL: 0.1426, Feat: 0.0735, Cal: 0.0126\n",
      "2025-04-17 22:59:02,255 [INFO] - Val - Loss: 0.1145, Acc: 96.78%, F1: 0.9676, ECE: 0.0196\n",
      "2025-04-17 22:59:02,527 [INFO] - GPU Memory: Current=840.72MB, Peak=4296.33MB, Reserved=4642.00MB\n",
      "2025-04-17 22:59:02,527 [INFO] - Epoch 7/50\n",
      "2025-04-17 22:59:02,716 [INFO] - GPU cache cleared: 840.72MB → 840.72MB (freed 0.00MB)\n",
      "2025-04-17 22:59:02,716 [INFO] - Current calibration weight: 0.0233\n",
      "2025-04-17 22:59:02,718 [INFO] - Updated teacher weights: {'vit': 0.26026342158325266, 'efficientnet': 0.26317402540328055, 'inception': 0.027479174681923423, 'mobilenet': 0.12267430704258969, 'resnet': 0.06287468541271736, 'densenet': 0.26353438587623634}\n",
      "2025-04-17 22:59:02,718 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:59:02,718 [INFO] - Current teacher temperatures: {'vit': 4.4917426109313965, 'efficientnet': 4.614378452301025, 'inception': 6.303608417510986, 'mobilenet': 4.6489105224609375, 'resnet': 4.6392927169799805, 'densenet': 4.639979362487793}\n",
      "2025-04-17 22:59:02,718 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 22:59:02,718 [INFO] - HFI attention weights: {'vit': 0.18759207427501678, 'efficientnet': 0.1706947535276413, 'inception': 0.16467365622520447, 'mobilenet': 0.15471620857715607, 'resnet': 0.1580507457256317, 'densenet': 0.16427256166934967}\n",
      "Training (cal_weight=0.0233): 100%|██████████| 704/704 [06:38<00:00,  1.77it/s, loss=1.57, acc=98.5%, ce=0.05, kl=0.14, feat=0.06, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.93it/s]\n",
      "2025-04-17 23:05:44,249 [INFO] - Epoch 7 Results - Time: 401.72s, LR: 0.000952\n",
      "2025-04-17 23:05:44,249 [INFO] - Train - Loss: 1.5715, Acc: 98.48%\n",
      "2025-04-17 23:05:44,250 [INFO] -   CE: 0.0479, KL: 0.1373, Feat: 0.0624, Cal: 0.0115\n",
      "2025-04-17 23:05:44,251 [INFO] - Val - Loss: 0.1106, Acc: 96.96%, F1: 0.9695, ECE: 0.0172\n",
      "2025-04-17 23:05:44,731 [INFO] - New best model saved (Val Loss: 0.1106)\n",
      "2025-04-17 23:05:44,963 [INFO] - New best model saved (Val Acc: 96.96%)\n",
      "2025-04-17 23:05:44,963 [INFO] - GPU Memory: Current=841.25MB, Peak=4296.33MB, Reserved=4732.00MB\n",
      "2025-04-17 23:05:44,963 [INFO] - Epoch 8/50\n",
      "2025-04-17 23:05:45,161 [INFO] - GPU cache cleared: 841.25MB → 841.25MB (freed 0.00MB)\n",
      "2025-04-17 23:05:45,162 [INFO] - Current calibration weight: 0.0267\n",
      "2025-04-17 23:05:45,162 [INFO] - Updated teacher weights: {'vit': 0.30036796976497504, 'efficientnet': 0.14146235315114075, 'inception': 0.07305630792706912, 'mobilenet': 0.21039488054887948, 'resnet': 0.15658142354348698, 'densenet': 0.11813706506444867}\n",
      "2025-04-17 23:05:45,162 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:05:45,165 [INFO] - Current teacher temperatures: {'vit': 4.395513534545898, 'efficientnet': 4.543320178985596, 'inception': 6.38742208480835, 'mobilenet': 4.579992294311523, 'resnet': 4.56995153427124, 'densenet': 4.570572853088379}\n",
      "2025-04-17 23:05:45,166 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:05:45,166 [INFO] - HFI attention weights: {'vit': 0.1895972341299057, 'efficientnet': 0.17150533199310303, 'inception': 0.1646176129579544, 'mobilenet': 0.1535443365573883, 'resnet': 0.1575596034526825, 'densenet': 0.16317586600780487}\n",
      "Training (cal_weight=0.0267): 100%|██████████| 704/704 [06:36<00:00,  1.78it/s, loss=1.52, acc=98.7%, ce=0.04, kl=0.13, feat=0.06, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.27it/s]\n",
      "2025-04-17 23:12:24,385 [INFO] - Epoch 8 Results - Time: 399.42s, LR: 0.000938\n",
      "2025-04-17 23:12:24,387 [INFO] - Train - Loss: 1.5184, Acc: 98.74%\n",
      "2025-04-17 23:12:24,388 [INFO] -   CE: 0.0384, KL: 0.1330, Feat: 0.0562, Cal: 0.0094\n",
      "2025-04-17 23:12:24,389 [INFO] - Val - Loss: 0.0975, Acc: 97.26%, F1: 0.9724, ECE: 0.0144\n",
      "2025-04-17 23:12:24,917 [INFO] - New best model saved (Val Loss: 0.0975)\n",
      "2025-04-17 23:12:25,159 [INFO] - New best model saved (Val Acc: 97.26%)\n",
      "2025-04-17 23:12:25,161 [INFO] - GPU Memory: Current=840.72MB, Peak=4296.33MB, Reserved=4672.00MB\n",
      "2025-04-17 23:12:25,161 [INFO] - Epoch 9/50\n",
      "2025-04-17 23:12:25,349 [INFO] - GPU cache cleared: 840.72MB → 840.72MB (freed 0.00MB)\n",
      "2025-04-17 23:12:25,349 [INFO] - Current calibration weight: 0.0300\n",
      "2025-04-17 23:12:25,349 [INFO] - Updated teacher weights: {'vit': 0.2098447051979451, 'efficientnet': 0.233321312142315, 'inception': 0.017055590123786855, 'mobilenet': 0.07636200248488254, 'resnet': 0.22977559433409256, 'densenet': 0.23364079571697793}\n",
      "2025-04-17 23:12:25,349 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:12:25,349 [INFO] - Current teacher temperatures: {'vit': 4.298609733581543, 'efficientnet': 4.471929550170898, 'inception': 6.4753522872924805, 'mobilenet': 4.509243965148926, 'resnet': 4.501745700836182, 'densenet': 4.498885631561279}\n",
      "2025-04-17 23:12:25,349 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:12:25,349 [INFO] - HFI attention weights: {'vit': 0.1915373057126999, 'efficientnet': 0.17252813279628754, 'inception': 0.16432657837867737, 'mobilenet': 0.1527094691991806, 'resnet': 0.15699982643127441, 'densenet': 0.16189861297607422}\n",
      "Training (cal_weight=0.0300): 100%|██████████| 704/704 [06:35<00:00,  1.78it/s, loss=1.45, acc=98.8%, ce=0.04, kl=0.13, feat=0.05, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.09it/s]\n",
      "2025-04-17 23:19:03,620 [INFO] - Epoch 9 Results - Time: 398.46s, LR: 0.000922\n",
      "2025-04-17 23:19:03,621 [INFO] - Train - Loss: 1.4506, Acc: 98.82%\n",
      "2025-04-17 23:19:03,621 [INFO] -   CE: 0.0357, KL: 0.1272, Feat: 0.0513, Cal: 0.0089\n",
      "2025-04-17 23:19:03,622 [INFO] - Val - Loss: 0.0946, Acc: 97.40%, F1: 0.9738, ECE: 0.0139\n",
      "2025-04-17 23:19:04,151 [INFO] - New best model saved (Val Loss: 0.0946)\n",
      "2025-04-17 23:19:04,386 [INFO] - New best model saved (Val Acc: 97.40%)\n",
      "2025-04-17 23:19:04,386 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-17 23:19:04,386 [INFO] - Epoch 10/50\n",
      "2025-04-17 23:19:04,575 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-17 23:19:04,579 [INFO] - Current calibration weight: 0.0333\n",
      "2025-04-17 23:19:04,579 [INFO] - Updated teacher weights: {'vit': 0.3367735037314556, 'efficientnet': 0.1559464601191016, 'inception': 0.0355186333736306, 'mobilenet': 0.15753472141615033, 'resnet': 0.1542570934959892, 'densenet': 0.15996958786367274}\n",
      "2025-04-17 23:19:04,579 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:19:04,579 [INFO] - Current teacher temperatures: {'vit': 4.201010704040527, 'efficientnet': 4.400570392608643, 'inception': 6.5534443855285645, 'mobilenet': 4.43840217590332, 'resnet': 4.431327819824219, 'densenet': 4.426312446594238}\n",
      "2025-04-17 23:19:04,579 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:19:04,579 [INFO] - HFI attention weights: {'vit': 0.1932663470506668, 'efficientnet': 0.17386697232723236, 'inception': 0.16360236704349518, 'mobilenet': 0.15209917724132538, 'resnet': 0.156321719288826, 'densenet': 0.16084343194961548}\n",
      "Training (cal_weight=0.0333): 100%|██████████| 704/704 [06:35<00:00,  1.78it/s, loss=1.39, acc=99.1%, ce=0.03, kl=0.12, feat=0.05, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.21it/s]\n",
      "2025-04-17 23:25:42,370 [INFO] - Epoch 10 Results - Time: 397.98s, LR: 0.000905\n",
      "2025-04-17 23:25:42,371 [INFO] - Train - Loss: 1.3853, Acc: 99.06%\n",
      "2025-04-17 23:25:42,372 [INFO] -   CE: 0.0276, KL: 0.1217, Feat: 0.0473, Cal: 0.0070\n",
      "2025-04-17 23:25:42,372 [INFO] - Val - Loss: 0.0963, Acc: 97.34%, F1: 0.9733, ECE: 0.0155\n",
      "2025-04-17 23:25:42,843 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4672.00MB\n",
      "2025-04-17 23:25:42,853 [INFO] - Epoch 11/50\n",
      "2025-04-17 23:25:43,041 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-17 23:25:43,041 [INFO] - Current calibration weight: 0.0367\n",
      "2025-04-17 23:25:43,041 [INFO] - Updated teacher weights: {'vit': 0.17706179172066722, 'efficientnet': 0.21745961035494962, 'inception': 0.02866140887191665, 'mobilenet': 0.2175512301612926, 'resnet': 0.21548978451857656, 'densenet': 0.14377617437259738}\n",
      "2025-04-17 23:25:43,041 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:25:43,041 [INFO] - Current teacher temperatures: {'vit': 4.105005264282227, 'efficientnet': 4.329894065856934, 'inception': 6.631916046142578, 'mobilenet': 4.368213176727295, 'resnet': 4.362628936767578, 'densenet': 4.354061603546143}\n",
      "2025-04-17 23:25:43,041 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:25:43,047 [INFO] - HFI attention weights: {'vit': 0.19512750208377838, 'efficientnet': 0.17486009001731873, 'inception': 0.1629510074853897, 'mobilenet': 0.15180553495883942, 'resnet': 0.15547800064086914, 'densenet': 0.15977780520915985}\n",
      "Training (cal_weight=0.0367): 100%|██████████| 704/704 [06:34<00:00,  1.78it/s, loss=1.35, acc=99.1%, ce=0.03, kl=0.12, feat=0.04, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.24it/s]\n",
      "2025-04-17 23:32:20,732 [INFO] - Epoch 11 Results - Time: 397.88s, LR: 0.000885\n",
      "2025-04-17 23:32:20,733 [INFO] - Train - Loss: 1.3539, Acc: 99.09%\n",
      "2025-04-17 23:32:20,734 [INFO] -   CE: 0.0264, KL: 0.1190, Feat: 0.0436, Cal: 0.0067\n",
      "2025-04-17 23:32:20,735 [INFO] - Val - Loss: 0.0929, Acc: 97.50%, F1: 0.9748, ECE: 0.0144\n",
      "2025-04-17 23:32:21,226 [INFO] - New best model saved (Val Loss: 0.0929)\n",
      "2025-04-17 23:32:21,460 [INFO] - New best model saved (Val Acc: 97.50%)\n",
      "2025-04-17 23:32:21,460 [INFO] - GPU Memory: Current=841.58MB, Peak=4296.33MB, Reserved=4678.00MB\n",
      "2025-04-17 23:32:21,460 [INFO] - Epoch 12/50\n",
      "2025-04-17 23:32:21,650 [INFO] - GPU cache cleared: 841.58MB → 841.58MB (freed 0.00MB)\n",
      "2025-04-17 23:32:21,657 [INFO] - Current calibration weight: 0.0400\n",
      "2025-04-17 23:32:21,657 [INFO] - Updated teacher weights: {'vit': 0.41507330311814467, 'efficientnet': 0.09538303994776155, 'inception': 0.02745763981580706, 'mobilenet': 0.26458100744215896, 'resnet': 0.09153254180200378, 'densenet': 0.10597246787412395}\n",
      "2025-04-17 23:32:21,657 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:32:21,657 [INFO] - Current teacher temperatures: {'vit': 4.009160041809082, 'efficientnet': 4.260684967041016, 'inception': 6.712468147277832, 'mobilenet': 4.298004150390625, 'resnet': 4.293595790863037, 'densenet': 4.280505180358887}\n",
      "2025-04-17 23:32:21,657 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:32:21,657 [INFO] - HFI attention weights: {'vit': 0.19651931524276733, 'efficientnet': 0.17600174248218536, 'inception': 0.1622128039598465, 'mobilenet': 0.15140467882156372, 'resnet': 0.15500980615615845, 'densenet': 0.15885166823863983}\n",
      "Training (cal_weight=0.0400): 100%|██████████| 704/704 [06:34<00:00,  1.79it/s, loss=1.29, acc=99.2%, ce=0.02, kl=0.11, feat=0.04, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.80it/s]\n",
      "2025-04-17 23:38:58,445 [INFO] - Epoch 12 Results - Time: 396.99s, LR: 0.000864\n",
      "2025-04-17 23:38:58,448 [INFO] - Train - Loss: 1.2852, Acc: 99.18%\n",
      "2025-04-17 23:38:58,449 [INFO] -   CE: 0.0235, KL: 0.1130, Feat: 0.0407, Cal: 0.0059\n",
      "2025-04-17 23:38:58,449 [INFO] - Val - Loss: 0.1082, Acc: 97.16%, F1: 0.9713, ECE: 0.0180\n",
      "2025-04-17 23:38:58,712 [INFO] - GPU Memory: Current=840.82MB, Peak=4296.33MB, Reserved=4676.00MB\n",
      "2025-04-17 23:38:58,712 [INFO] - Epoch 13/50\n",
      "2025-04-17 23:38:58,900 [INFO] - GPU cache cleared: 840.82MB → 840.82MB (freed 0.00MB)\n",
      "2025-04-17 23:38:58,900 [INFO] - Current calibration weight: 0.0433\n",
      "2025-04-17 23:38:58,900 [INFO] - Updated teacher weights: {'vit': 0.1699654010830722, 'efficientnet': 0.21730062163944897, 'inception': 0.01634944324162505, 'mobilenet': 0.1634541297478638, 'resnet': 0.21533223597892728, 'densenet': 0.21759816830906267}\n",
      "2025-04-17 23:38:58,900 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:38:58,900 [INFO] - Current teacher temperatures: {'vit': 3.9152843952178955, 'efficientnet': 4.192223072052002, 'inception': 6.786430358886719, 'mobilenet': 4.228532314300537, 'resnet': 4.226891040802002, 'densenet': 4.208552360534668}\n",
      "2025-04-17 23:38:58,910 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:38:58,910 [INFO] - HFI attention weights: {'vit': 0.19758716225624084, 'efficientnet': 0.17694684863090515, 'inception': 0.16165201365947723, 'mobilenet': 0.15101949870586395, 'resnet': 0.15483726561069489, 'densenet': 0.15795719623565674}\n",
      "Training (cal_weight=0.0433): 100%|██████████| 704/704 [06:33<00:00,  1.79it/s, loss=1.25, acc=99.3%, ce=0.02, kl=0.11, feat=0.04, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.98it/s]\n",
      "2025-04-17 23:45:35,657 [INFO] - Epoch 13 Results - Time: 396.94s, LR: 0.000842\n",
      "2025-04-17 23:45:35,658 [INFO] - Train - Loss: 1.2518, Acc: 99.27%\n",
      "2025-04-17 23:45:35,658 [INFO] -   CE: 0.0225, KL: 0.1101, Feat: 0.0374, Cal: 0.0057\n",
      "2025-04-17 23:45:35,659 [INFO] - Val - Loss: 0.0877, Acc: 97.44%, F1: 0.9742, ECE: 0.0138\n",
      "2025-04-17 23:45:36,146 [INFO] - New best model saved (Val Loss: 0.0877)\n",
      "2025-04-17 23:45:36,146 [INFO] - GPU Memory: Current=840.90MB, Peak=4296.33MB, Reserved=4678.00MB\n",
      "2025-04-17 23:45:36,156 [INFO] - Epoch 14/50\n",
      "2025-04-17 23:45:36,342 [INFO] - GPU cache cleared: 840.90MB → 840.90MB (freed 0.00MB)\n",
      "2025-04-17 23:45:36,343 [INFO] - Current calibration weight: 0.0467\n",
      "2025-04-17 23:45:36,345 [INFO] - Updated teacher weights: {'vit': 0.23521981096823905, 'efficientnet': 0.132836297195872, 'inception': 0.02407941163991258, 'mobilenet': 0.058510020599695255, 'resnet': 0.39663036341507096, 'densenet': 0.1527240961812101}\n",
      "2025-04-17 23:45:36,345 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:45:36,345 [INFO] - Current teacher temperatures: {'vit': 3.823667049407959, 'efficientnet': 4.1247758865356445, 'inception': 6.862956523895264, 'mobilenet': 4.160894393920898, 'resnet': 4.161348342895508, 'densenet': 4.137840270996094}\n",
      "2025-04-17 23:45:36,345 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:45:36,345 [INFO] - HFI attention weights: {'vit': 0.19854773581027985, 'efficientnet': 0.1776900291442871, 'inception': 0.1612025499343872, 'mobilenet': 0.15059317648410797, 'resnet': 0.15479566156864166, 'densenet': 0.1571708470582962}\n",
      "Training (cal_weight=0.0467): 100%|██████████| 704/704 [06:33<00:00,  1.79it/s, loss=1.20, acc=99.3%, ce=0.02, kl=0.11, feat=0.04, cal=0.01]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.51it/s]\n",
      "2025-04-17 23:52:12,779 [INFO] - Epoch 14 Results - Time: 396.62s, LR: 0.000819\n",
      "2025-04-17 23:52:12,780 [INFO] - Train - Loss: 1.2029, Acc: 99.32%\n",
      "2025-04-17 23:52:12,781 [INFO] -   CE: 0.0208, KL: 0.1059, Feat: 0.0357, Cal: 0.0053\n",
      "2025-04-17 23:52:12,782 [INFO] - Val - Loss: 0.0858, Acc: 97.58%, F1: 0.9756, ECE: 0.0131\n",
      "2025-04-17 23:52:13,290 [INFO] - New best model saved (Val Loss: 0.0858)\n",
      "2025-04-17 23:52:13,517 [INFO] - New best model saved (Val Acc: 97.58%)\n",
      "2025-04-17 23:52:13,517 [INFO] - GPU Memory: Current=840.72MB, Peak=4296.33MB, Reserved=4678.00MB\n",
      "2025-04-17 23:52:13,527 [INFO] - Epoch 15/50\n",
      "2025-04-17 23:52:13,716 [INFO] - GPU cache cleared: 840.72MB → 840.72MB (freed 0.00MB)\n",
      "2025-04-17 23:52:13,716 [INFO] - Current calibration weight: 0.0500\n",
      "2025-04-17 23:52:13,716 [INFO] - Updated teacher weights: {'vit': 0.2622214228683421, 'efficientnet': 0.13493053427067617, 'inception': 0.08838212182554224, 'mobilenet': 0.23487585837240088, 'resnet': 0.14623747066485623, 'densenet': 0.13335259199818245}\n",
      "2025-04-17 23:52:13,716 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:52:13,716 [INFO] - Current teacher temperatures: {'vit': 3.7337706089019775, 'efficientnet': 4.059850692749023, 'inception': 6.934475421905518, 'mobilenet': 4.093810081481934, 'resnet': 4.096569061279297, 'densenet': 4.067325592041016}\n",
      "2025-04-17 23:52:13,716 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:52:13,716 [INFO] - HFI attention weights: {'vit': 0.1993001401424408, 'efficientnet': 0.17862462997436523, 'inception': 0.1605190485715866, 'mobilenet': 0.15053033828735352, 'resnet': 0.15464237332344055, 'densenet': 0.15638349950313568}\n",
      "Training (cal_weight=0.0500): 100%|██████████| 704/704 [06:32<00:00,  1.79it/s, loss=1.16, acc=99.4%, ce=0.02, kl=0.10, feat=0.03, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.10it/s]\n",
      "2025-04-17 23:58:48,815 [INFO] - Epoch 15 Results - Time: 395.29s, LR: 0.000794\n",
      "2025-04-17 23:58:48,816 [INFO] - Train - Loss: 1.1575, Acc: 99.39%\n",
      "2025-04-17 23:58:48,817 [INFO] -   CE: 0.0180, KL: 0.1020, Feat: 0.0331, Cal: 0.0045\n",
      "2025-04-17 23:58:48,817 [INFO] - Val - Loss: 0.0869, Acc: 97.54%, F1: 0.9752, ECE: 0.0137\n",
      "2025-04-17 23:58:49,081 [INFO] - GPU Memory: Current=840.72MB, Peak=4296.33MB, Reserved=4676.00MB\n",
      "2025-04-17 23:58:49,081 [INFO] - Epoch 16/50\n",
      "2025-04-17 23:58:49,278 [INFO] - GPU cache cleared: 840.72MB → 840.72MB (freed 0.00MB)\n",
      "2025-04-17 23:58:49,278 [INFO] - Current calibration weight: 0.0533\n",
      "2025-04-17 23:58:49,278 [INFO] - Updated teacher weights: {'vit': 0.5182888525722573, 'efficientnet': 0.10874111106984777, 'inception': 0.03997607816979757, 'mobilenet': 0.10982214171601286, 'resnet': 0.10785233183307556, 'densenet': 0.11531948463900893}\n",
      "2025-04-17 23:58:49,278 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:58:49,283 [INFO] - Current teacher temperatures: {'vit': 3.647338390350342, 'efficientnet': 3.9969232082366943, 'inception': 7.00446081161499, 'mobilenet': 4.027655601501465, 'resnet': 4.034214019775391, 'densenet': 3.9988553524017334}\n",
      "2025-04-17 23:58:49,283 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-17 23:58:49,283 [INFO] - HFI attention weights: {'vit': 0.1998744010925293, 'efficientnet': 0.17959216237068176, 'inception': 0.15987363457679749, 'mobilenet': 0.15022152662277222, 'resnet': 0.15475182235240936, 'densenet': 0.15568648278713226}\n",
      "Training (cal_weight=0.0533): 100%|██████████| 704/704 [06:30<00:00,  1.80it/s, loss=1.12, acc=99.5%, ce=0.02, kl=0.10, feat=0.03, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.88it/s]\n",
      "2025-04-18 00:05:22,981 [INFO] - Epoch 16 Results - Time: 393.90s, LR: 0.000768\n",
      "2025-04-18 00:05:22,982 [INFO] - Train - Loss: 1.1197, Acc: 99.50%\n",
      "2025-04-18 00:05:22,982 [INFO] -   CE: 0.0162, KL: 0.0987, Feat: 0.0315, Cal: 0.0039\n",
      "2025-04-18 00:05:22,983 [INFO] - Val - Loss: 0.0867, Acc: 97.58%, F1: 0.9756, ECE: 0.0135\n",
      "2025-04-18 00:05:23,253 [INFO] - GPU Memory: Current=841.35MB, Peak=4296.33MB, Reserved=4680.00MB\n",
      "2025-04-18 00:05:23,253 [INFO] - Epoch 17/50\n",
      "2025-04-18 00:05:23,446 [INFO] - GPU cache cleared: 841.35MB → 841.35MB (freed 0.00MB)\n",
      "2025-04-18 00:05:23,447 [INFO] - Current calibration weight: 0.0567\n",
      "2025-04-18 00:05:23,448 [INFO] - Updated teacher weights: {'vit': 0.08439458938653671, 'efficientnet': 0.09592178650808451, 'inception': 0.033071974400673644, 'mobilenet': 0.3022022896224069, 'resnet': 0.2993387177055805, 'densenet': 0.18507064237671766}\n",
      "2025-04-18 00:05:23,448 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:05:23,450 [INFO] - Current teacher temperatures: {'vit': 3.5630900859832764, 'efficientnet': 3.93691349029541, 'inception': 7.0717878341674805, 'mobilenet': 3.9632408618927, 'resnet': 3.9723639488220215, 'densenet': 3.9321913719177246}\n",
      "2025-04-18 00:05:23,450 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:05:23,451 [INFO] - HFI attention weights: {'vit': 0.2005026489496231, 'efficientnet': 0.1803915798664093, 'inception': 0.15923379361629486, 'mobilenet': 0.1498507559299469, 'resnet': 0.1549024134874344, 'densenet': 0.15511883795261383}\n",
      "Training (cal_weight=0.0567): 100%|██████████| 704/704 [06:30<00:00,  1.80it/s, loss=1.10, acc=99.6%, ce=0.01, kl=0.10, feat=0.03, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.83it/s]\n",
      "2025-04-18 00:11:56,917 [INFO] - Epoch 17 Results - Time: 393.66s, LR: 0.000741\n",
      "2025-04-18 00:11:56,918 [INFO] - Train - Loss: 1.1050, Acc: 99.55%\n",
      "2025-04-18 00:11:56,919 [INFO] -   CE: 0.0142, KL: 0.0975, Feat: 0.0302, Cal: 0.0035\n",
      "2025-04-18 00:11:56,921 [INFO] - Val - Loss: 0.0901, Acc: 97.60%, F1: 0.9758, ECE: 0.0140\n",
      "2025-04-18 00:11:57,420 [INFO] - New best model saved (Val Acc: 97.60%)\n",
      "2025-04-18 00:11:57,420 [INFO] - GPU Memory: Current=840.72MB, Peak=4296.33MB, Reserved=4726.00MB\n",
      "2025-04-18 00:11:57,420 [INFO] - Epoch 18/50\n",
      "2025-04-18 00:11:57,612 [INFO] - GPU cache cleared: 840.72MB → 840.72MB (freed 0.00MB)\n",
      "2025-04-18 00:11:57,612 [INFO] - Current calibration weight: 0.0600\n",
      "2025-04-18 00:11:57,612 [INFO] - Updated teacher weights: {'vit': 0.23250192574605677, 'efficientnet': 0.2351020644406287, 'inception': 0.022063290902365047, 'mobilenet': 0.07259454894610207, 'resnet': 0.20231418359060513, 'densenet': 0.23542398637424233}\n",
      "2025-04-18 00:11:57,612 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:11:57,612 [INFO] - Current teacher temperatures: {'vit': 3.483564853668213, 'efficientnet': 3.8781545162200928, 'inception': 7.141489028930664, 'mobilenet': 3.899916887283325, 'resnet': 3.912879467010498, 'densenet': 3.866224527359009}\n",
      "2025-04-18 00:11:57,612 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:11:57,612 [INFO] - HFI attention weights: {'vit': 0.2010156810283661, 'efficientnet': 0.18143802881240845, 'inception': 0.15863433480262756, 'mobilenet': 0.14960098266601562, 'resnet': 0.15508326888084412, 'densenet': 0.1542276293039322}\n",
      "Training (cal_weight=0.0600): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=1.05, acc=99.6%, ce=0.01, kl=0.09, feat=0.03, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.20it/s]\n",
      "2025-04-18 00:18:30,310 [INFO] - Epoch 18 Results - Time: 392.89s, LR: 0.000713\n",
      "2025-04-18 00:18:30,310 [INFO] - Train - Loss: 1.0477, Acc: 99.57%\n",
      "2025-04-18 00:18:30,312 [INFO] -   CE: 0.0139, KL: 0.0924, Feat: 0.0288, Cal: 0.0033\n",
      "2025-04-18 00:18:30,312 [INFO] - Val - Loss: 0.0982, Acc: 97.40%, F1: 0.9739, ECE: 0.0159\n",
      "2025-04-18 00:18:30,591 [INFO] - GPU Memory: Current=840.63MB, Peak=4296.33MB, Reserved=4672.00MB\n",
      "2025-04-18 00:18:30,592 [INFO] - Epoch 19/50\n",
      "2025-04-18 00:18:30,780 [INFO] - GPU cache cleared: 840.63MB → 840.63MB (freed 0.00MB)\n",
      "2025-04-18 00:18:30,780 [INFO] - Current calibration weight: 0.0633\n",
      "2025-04-18 00:18:30,780 [INFO] - Updated teacher weights: {'vit': 0.19790271138519577, 'efficientnet': 0.2174295077228206, 'inception': 0.013563203982905992, 'mobilenet': 0.21752111484636089, 'resnet': 0.13585623118839024, 'densenet': 0.21772723087432644}\n",
      "2025-04-18 00:18:30,780 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:18:30,780 [INFO] - Current teacher temperatures: {'vit': 3.4076759815216064, 'efficientnet': 3.821101665496826, 'inception': 7.204405307769775, 'mobilenet': 3.8400776386260986, 'resnet': 3.8558993339538574, 'densenet': 3.8040177822113037}\n",
      "2025-04-18 00:18:30,780 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:18:30,786 [INFO] - HFI attention weights: {'vit': 0.201186403632164, 'efficientnet': 0.18238545954227448, 'inception': 0.1578870564699173, 'mobilenet': 0.14934206008911133, 'resnet': 0.15535052120685577, 'densenet': 0.15384848415851593}\n",
      "Training (cal_weight=0.0633): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=1.01, acc=99.5%, ce=0.01, kl=0.09, feat=0.03, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.47it/s]\n",
      "2025-04-18 00:25:03,403 [INFO] - Epoch 19 Results - Time: 392.81s, LR: 0.000684\n",
      "2025-04-18 00:25:03,404 [INFO] - Train - Loss: 1.0095, Acc: 99.55%\n",
      "2025-04-18 00:25:03,405 [INFO] -   CE: 0.0144, KL: 0.0890, Feat: 0.0275, Cal: 0.0035\n",
      "2025-04-18 00:25:03,406 [INFO] - Val - Loss: 0.0878, Acc: 97.42%, F1: 0.9740, ECE: 0.0144\n",
      "2025-04-18 00:25:03,658 [INFO] - GPU Memory: Current=841.25MB, Peak=4296.33MB, Reserved=4680.00MB\n",
      "2025-04-18 00:25:03,658 [INFO] - Epoch 20/50\n",
      "2025-04-18 00:25:03,859 [INFO] - GPU cache cleared: 841.25MB → 841.25MB (freed 0.00MB)\n",
      "2025-04-18 00:25:03,859 [INFO] - Current calibration weight: 0.0667\n",
      "2025-04-18 00:25:03,859 [INFO] - Updated teacher weights: {'vit': 0.1928459467782511, 'efficientnet': 0.19500260077885992, 'inception': 0.028560879878137966, 'mobilenet': 0.19508475902650219, 'resnet': 0.19323619845455173, 'densenet': 0.1952696150836972}\n",
      "2025-04-18 00:25:03,859 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:25:03,859 [INFO] - Current teacher temperatures: {'vit': 3.3340911865234375, 'efficientnet': 3.766674041748047, 'inception': 7.263909816741943, 'mobilenet': 3.781352996826172, 'resnet': 3.802657127380371, 'densenet': 3.7433509826660156}\n",
      "2025-04-18 00:25:03,859 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:25:03,859 [INFO] - HFI attention weights: {'vit': 0.20147474110126495, 'efficientnet': 0.18328259885311127, 'inception': 0.15727189183235168, 'mobilenet': 0.14909560978412628, 'resnet': 0.155551478266716, 'densenet': 0.15332363545894623}\n",
      "Training (cal_weight=0.0667): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=0.98, acc=99.6%, ce=0.01, kl=0.09, feat=0.03, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.47it/s]\n",
      "2025-04-18 00:31:36,171 [INFO] - Epoch 20 Results - Time: 392.51s, LR: 0.000655\n",
      "2025-04-18 00:31:36,172 [INFO] - Train - Loss: 0.9760, Acc: 99.64%\n",
      "2025-04-18 00:31:36,172 [INFO] -   CE: 0.0121, KL: 0.0861, Feat: 0.0262, Cal: 0.0028\n",
      "2025-04-18 00:31:36,173 [INFO] - Val - Loss: 0.0805, Acc: 97.54%, F1: 0.9753, ECE: 0.0125\n",
      "2025-04-18 00:31:36,664 [INFO] - New best model saved (Val Loss: 0.0805)\n",
      "2025-04-18 00:31:36,923 [INFO] - GPU Memory: Current=840.95MB, Peak=4296.33MB, Reserved=4672.00MB\n",
      "2025-04-18 00:31:36,924 [INFO] - Epoch 21/50\n",
      "2025-04-18 00:31:37,116 [INFO] - GPU cache cleared: 840.95MB → 840.95MB (freed 0.00MB)\n",
      "2025-04-18 00:31:37,116 [INFO] - Current calibration weight: 0.0700\n",
      "2025-04-18 00:31:37,116 [INFO] - Updated teacher weights: {'vit': 0.1896159617534577, 'efficientnet': 0.19173649386381164, 'inception': 0.04483155532095692, 'mobilenet': 0.1918172760394442, 'resnet': 0.1899996770877122, 'densenet': 0.19199903593461734}\n",
      "2025-04-18 00:31:37,116 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:31:37,116 [INFO] - Current teacher temperatures: {'vit': 3.2643661499023438, 'efficientnet': 3.715048313140869, 'inception': 7.322025775909424, 'mobilenet': 3.727614402770996, 'resnet': 3.749971866607666, 'densenet': 3.68483304977417}\n",
      "2025-04-18 00:31:37,116 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:31:37,116 [INFO] - HFI attention weights: {'vit': 0.20170722901821136, 'efficientnet': 0.18426978588104248, 'inception': 0.15669062733650208, 'mobilenet': 0.14875848591327667, 'resnet': 0.15579283237457275, 'densenet': 0.15278102457523346}\n",
      "Training (cal_weight=0.0700): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=0.94, acc=99.7%, ce=0.01, kl=0.08, feat=0.03, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.63it/s]\n",
      "2025-04-18 00:38:09,222 [INFO] - Epoch 21 Results - Time: 392.30s, LR: 0.000624\n",
      "2025-04-18 00:38:09,223 [INFO] - Train - Loss: 0.9425, Acc: 99.69%\n",
      "2025-04-18 00:38:09,224 [INFO] -   CE: 0.0114, KL: 0.0832, Feat: 0.0250, Cal: 0.0026\n",
      "2025-04-18 00:38:09,225 [INFO] - Val - Loss: 0.0807, Acc: 97.52%, F1: 0.9750, ECE: 0.0132\n",
      "2025-04-18 00:38:09,485 [INFO] - GPU Memory: Current=840.72MB, Peak=4296.33MB, Reserved=4734.00MB\n",
      "2025-04-18 00:38:09,491 [INFO] - Epoch 22/50\n",
      "2025-04-18 00:38:09,679 [INFO] - GPU cache cleared: 840.72MB → 840.72MB (freed 0.00MB)\n",
      "2025-04-18 00:38:09,680 [INFO] - Current calibration weight: 0.0733\n",
      "2025-04-18 00:38:09,680 [INFO] - Updated teacher weights: {'vit': 0.1771526633293424, 'efficientnet': 0.10943325515829323, 'inception': 0.03443023093335149, 'mobilenet': 0.38681941599929015, 'resnet': 0.14589738882015857, 'densenet': 0.14626704575956415}\n",
      "2025-04-18 00:38:09,680 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:38:09,680 [INFO] - Current teacher temperatures: {'vit': 3.1984481811523438, 'efficientnet': 3.6656346321105957, 'inception': 7.376290321350098, 'mobilenet': 3.675612211227417, 'resnet': 3.7003414630889893, 'densenet': 3.6292827129364014}\n",
      "2025-04-18 00:38:09,680 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:38:09,680 [INFO] - HFI attention weights: {'vit': 0.20169135928153992, 'efficientnet': 0.18514037132263184, 'inception': 0.15604303777217865, 'mobilenet': 0.14849671721458435, 'resnet': 0.15613867342472076, 'densenet': 0.1524897962808609}\n",
      "Training (cal_weight=0.0733): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.92, acc=99.7%, ce=0.01, kl=0.08, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.07it/s]\n",
      "2025-04-18 00:44:41,039 [INFO] - Epoch 22 Results - Time: 391.55s, LR: 0.000594\n",
      "2025-04-18 00:44:41,039 [INFO] - Train - Loss: 0.9194, Acc: 99.70%\n",
      "2025-04-18 00:44:41,040 [INFO] -   CE: 0.0105, KL: 0.0812, Feat: 0.0239, Cal: 0.0024\n",
      "2025-04-18 00:44:41,041 [INFO] - Val - Loss: 0.0800, Acc: 97.46%, F1: 0.9744, ECE: 0.0148\n",
      "2025-04-18 00:44:41,543 [INFO] - New best model saved (Val Loss: 0.0800)\n",
      "2025-04-18 00:44:41,545 [INFO] - GPU Memory: Current=840.69MB, Peak=4296.33MB, Reserved=4676.00MB\n",
      "2025-04-18 00:44:41,545 [INFO] - Epoch 23/50\n",
      "2025-04-18 00:44:41,734 [INFO] - GPU cache cleared: 840.69MB → 840.69MB (freed 0.00MB)\n",
      "2025-04-18 00:44:41,734 [INFO] - Current calibration weight: 0.0767\n",
      "2025-04-18 00:44:41,734 [INFO] - Updated teacher weights: {'vit': 0.20766105372445534, 'efficientnet': 0.20998338950473736, 'inception': 0.03856678076200712, 'mobilenet': 0.12543657330371205, 'resnet': 0.20808128591326827, 'densenet': 0.21027091679181986}\n",
      "2025-04-18 00:44:41,734 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:44:41,734 [INFO] - Current teacher temperatures: {'vit': 3.136962652206421, 'efficientnet': 3.6192421913146973, 'inception': 7.428650856018066, 'mobilenet': 3.6243395805358887, 'resnet': 3.652376890182495, 'densenet': 3.5769715309143066}\n",
      "2025-04-18 00:44:41,734 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:44:41,734 [INFO] - HFI attention weights: {'vit': 0.20178405940532684, 'efficientnet': 0.1861511915922165, 'inception': 0.15546590089797974, 'mobilenet': 0.14818376302719116, 'resnet': 0.15653151273727417, 'densenet': 0.1518835723400116}\n",
      "Training (cal_weight=0.0767): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.89, acc=99.7%, ce=0.01, kl=0.08, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.57it/s]\n",
      "2025-04-18 00:51:13,288 [INFO] - Epoch 23 Results - Time: 391.74s, LR: 0.000563\n",
      "2025-04-18 00:51:13,289 [INFO] - Train - Loss: 0.8884, Acc: 99.74%\n",
      "2025-04-18 00:51:13,290 [INFO] -   CE: 0.0101, KL: 0.0784, Feat: 0.0229, Cal: 0.0022\n",
      "2025-04-18 00:51:13,291 [INFO] - Val - Loss: 0.0849, Acc: 97.54%, F1: 0.9752, ECE: 0.0144\n",
      "2025-04-18 00:51:13,558 [INFO] - GPU Memory: Current=841.66MB, Peak=4296.33MB, Reserved=4680.00MB\n",
      "2025-04-18 00:51:13,558 [INFO] - Epoch 24/50\n",
      "2025-04-18 00:51:13,745 [INFO] - GPU cache cleared: 841.66MB → 841.66MB (freed 0.00MB)\n",
      "2025-04-18 00:51:13,745 [INFO] - Current calibration weight: 0.0800\n",
      "2025-04-18 00:51:13,745 [INFO] - Updated teacher weights: {'vit': 0.20017683206727493, 'efficientnet': 0.20446538289653599, 'inception': 0.025032580292174398, 'mobilenet': 0.1874196195196731, 'resnet': 0.1781602307691734, 'densenet': 0.20474535445516823}\n",
      "2025-04-18 00:51:13,745 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:51:13,745 [INFO] - Current teacher temperatures: {'vit': 3.079115867614746, 'efficientnet': 3.574533224105835, 'inception': 7.478969097137451, 'mobilenet': 3.5768775939941406, 'resnet': 3.6073555946350098, 'densenet': 3.5278267860412598}\n",
      "2025-04-18 00:51:13,755 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:51:13,755 [INFO] - HFI attention weights: {'vit': 0.20171137154102325, 'efficientnet': 0.18697749078273773, 'inception': 0.15490584075450897, 'mobilenet': 0.1480201631784439, 'resnet': 0.15694773197174072, 'densenet': 0.15143735706806183}\n",
      "Training (cal_weight=0.0800): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.86, acc=99.8%, ce=0.01, kl=0.08, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.45it/s]\n",
      "2025-04-18 00:57:45,333 [INFO] - Epoch 24 Results - Time: 391.78s, LR: 0.000531\n",
      "2025-04-18 00:57:45,335 [INFO] - Train - Loss: 0.8572, Acc: 99.75%\n",
      "2025-04-18 00:57:45,335 [INFO] -   CE: 0.0095, KL: 0.0757, Feat: 0.0222, Cal: 0.0021\n",
      "2025-04-18 00:57:45,336 [INFO] - Val - Loss: 0.0862, Acc: 97.62%, F1: 0.9760, ECE: 0.0149\n",
      "2025-04-18 00:57:45,824 [INFO] - New best model saved (Val Acc: 97.62%)\n",
      "2025-04-18 00:57:45,824 [INFO] - GPU Memory: Current=841.48MB, Peak=4296.33MB, Reserved=4678.00MB\n",
      "2025-04-18 00:57:45,824 [INFO] - Epoch 25/50\n",
      "2025-04-18 00:57:46,074 [INFO] - GPU cache cleared: 841.48MB → 841.48MB (freed 0.00MB)\n",
      "2025-04-18 00:57:46,075 [INFO] - Current calibration weight: 0.0833\n",
      "2025-04-18 00:57:46,075 [INFO] - Updated teacher weights: {'vit': 0.19352042057191657, 'efficientnet': 0.19568461741503632, 'inception': 0.03860271484679168, 'mobilenet': 0.18232764442711386, 'resnet': 0.1939120371435287, 'densenet': 0.195952565595613}\n",
      "2025-04-18 00:57:46,076 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:57:46,078 [INFO] - Current teacher temperatures: {'vit': 3.0252292156219482, 'efficientnet': 3.5321643352508545, 'inception': 7.526115417480469, 'mobilenet': 3.532027006149292, 'resnet': 3.565781354904175, 'densenet': 3.4805397987365723}\n",
      "2025-04-18 00:57:46,078 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 00:57:46,078 [INFO] - HFI attention weights: {'vit': 0.20157939195632935, 'efficientnet': 0.18791575729846954, 'inception': 0.15430809557437897, 'mobilenet': 0.14780376851558685, 'resnet': 0.157383531332016, 'densenet': 0.15100939571857452}\n",
      "Training (cal_weight=0.0833): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=0.84, acc=99.8%, ce=0.01, kl=0.07, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.23it/s]\n",
      "2025-04-18 01:04:18,261 [INFO] - Epoch 25 Results - Time: 392.44s, LR: 0.000500\n",
      "2025-04-18 01:04:18,261 [INFO] - Train - Loss: 0.8415, Acc: 99.76%\n",
      "2025-04-18 01:04:18,261 [INFO] -   CE: 0.0109, KL: 0.0743, Feat: 0.0211, Cal: 0.0022\n",
      "2025-04-18 01:04:18,263 [INFO] - Val - Loss: 0.0817, Acc: 97.54%, F1: 0.9752, ECE: 0.0134\n",
      "2025-04-18 01:04:18,532 [INFO] - GPU Memory: Current=841.60MB, Peak=4296.33MB, Reserved=4678.00MB\n",
      "2025-04-18 01:04:18,532 [INFO] - Epoch 26/50\n",
      "2025-04-18 01:04:18,729 [INFO] - GPU cache cleared: 841.60MB → 841.60MB (freed 0.00MB)\n",
      "2025-04-18 01:04:18,729 [INFO] - Current calibration weight: 0.0867\n",
      "2025-04-18 01:04:18,729 [INFO] - Updated teacher weights: {'vit': 0.15243410357194, 'efficientnet': 0.22948462950730786, 'inception': 0.015941871622111345, 'mobilenet': 0.22958131567941967, 'resnet': 0.14275922005255004, 'densenet': 0.2297988595666711}\n",
      "2025-04-18 01:04:18,729 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:04:18,729 [INFO] - Current teacher temperatures: {'vit': 2.9745566844940186, 'efficientnet': 3.493314266204834, 'inception': 7.569159507751465, 'mobilenet': 3.4895761013031006, 'resnet': 3.5250892639160156, 'densenet': 3.4359734058380127}\n",
      "2025-04-18 01:04:18,729 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:04:18,729 [INFO] - HFI attention weights: {'vit': 0.20145609974861145, 'efficientnet': 0.18891318142414093, 'inception': 0.15380632877349854, 'mobilenet': 0.14752550423145294, 'resnet': 0.1578465849161148, 'densenet': 0.15045234560966492}\n",
      "Training (cal_weight=0.0867): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=0.82, acc=99.8%, ce=0.01, kl=0.07, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.24it/s]\n",
      "2025-04-18 01:10:50,657 [INFO] - Epoch 26 Results - Time: 392.13s, LR: 0.000469\n",
      "2025-04-18 01:10:50,658 [INFO] - Train - Loss: 0.8155, Acc: 99.76%\n",
      "2025-04-18 01:10:50,659 [INFO] -   CE: 0.0091, KL: 0.0720, Feat: 0.0211, Cal: 0.0020\n",
      "2025-04-18 01:10:50,659 [INFO] - Val - Loss: 0.0784, Acc: 97.70%, F1: 0.9768, ECE: 0.0118\n",
      "2025-04-18 01:10:51,154 [INFO] - New best model saved (Val Loss: 0.0784)\n",
      "2025-04-18 01:10:51,392 [INFO] - New best model saved (Val Acc: 97.70%)\n",
      "2025-04-18 01:10:51,392 [INFO] - GPU Memory: Current=841.69MB, Peak=4296.33MB, Reserved=4680.00MB\n",
      "2025-04-18 01:10:51,392 [INFO] - Epoch 27/50\n",
      "2025-04-18 01:10:51,581 [INFO] - GPU cache cleared: 841.69MB → 841.69MB (freed 0.00MB)\n",
      "2025-04-18 01:10:51,581 [INFO] - Current calibration weight: 0.0900\n",
      "2025-04-18 01:10:51,581 [INFO] - Updated teacher weights: {'vit': 0.228538947918469, 'efficientnet': 0.11860354174592275, 'inception': 0.012754530411292835, 'mobilenet': 0.17969034813415247, 'resnet': 0.22900142954701844, 'densenet': 0.2314112022431446}\n",
      "2025-04-18 01:10:51,592 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:10:51,592 [INFO] - Current teacher temperatures: {'vit': 2.928088426589966, 'efficientnet': 3.4560248851776123, 'inception': 7.611278057098389, 'mobilenet': 3.4499988555908203, 'resnet': 3.4881675243377686, 'densenet': 3.393801212310791}\n",
      "2025-04-18 01:10:51,592 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:10:51,592 [INFO] - HFI attention weights: {'vit': 0.2013339251279831, 'efficientnet': 0.1896693855524063, 'inception': 0.15326747298240662, 'mobilenet': 0.1473790854215622, 'resnet': 0.1582300215959549, 'densenet': 0.1501201093196869}\n",
      "Training (cal_weight=0.0900): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.79, acc=99.8%, ce=0.01, kl=0.07, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.77it/s]\n",
      "2025-04-18 01:17:23,032 [INFO] - Epoch 27 Results - Time: 391.64s, LR: 0.000437\n",
      "2025-04-18 01:17:23,033 [INFO] - Train - Loss: 0.7925, Acc: 99.81%\n",
      "2025-04-18 01:17:23,034 [INFO] -   CE: 0.0083, KL: 0.0700, Feat: 0.0202, Cal: 0.0016\n",
      "2025-04-18 01:17:23,035 [INFO] - Val - Loss: 0.0832, Acc: 97.64%, F1: 0.9762, ECE: 0.0133\n",
      "2025-04-18 01:17:23,285 [INFO] - GPU Memory: Current=841.01MB, Peak=4296.33MB, Reserved=4672.00MB\n",
      "2025-04-18 01:17:23,285 [INFO] - Epoch 28/50\n",
      "2025-04-18 01:17:23,477 [INFO] - GPU cache cleared: 841.01MB → 841.01MB (freed 0.00MB)\n",
      "2025-04-18 01:17:23,477 [INFO] - Current calibration weight: 0.0933\n",
      "2025-04-18 01:17:23,477 [INFO] - Updated teacher weights: {'vit': 0.48466934305095494, 'efficientnet': 0.11749221094670804, 'inception': 0.039701376416756376, 'mobilenet': 0.12012929871765622, 'resnet': 0.12006433652055844, 'densenet': 0.11794343434736601}\n",
      "2025-04-18 01:17:23,477 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:17:23,488 [INFO] - Current teacher temperatures: {'vit': 2.884784698486328, 'efficientnet': 3.4215691089630127, 'inception': 7.650013446807861, 'mobilenet': 3.412886619567871, 'resnet': 3.4531331062316895, 'densenet': 3.354592800140381}\n",
      "2025-04-18 01:17:23,488 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:17:23,488 [INFO] - HFI attention weights: {'vit': 0.20120148360729218, 'efficientnet': 0.19034802913665771, 'inception': 0.15283022820949554, 'mobilenet': 0.14705558121204376, 'resnet': 0.15864910185337067, 'densenet': 0.14991554617881775}\n",
      "Training (cal_weight=0.0933): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=0.77, acc=99.8%, ce=0.01, kl=0.07, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.57it/s]\n",
      "2025-04-18 01:23:55,371 [INFO] - Epoch 28 Results - Time: 392.09s, LR: 0.000406\n",
      "2025-04-18 01:23:55,371 [INFO] - Train - Loss: 0.7701, Acc: 99.82%\n",
      "2025-04-18 01:23:55,371 [INFO] -   CE: 0.0076, KL: 0.0680, Feat: 0.0198, Cal: 0.0015\n",
      "2025-04-18 01:23:55,381 [INFO] - Val - Loss: 0.0796, Acc: 97.72%, F1: 0.9771, ECE: 0.0119\n",
      "2025-04-18 01:23:55,886 [INFO] - New best model saved (Val Acc: 97.72%)\n",
      "2025-04-18 01:23:55,886 [INFO] - GPU Memory: Current=841.58MB, Peak=4296.33MB, Reserved=4732.00MB\n",
      "2025-04-18 01:23:55,886 [INFO] - Epoch 29/50\n",
      "2025-04-18 01:23:56,080 [INFO] - GPU cache cleared: 841.58MB → 841.58MB (freed 0.00MB)\n",
      "2025-04-18 01:23:56,080 [INFO] - Current calibration weight: 0.0967\n",
      "2025-04-18 01:23:56,080 [INFO] - Updated teacher weights: {'vit': 0.2636469171356108, 'efficientnet': 0.2690159005252114, 'inception': 0.03565247086491135, 'mobilenet': 0.0663328383336716, 'resnet': 0.09596761294539946, 'densenet': 0.26938426019519535}\n",
      "2025-04-18 01:23:56,080 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:23:56,080 [INFO] - Current teacher temperatures: {'vit': 2.8444783687591553, 'efficientnet': 3.3889214992523193, 'inception': 7.684991836547852, 'mobilenet': 3.378549814224243, 'resnet': 3.4198310375213623, 'densenet': 3.319101095199585}\n",
      "2025-04-18 01:23:56,080 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:23:56,080 [INFO] - HFI attention weights: {'vit': 0.20101167261600494, 'efficientnet': 0.19127030670642853, 'inception': 0.1523279845714569, 'mobilenet': 0.14684052765369415, 'resnet': 0.1590871810913086, 'densenet': 0.14946229755878448}\n",
      "Training (cal_weight=0.0967): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.76, acc=99.9%, ce=0.01, kl=0.07, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.98it/s]\n",
      "2025-04-18 01:30:27,286 [INFO] - Epoch 29 Results - Time: 391.40s, LR: 0.000376\n",
      "2025-04-18 01:30:27,286 [INFO] - Train - Loss: 0.7587, Acc: 99.85%\n",
      "2025-04-18 01:30:27,287 [INFO] -   CE: 0.0076, KL: 0.0670, Feat: 0.0191, Cal: 0.0015\n",
      "2025-04-18 01:30:27,288 [INFO] - Val - Loss: 0.0784, Acc: 97.78%, F1: 0.9777, ECE: 0.0115\n",
      "2025-04-18 01:30:27,785 [INFO] - New best model saved (Val Acc: 97.78%)\n",
      "2025-04-18 01:30:27,785 [INFO] - GPU Memory: Current=841.35MB, Peak=4296.33MB, Reserved=4672.00MB\n",
      "2025-04-18 01:30:27,785 [INFO] - Epoch 30/50\n",
      "2025-04-18 01:30:27,974 [INFO] - GPU cache cleared: 841.35MB → 841.35MB (freed 0.00MB)\n",
      "2025-04-18 01:30:27,974 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 01:30:27,974 [INFO] - Updated teacher weights: {'vit': 0.27927633577313044, 'efficientnet': 0.2823995667089254, 'inception': 0.044058558462567406, 'mobilenet': 0.05987887529864416, 'resnet': 0.05160041131289921, 'densenet': 0.2827862524438333}\n",
      "2025-04-18 01:30:27,974 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:30:27,974 [INFO] - Current teacher temperatures: {'vit': 2.8078415393829346, 'efficientnet': 3.3593692779541016, 'inception': 7.719114780426025, 'mobilenet': 3.3466529846191406, 'resnet': 3.3889236450195312, 'densenet': 3.285221815109253}\n",
      "2025-04-18 01:30:27,974 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:30:27,974 [INFO] - HFI attention weights: {'vit': 0.20078958570957184, 'efficientnet': 0.19201156497001648, 'inception': 0.15190759301185608, 'mobilenet': 0.14659413695335388, 'resnet': 0.15961267054080963, 'densenet': 0.14908447861671448}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=0.73, acc=99.8%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.22it/s]\n",
      "2025-04-18 01:37:00,006 [INFO] - Epoch 30 Results - Time: 392.22s, LR: 0.000345\n",
      "2025-04-18 01:37:00,006 [INFO] - Train - Loss: 0.7329, Acc: 99.85%\n",
      "2025-04-18 01:37:00,007 [INFO] -   CE: 0.0067, KL: 0.0647, Feat: 0.0188, Cal: 0.0013\n",
      "2025-04-18 01:37:00,008 [INFO] - Val - Loss: 0.0786, Acc: 97.68%, F1: 0.9766, ECE: 0.0122\n",
      "2025-04-18 01:37:00,477 [INFO] - GPU Memory: Current=840.82MB, Peak=4296.33MB, Reserved=4638.00MB\n",
      "2025-04-18 01:37:00,477 [INFO] - Epoch 31/50\n",
      "2025-04-18 01:37:00,674 [INFO] - GPU cache cleared: 840.82MB → 840.82MB (freed 0.00MB)\n",
      "2025-04-18 01:37:00,675 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 01:37:00,676 [INFO] - Updated teacher weights: {'vit': 0.2022410714217437, 'efficientnet': 0.2045027939160757, 'inception': 0.015153143830884648, 'mobilenet': 0.20458895477300265, 'resnet': 0.16873121935720511, 'densenet': 0.2047828167010882}\n",
      "2025-04-18 01:37:00,676 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:37:00,677 [INFO] - Current teacher temperatures: {'vit': 2.7747321128845215, 'efficientnet': 3.3310766220092773, 'inception': 7.750066757202148, 'mobilenet': 3.316768169403076, 'resnet': 3.3609402179718018, 'densenet': 3.254368305206299}\n",
      "2025-04-18 01:37:00,677 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:37:00,677 [INFO] - HFI attention weights: {'vit': 0.20048242807388306, 'efficientnet': 0.1927691549062729, 'inception': 0.1514635682106018, 'mobilenet': 0.14641089737415314, 'resnet': 0.1600428968667984, 'densenet': 0.1488310694694519}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:29<00:00,  1.81it/s, loss=0.72, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.29it/s]\n",
      "2025-04-18 01:43:32,496 [INFO] - Epoch 31 Results - Time: 392.02s, LR: 0.000316\n",
      "2025-04-18 01:43:32,499 [INFO] - Train - Loss: 0.7247, Acc: 99.87%\n",
      "2025-04-18 01:43:32,500 [INFO] -   CE: 0.0071, KL: 0.0640, Feat: 0.0180, Cal: 0.0013\n",
      "2025-04-18 01:43:32,500 [INFO] - Val - Loss: 0.0784, Acc: 97.74%, F1: 0.9773, ECE: 0.0122\n",
      "2025-04-18 01:43:32,763 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 01:43:32,763 [INFO] - Epoch 32/50\n",
      "2025-04-18 01:43:32,961 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 01:43:32,961 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 01:43:32,961 [INFO] - Updated teacher weights: {'vit': 0.26159087767414324, 'efficientnet': 0.26451632683334925, 'inception': 0.03210300672214157, 'mobilenet': 0.0898642429992874, 'resnet': 0.2621202446648567, 'densenet': 0.08980530110622184}\n",
      "2025-04-18 01:43:32,961 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:43:32,961 [INFO] - Current teacher temperatures: {'vit': 2.744474411010742, 'efficientnet': 3.3054959774017334, 'inception': 7.779212474822998, 'mobilenet': 3.289931535720825, 'resnet': 3.335127353668213, 'densenet': 3.225797414779663}\n",
      "2025-04-18 01:43:32,961 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:43:32,961 [INFO] - HFI attention weights: {'vit': 0.2002420872449875, 'efficientnet': 0.19352741539478302, 'inception': 0.15111039578914642, 'mobilenet': 0.146210715174675, 'resnet': 0.1604723334312439, 'densenet': 0.14843705296516418}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.71, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.98it/s]\n",
      "2025-04-18 01:50:04,590 [INFO] - Epoch 32 Results - Time: 391.83s, LR: 0.000287\n",
      "2025-04-18 01:50:04,591 [INFO] - Train - Loss: 0.7085, Acc: 99.88%\n",
      "2025-04-18 01:50:04,592 [INFO] -   CE: 0.0065, KL: 0.0626, Feat: 0.0177, Cal: 0.0011\n",
      "2025-04-18 01:50:04,593 [INFO] - Val - Loss: 0.0748, Acc: 97.86%, F1: 0.9784, ECE: 0.0107\n",
      "2025-04-18 01:50:05,075 [INFO] - New best model saved (Val Loss: 0.0748)\n",
      "2025-04-18 01:50:05,295 [INFO] - New best model saved (Val Acc: 97.86%)\n",
      "2025-04-18 01:50:05,295 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 01:50:05,295 [INFO] - Epoch 33/50\n",
      "2025-04-18 01:50:05,492 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 01:50:05,492 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 01:50:05,492 [INFO] - Updated teacher weights: {'vit': 0.1386910535595028, 'efficientnet': 0.28268826518412854, 'inception': 0.023682163805150234, 'mobilenet': 0.12682948201734542, 'resnet': 0.1450336892040953, 'densenet': 0.2830753462297777}\n",
      "2025-04-18 01:50:05,492 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:50:05,492 [INFO] - Current teacher temperatures: {'vit': 2.7164230346679688, 'efficientnet': 3.2820706367492676, 'inception': 7.804623126983643, 'mobilenet': 3.2649288177490234, 'resnet': 3.311124086380005, 'densenet': 3.200137138366699}\n",
      "2025-04-18 01:50:05,492 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:50:05,492 [INFO] - HFI attention weights: {'vit': 0.19998379051685333, 'efficientnet': 0.19425395131111145, 'inception': 0.15072497725486755, 'mobilenet': 0.1460084766149521, 'resnet': 0.1609482765197754, 'densenet': 0.14808055758476257}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:27<00:00,  1.81it/s, loss=0.69, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.69it/s]\n",
      "2025-04-18 01:56:36,148 [INFO] - Epoch 33 Results - Time: 390.85s, LR: 0.000259\n",
      "2025-04-18 01:56:36,149 [INFO] - Train - Loss: 0.6930, Acc: 99.88%\n",
      "2025-04-18 01:56:36,149 [INFO] -   CE: 0.0066, KL: 0.0612, Feat: 0.0173, Cal: 0.0012\n",
      "2025-04-18 01:56:36,150 [INFO] - Val - Loss: 0.0751, Acc: 98.06%, F1: 0.9805, ECE: 0.0106\n",
      "2025-04-18 01:56:36,644 [INFO] - New best model saved (Val Acc: 98.06%)\n",
      "2025-04-18 01:56:36,644 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 01:56:36,644 [INFO] - Epoch 34/50\n",
      "2025-04-18 01:56:36,836 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 01:56:36,837 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 01:56:36,837 [INFO] - Updated teacher weights: {'vit': 0.18040230900399237, 'efficientnet': 0.200391474925101, 'inception': 0.044032814799490984, 'mobilenet': 0.20047590360634185, 'resnet': 0.17403162952594015, 'densenet': 0.20066586813913365}\n",
      "2025-04-18 01:56:36,838 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:56:36,840 [INFO] - Current teacher temperatures: {'vit': 2.69126558303833, 'efficientnet': 3.260392904281616, 'inception': 7.827913284301758, 'mobilenet': 3.24204683303833, 'resnet': 3.2898519039154053, 'densenet': 3.1769628524780273}\n",
      "2025-04-18 01:56:36,840 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 01:56:36,840 [INFO] - HFI attention weights: {'vit': 0.19969947636127472, 'efficientnet': 0.19490434229373932, 'inception': 0.1503610461950302, 'mobilenet': 0.14581456780433655, 'resnet': 0.16133877635002136, 'densenet': 0.14788176119327545}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.69, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.19it/s]\n",
      "2025-04-18 02:03:07,762 [INFO] - Epoch 34 Results - Time: 391.12s, LR: 0.000232\n",
      "2025-04-18 02:03:07,763 [INFO] - Train - Loss: 0.6866, Acc: 99.93%\n",
      "2025-04-18 02:03:07,763 [INFO] -   CE: 0.0055, KL: 0.0607, Feat: 0.0168, Cal: 0.0008\n",
      "2025-04-18 02:03:07,764 [INFO] - Val - Loss: 0.0735, Acc: 97.92%, F1: 0.9790, ECE: 0.0117\n",
      "2025-04-18 02:03:08,267 [INFO] - New best model saved (Val Loss: 0.0735)\n",
      "2025-04-18 02:03:08,267 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:03:08,267 [INFO] - Epoch 35/50\n",
      "2025-04-18 02:03:08,453 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:03:08,453 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:03:08,453 [INFO] - Updated teacher weights: {'vit': 0.3155924615105881, 'efficientnet': 0.15937072040279496, 'inception': 0.07246030276341534, 'mobilenet': 0.15400766360621201, 'resnet': 0.12869873142900876, 'densenet': 0.16987012028798087}\n",
      "2025-04-18 02:03:08,453 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:03:08,463 [INFO] - Current teacher temperatures: {'vit': 2.6689445972442627, 'efficientnet': 3.2411906719207764, 'inception': 7.849041938781738, 'mobilenet': 3.2215332984924316, 'resnet': 3.270482301712036, 'densenet': 3.155381679534912}\n",
      "2025-04-18 02:03:08,464 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:03:08,465 [INFO] - HFI attention weights: {'vit': 0.19944778084754944, 'efficientnet': 0.1954539567232132, 'inception': 0.15007542073726654, 'mobilenet': 0.14564664661884308, 'resnet': 0.16173961758613586, 'densenet': 0.14763659238815308}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.68, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.64it/s]\n",
      "2025-04-18 02:09:40,245 [INFO] - Epoch 35 Results - Time: 391.98s, LR: 0.000206\n",
      "2025-04-18 02:09:40,245 [INFO] - Train - Loss: 0.6785, Acc: 99.89%\n",
      "2025-04-18 02:09:40,246 [INFO] -   CE: 0.0060, KL: 0.0600, Feat: 0.0165, Cal: 0.0010\n",
      "2025-04-18 02:09:40,247 [INFO] - Val - Loss: 0.0763, Acc: 97.92%, F1: 0.9791, ECE: 0.0114\n",
      "2025-04-18 02:09:40,511 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:09:40,521 [INFO] - Epoch 36/50\n",
      "2025-04-18 02:09:40,712 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:09:40,717 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:09:40,717 [INFO] - Updated teacher weights: {'vit': 0.10825845104283807, 'efficientnet': 0.23245747467571012, 'inception': 0.025131220218875316, 'mobilenet': 0.23255541336316568, 'resnet': 0.2303517928954161, 'densenet': 0.1712456478039947}\n",
      "2025-04-18 02:09:40,717 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:09:40,717 [INFO] - Current teacher temperatures: {'vit': 2.6491682529449463, 'efficientnet': 3.2240662574768066, 'inception': 7.868402004241943, 'mobilenet': 3.202949047088623, 'resnet': 3.2526326179504395, 'densenet': 3.1362035274505615}\n",
      "2025-04-18 02:09:40,717 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:09:40,717 [INFO] - HFI attention weights: {'vit': 0.19918322563171387, 'efficientnet': 0.19612015783786774, 'inception': 0.14978137612342834, 'mobilenet': 0.1454627960920334, 'resnet': 0.16215483844280243, 'densenet': 0.147297665476799}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.67, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.13it/s]\n",
      "2025-04-18 02:16:12,184 [INFO] - Epoch 36 Results - Time: 391.66s, LR: 0.000181\n",
      "2025-04-18 02:16:12,185 [INFO] - Train - Loss: 0.6713, Acc: 99.91%\n",
      "2025-04-18 02:16:12,186 [INFO] -   CE: 0.0058, KL: 0.0593, Feat: 0.0163, Cal: 0.0009\n",
      "2025-04-18 02:16:12,186 [INFO] - Val - Loss: 0.0725, Acc: 97.78%, F1: 0.9776, ECE: 0.0109\n",
      "2025-04-18 02:16:12,679 [INFO] - New best model saved (Val Loss: 0.0725)\n",
      "2025-04-18 02:16:12,681 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:16:12,682 [INFO] - Epoch 37/50\n",
      "2025-04-18 02:16:12,869 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:16:12,869 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:16:12,869 [INFO] - Updated teacher weights: {'vit': 0.1960846912473556, 'efficientnet': 0.1982775650977095, 'inception': 0.01224607974918184, 'mobilenet': 0.19836110314915156, 'resnet': 0.19648149699170536, 'densenet': 0.19854906376489612}\n",
      "2025-04-18 02:16:12,877 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:16:12,879 [INFO] - Current teacher temperatures: {'vit': 2.631621837615967, 'efficientnet': 3.2082765102386475, 'inception': 7.885851860046387, 'mobilenet': 3.1863887310028076, 'resnet': 3.2373976707458496, 'densenet': 3.119316577911377}\n",
      "2025-04-18 02:16:12,879 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:16:12,880 [INFO] - HFI attention weights: {'vit': 0.1989281177520752, 'efficientnet': 0.1966870129108429, 'inception': 0.14951391518115997, 'mobilenet': 0.1453585922718048, 'resnet': 0.16248780488967896, 'densenet': 0.14702461659908295}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.66, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.93it/s]\n",
      "2025-04-18 02:22:43,722 [INFO] - Epoch 37 Results - Time: 391.04s, LR: 0.000158\n",
      "2025-04-18 02:22:43,722 [INFO] - Train - Loss: 0.6592, Acc: 99.92%\n",
      "2025-04-18 02:22:43,722 [INFO] -   CE: 0.0059, KL: 0.0583, Feat: 0.0161, Cal: 0.0010\n",
      "2025-04-18 02:22:43,724 [INFO] - Val - Loss: 0.0713, Acc: 97.92%, F1: 0.9791, ECE: 0.0098\n",
      "2025-04-18 02:22:44,207 [INFO] - New best model saved (Val Loss: 0.0713)\n",
      "2025-04-18 02:22:44,207 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:22:44,207 [INFO] - Epoch 38/50\n",
      "2025-04-18 02:22:44,406 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:22:44,406 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:22:44,406 [INFO] - Updated teacher weights: {'vit': 0.22016522799011243, 'efficientnet': 0.22262740169753195, 'inception': 0.012844631189679734, 'mobilenet': 0.10081973268610367, 'resnet': 0.2206107641847883, 'densenet': 0.22293224225178385}\n",
      "2025-04-18 02:22:44,406 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:22:44,406 [INFO] - Current teacher temperatures: {'vit': 2.6161272525787354, 'efficientnet': 3.1945459842681885, 'inception': 7.900829792022705, 'mobilenet': 3.171881675720215, 'resnet': 3.223482847213745, 'densenet': 3.1046605110168457}\n",
      "2025-04-18 02:22:44,406 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:22:44,406 [INFO] - HFI attention weights: {'vit': 0.1987295001745224, 'efficientnet': 0.19712059199810028, 'inception': 0.14930729568004608, 'mobilenet': 0.14521047472953796, 'resnet': 0.16285423934459686, 'densenet': 0.14677786827087402}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.65, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.87it/s]\n",
      "2025-04-18 02:29:15,791 [INFO] - Epoch 38 Results - Time: 391.58s, LR: 0.000136\n",
      "2025-04-18 02:29:15,792 [INFO] - Train - Loss: 0.6530, Acc: 99.91%\n",
      "2025-04-18 02:29:15,792 [INFO] -   CE: 0.0056, KL: 0.0577, Feat: 0.0158, Cal: 0.0009\n",
      "2025-04-18 02:29:15,793 [INFO] - Val - Loss: 0.0735, Acc: 97.82%, F1: 0.9781, ECE: 0.0107\n",
      "2025-04-18 02:29:16,062 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:29:16,062 [INFO] - Epoch 39/50\n",
      "2025-04-18 02:29:16,261 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:29:16,265 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:29:16,265 [INFO] - Updated teacher weights: {'vit': 0.24006357465890457, 'efficientnet': 0.2427482775387837, 'inception': 0.01830471116086361, 'mobilenet': 0.1985756681812561, 'resnet': 0.05722709913628045, 'densenet': 0.24308066932391154}\n",
      "2025-04-18 02:29:16,265 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:29:16,265 [INFO] - Current teacher temperatures: {'vit': 2.602609634399414, 'efficientnet': 3.18257999420166, 'inception': 7.913568496704102, 'mobilenet': 3.1590209007263184, 'resnet': 3.211421489715576, 'densenet': 3.0918960571289062}\n",
      "2025-04-18 02:29:16,265 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:29:16,265 [INFO] - HFI attention weights: {'vit': 0.19852569699287415, 'efficientnet': 0.19758687913417816, 'inception': 0.1491101235151291, 'mobilenet': 0.14506183564662933, 'resnet': 0.16318099200725555, 'densenet': 0.1465345323085785}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.65, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.73it/s]\n",
      "2025-04-18 02:35:47,417 [INFO] - Epoch 39 Results - Time: 391.35s, LR: 0.000115\n",
      "2025-04-18 02:35:47,417 [INFO] - Train - Loss: 0.6473, Acc: 99.92%\n",
      "2025-04-18 02:35:47,417 [INFO] -   CE: 0.0062, KL: 0.0572, Feat: 0.0156, Cal: 0.0010\n",
      "2025-04-18 02:35:47,424 [INFO] - Val - Loss: 0.0720, Acc: 98.00%, F1: 0.9799, ECE: 0.0106\n",
      "2025-04-18 02:35:47,682 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:35:47,689 [INFO] - Epoch 40/50\n",
      "2025-04-18 02:35:47,880 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:35:47,880 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:35:47,880 [INFO] - Updated teacher weights: {'vit': 0.1938632443372685, 'efficientnet': 0.19603127508126822, 'inception': 0.023436361255144186, 'mobilenet': 0.19611386672865871, 'resnet': 0.1942555546623732, 'densenet': 0.1962996979352872}\n",
      "2025-04-18 02:35:47,880 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:35:47,880 [INFO] - Current teacher temperatures: {'vit': 2.591071128845215, 'efficientnet': 3.1723756790161133, 'inception': 7.924449443817139, 'mobilenet': 3.1482491493225098, 'resnet': 3.20110821723938, 'densenet': 3.08093523979187}\n",
      "2025-04-18 02:35:47,880 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:35:47,890 [INFO] - HFI attention weights: {'vit': 0.19832541048526764, 'efficientnet': 0.19797885417938232, 'inception': 0.1489231437444687, 'mobilenet': 0.14496301114559174, 'resnet': 0.16347022354602814, 'densenet': 0.1463393121957779}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.64, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.26it/s]\n",
      "2025-04-18 02:42:18,804 [INFO] - Epoch 40 Results - Time: 391.12s, LR: 0.000095\n",
      "2025-04-18 02:42:18,805 [INFO] - Train - Loss: 0.6387, Acc: 99.94%\n",
      "2025-04-18 02:42:18,806 [INFO] -   CE: 0.0055, KL: 0.0565, Feat: 0.0154, Cal: 0.0008\n",
      "2025-04-18 02:42:18,807 [INFO] - Val - Loss: 0.0714, Acc: 97.88%, F1: 0.9787, ECE: 0.0099\n",
      "2025-04-18 02:42:19,301 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:42:19,301 [INFO] - Epoch 41/50\n",
      "2025-04-18 02:42:19,492 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:42:19,498 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:42:19,498 [INFO] - Updated teacher weights: {'vit': 0.2288590182276673, 'efficientnet': 0.218457671536114, 'inception': 0.016007191507170875, 'mobilenet': 0.2315159181091047, 'resnet': 0.07342490545640779, 'densenet': 0.23173529516353528}\n",
      "2025-04-18 02:42:19,498 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:42:19,500 [INFO] - Current teacher temperatures: {'vit': 2.5813064575195312, 'efficientnet': 3.1635422706604004, 'inception': 7.933732986450195, 'mobilenet': 3.1390273571014404, 'resnet': 3.1923325061798096, 'densenet': 3.071462631225586}\n",
      "2025-04-18 02:42:19,500 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:42:19,500 [INFO] - HFI attention weights: {'vit': 0.19813115894794464, 'efficientnet': 0.19833549857139587, 'inception': 0.14875288307666779, 'mobilenet': 0.14486807584762573, 'resnet': 0.1637335568666458, 'densenet': 0.14617885649204254}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.63, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.64it/s]\n",
      "2025-04-18 02:48:50,517 [INFO] - Epoch 41 Results - Time: 391.22s, LR: 0.000078\n",
      "2025-04-18 02:48:50,519 [INFO] - Train - Loss: 0.6340, Acc: 99.89%\n",
      "2025-04-18 02:48:50,519 [INFO] -   CE: 0.0063, KL: 0.0560, Feat: 0.0152, Cal: 0.0010\n",
      "2025-04-18 02:48:50,519 [INFO] - Val - Loss: 0.0702, Acc: 97.92%, F1: 0.9791, ECE: 0.0101\n",
      "2025-04-18 02:48:51,009 [INFO] - New best model saved (Val Loss: 0.0702)\n",
      "2025-04-18 02:48:51,009 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:48:51,009 [INFO] - Epoch 42/50\n",
      "2025-04-18 02:48:51,203 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:48:51,203 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:48:51,206 [INFO] - Updated teacher weights: {'vit': 0.17518435846650593, 'efficientnet': 0.24881931754712738, 'inception': 0.01774287010514365, 'mobilenet': 0.248924149785403, 'resnet': 0.06016928177429699, 'densenet': 0.24916002232152304}\n",
      "2025-04-18 02:48:51,206 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:48:51,206 [INFO] - Current teacher temperatures: {'vit': 2.5732908248901367, 'efficientnet': 3.1560821533203125, 'inception': 7.941520690917969, 'mobilenet': 3.131351947784424, 'resnet': 3.1850619316101074, 'densenet': 3.063584804534912}\n",
      "2025-04-18 02:48:51,206 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:48:51,206 [INFO] - HFI attention weights: {'vit': 0.19794410467147827, 'efficientnet': 0.1986626833677292, 'inception': 0.14859656989574432, 'mobilenet': 0.14478205144405365, 'resnet': 0.16395999491214752, 'densenet': 0.14605461061000824}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.63, acc=99.9%, ce=0.01, kl=0.06, feat=0.02, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.83it/s]\n",
      "2025-04-18 02:55:22,335 [INFO] - Epoch 42 Results - Time: 391.33s, LR: 0.000062\n",
      "2025-04-18 02:55:22,336 [INFO] - Train - Loss: 0.6273, Acc: 99.95%\n",
      "2025-04-18 02:55:22,337 [INFO] -   CE: 0.0051, KL: 0.0555, Feat: 0.0151, Cal: 0.0007\n",
      "2025-04-18 02:55:22,338 [INFO] - Val - Loss: 0.0690, Acc: 98.02%, F1: 0.9801, ECE: 0.0098\n",
      "2025-04-18 02:55:22,839 [INFO] - New best model saved (Val Loss: 0.0690)\n",
      "2025-04-18 02:55:22,839 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 02:55:22,849 [INFO] - Epoch 43/50\n",
      "2025-04-18 02:55:23,044 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 02:55:23,045 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 02:55:23,046 [INFO] - Updated teacher weights: {'vit': 0.2348352504612492, 'efficientnet': 0.05248100568068286, 'inception': 0.03551179968924138, 'mobilenet': 0.2040748345219496, 'resnet': 0.23531047356900972, 'densenet': 0.2377866360778673}\n",
      "2025-04-18 02:55:23,046 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:55:23,046 [INFO] - Current teacher temperatures: {'vit': 2.5667285919189453, 'efficientnet': 3.149959087371826, 'inception': 7.947849273681641, 'mobilenet': 3.1251609325408936, 'resnet': 3.1789824962615967, 'densenet': 3.0572080612182617}\n",
      "2025-04-18 02:55:23,046 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 02:55:23,046 [INFO] - HFI attention weights: {'vit': 0.19779494404792786, 'efficientnet': 0.19891484081745148, 'inception': 0.14847908914089203, 'mobilenet': 0.14470426738262177, 'resnet': 0.1641567349433899, 'densenet': 0.1459500789642334}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.63, acc=99.9%, ce=0.01, kl=0.06, feat=0.01, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.99it/s]\n",
      "2025-04-18 03:01:54,084 [INFO] - Epoch 43 Results - Time: 391.23s, LR: 0.000048\n",
      "2025-04-18 03:01:54,086 [INFO] - Train - Loss: 0.6291, Acc: 99.94%\n",
      "2025-04-18 03:01:54,086 [INFO] -   CE: 0.0052, KL: 0.0556, Feat: 0.0149, Cal: 0.0007\n",
      "2025-04-18 03:01:54,087 [INFO] - Val - Loss: 0.0683, Acc: 97.92%, F1: 0.9791, ECE: 0.0099\n",
      "2025-04-18 03:01:54,569 [INFO] - New best model saved (Val Loss: 0.0683)\n",
      "2025-04-18 03:01:54,569 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 03:01:54,569 [INFO] - Epoch 44/50\n",
      "2025-04-18 03:01:54,763 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 03:01:54,763 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 03:01:54,763 [INFO] - Updated teacher weights: {'vit': 0.1814486811126279, 'efficientnet': 0.18347787607660979, 'inception': 0.08597328555726394, 'mobilenet': 0.18355517874190436, 'resnet': 0.181815868772777, 'densenet': 0.18372910973881706}\n",
      "2025-04-18 03:01:54,763 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:01:54,774 [INFO] - Current teacher temperatures: {'vit': 2.5615267753601074, 'efficientnet': 3.145233392715454, 'inception': 7.952969551086426, 'mobilenet': 3.120157241821289, 'resnet': 3.1740338802337646, 'densenet': 3.052077293395996}\n",
      "2025-04-18 03:01:54,774 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:01:54,774 [INFO] - HFI attention weights: {'vit': 0.19768832623958588, 'efficientnet': 0.19910885393619537, 'inception': 0.14839275181293488, 'mobilenet': 0.144627183675766, 'resnet': 0.16431359946727753, 'densenet': 0.14586926996707916}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.62, acc=100.0%, ce=0.01, kl=0.05, feat=0.01, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 28.62it/s]\n",
      "2025-04-18 03:08:25,708 [INFO] - Epoch 44 Results - Time: 391.14s, LR: 0.000035\n",
      "2025-04-18 03:08:25,708 [INFO] - Train - Loss: 0.6217, Acc: 99.95%\n",
      "2025-04-18 03:08:25,708 [INFO] -   CE: 0.0052, KL: 0.0550, Feat: 0.0149, Cal: 0.0007\n",
      "2025-04-18 03:08:25,713 [INFO] - Val - Loss: 0.0684, Acc: 98.02%, F1: 0.9801, ECE: 0.0094\n",
      "2025-04-18 03:08:25,970 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 03:08:25,970 [INFO] - Epoch 45/50\n",
      "2025-04-18 03:08:26,157 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 03:08:26,157 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 03:08:26,157 [INFO] - Updated teacher weights: {'vit': 0.22167629205810568, 'efficientnet': 0.22415536444772133, 'inception': 0.018809587212228595, 'mobilenet': 0.08877157295261551, 'resnet': 0.22212488610955994, 'densenet': 0.22446229721976893}\n",
      "2025-04-18 03:08:26,167 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:08:26,167 [INFO] - Current teacher temperatures: {'vit': 2.5575995445251465, 'efficientnet': 3.141484260559082, 'inception': 7.956943035125732, 'mobilenet': 3.116288661956787, 'resnet': 3.1702721118927, 'densenet': 3.048156976699829}\n",
      "2025-04-18 03:08:26,167 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:08:26,167 [INFO] - HFI attention weights: {'vit': 0.1975814551115036, 'efficientnet': 0.19929280877113342, 'inception': 0.1483037918806076, 'mobilenet': 0.14457732439041138, 'resnet': 0.16445152461528778, 'densenet': 0.1457931250333786}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.62, acc=99.9%, ce=0.01, kl=0.06, feat=0.01, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.49it/s]\n",
      "2025-04-18 03:14:56,972 [INFO] - Epoch 45 Results - Time: 391.00s, LR: 0.000024\n",
      "2025-04-18 03:14:56,973 [INFO] - Train - Loss: 0.6240, Acc: 99.94%\n",
      "2025-04-18 03:14:56,974 [INFO] -   CE: 0.0053, KL: 0.0552, Feat: 0.0148, Cal: 0.0007\n",
      "2025-04-18 03:14:56,974 [INFO] - Val - Loss: 0.0678, Acc: 98.00%, F1: 0.9799, ECE: 0.0098\n",
      "2025-04-18 03:14:57,538 [INFO] - New best model saved (Val Loss: 0.0678)\n",
      "2025-04-18 03:14:57,538 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 03:14:57,549 [INFO] - Epoch 46/50\n",
      "2025-04-18 03:14:57,738 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 03:14:57,738 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 03:14:57,738 [INFO] - Updated teacher weights: {'vit': 0.23025984038294242, 'efficientnet': 0.2328349051651566, 'inception': 0.023702780985146304, 'mobilenet': 0.04932294627074282, 'resnet': 0.23072580448639068, 'densenet': 0.23315372270962118}\n",
      "2025-04-18 03:14:57,738 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:14:57,738 [INFO] - Current teacher temperatures: {'vit': 2.554654121398926, 'efficientnet': 3.1387243270874023, 'inception': 7.959887981414795, 'mobilenet': 3.1134607791900635, 'resnet': 3.1674928665161133, 'densenet': 3.045166492462158}\n",
      "2025-04-18 03:14:57,738 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:14:57,738 [INFO] - HFI attention weights: {'vit': 0.19750775396823883, 'efficientnet': 0.1994241178035736, 'inception': 0.1482469141483307, 'mobilenet': 0.14453090727329254, 'resnet': 0.16455969214439392, 'densenet': 0.14573058485984802}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:27<00:00,  1.81it/s, loss=0.62, acc=99.9%, ce=0.01, kl=0.05, feat=0.01, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.16it/s]\n",
      "2025-04-18 03:21:28,469 [INFO] - Epoch 46 Results - Time: 390.92s, LR: 0.000016\n",
      "2025-04-18 03:21:28,470 [INFO] - Train - Loss: 0.6217, Acc: 99.94%\n",
      "2025-04-18 03:21:28,470 [INFO] -   CE: 0.0056, KL: 0.0550, Feat: 0.0147, Cal: 0.0008\n",
      "2025-04-18 03:21:28,472 [INFO] - Val - Loss: 0.0680, Acc: 97.98%, F1: 0.9797, ECE: 0.0096\n",
      "2025-04-18 03:21:28,727 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 03:21:28,727 [INFO] - Epoch 47/50\n",
      "2025-04-18 03:21:28,921 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 03:21:28,921 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 03:21:28,921 [INFO] - Updated teacher weights: {'vit': 0.16043712388926767, 'efficientnet': 0.18266870493046514, 'inception': 0.11442856934432563, 'mobilenet': 0.1302585571359645, 'resnet': 0.29212039085348107, 'densenet': 0.12008665384649604}\n",
      "2025-04-18 03:21:28,927 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:21:28,927 [INFO] - Current teacher temperatures: {'vit': 2.552567481994629, 'efficientnet': 3.136788845062256, 'inception': 7.961921691894531, 'mobilenet': 3.1114306449890137, 'resnet': 3.165517568588257, 'densenet': 3.0431618690490723}\n",
      "2025-04-18 03:21:28,927 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:21:28,927 [INFO] - HFI attention weights: {'vit': 0.1974576711654663, 'efficientnet': 0.1995120495557785, 'inception': 0.14820873737335205, 'mobilenet': 0.14449894428253174, 'resnet': 0.16463759541511536, 'densenet': 0.14568498730659485}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.63, acc=99.9%, ce=0.01, kl=0.06, feat=0.01, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.19it/s]\n",
      "2025-04-18 03:28:00,134 [INFO] - Epoch 47 Results - Time: 391.41s, LR: 0.000009\n",
      "2025-04-18 03:28:00,143 [INFO] - Train - Loss: 0.6293, Acc: 99.95%\n",
      "2025-04-18 03:28:00,143 [INFO] -   CE: 0.0062, KL: 0.0556, Feat: 0.0147, Cal: 0.0009\n",
      "2025-04-18 03:28:00,143 [INFO] - Val - Loss: 0.0685, Acc: 97.94%, F1: 0.9793, ECE: 0.0097\n",
      "2025-04-18 03:28:00,408 [INFO] - GPU Memory: Current=840.79MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 03:28:00,408 [INFO] - Epoch 48/50\n",
      "2025-04-18 03:28:00,594 [INFO] - GPU cache cleared: 840.79MB → 840.79MB (freed 0.00MB)\n",
      "2025-04-18 03:28:00,604 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 03:28:00,604 [INFO] - Updated teacher weights: {'vit': 0.2610765863096313, 'efficientnet': 0.12601862651842036, 'inception': 0.03247128797655498, 'mobilenet': 0.27433343219466827, 'resnet': 0.203886666918535, 'densenet': 0.10221340008219}\n",
      "2025-04-18 03:28:00,604 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:28:00,604 [INFO] - Current teacher temperatures: {'vit': 2.5512399673461914, 'efficientnet': 3.1355252265930176, 'inception': 7.963255882263184, 'mobilenet': 3.1101162433624268, 'resnet': 3.1642444133758545, 'densenet': 3.0418436527252197}\n",
      "2025-04-18 03:28:00,604 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:28:00,604 [INFO] - HFI attention weights: {'vit': 0.19742190837860107, 'efficientnet': 0.1995706409215927, 'inception': 0.14818255603313446, 'mobilenet': 0.1444811075925827, 'resnet': 0.16469012200832367, 'densenet': 0.14565369486808777}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:27<00:00,  1.82it/s, loss=0.62, acc=100.0%, ce=0.00, kl=0.05, feat=0.01, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.36it/s]\n",
      "2025-04-18 03:34:30,896 [INFO] - Epoch 48 Results - Time: 390.49s, LR: 0.000004\n",
      "2025-04-18 03:34:30,896 [INFO] - Train - Loss: 0.6197, Acc: 99.97%\n",
      "2025-04-18 03:34:30,896 [INFO] -   CE: 0.0049, KL: 0.0548, Feat: 0.0146, Cal: 0.0006\n",
      "2025-04-18 03:34:30,898 [INFO] - Val - Loss: 0.0676, Acc: 98.02%, F1: 0.9801, ECE: 0.0094\n",
      "2025-04-18 03:34:31,428 [INFO] - New best model saved (Val Loss: 0.0676)\n",
      "2025-04-18 03:34:31,428 [INFO] - GPU Memory: Current=841.60MB, Peak=4296.33MB, Reserved=4674.00MB\n",
      "2025-04-18 03:34:31,430 [INFO] - Epoch 49/50\n",
      "2025-04-18 03:34:31,616 [INFO] - GPU cache cleared: 841.60MB → 841.60MB (freed 0.00MB)\n",
      "2025-04-18 03:34:31,616 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 03:34:31,616 [INFO] - Updated teacher weights: {'vit': 0.1920451209915752, 'efficientnet': 0.19419281911747951, 'inception': 0.0325949470212438, 'mobilenet': 0.19427463618894256, 'resnet': 0.19243375208102453, 'densenet': 0.1944587245997343}\n",
      "2025-04-18 03:34:31,626 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:34:31,626 [INFO] - Current teacher temperatures: {'vit': 2.5504884719848633, 'efficientnet': 3.1348161697387695, 'inception': 7.963999271392822, 'mobilenet': 3.109372615814209, 'resnet': 3.163522720336914, 'densenet': 3.0411038398742676}\n",
      "2025-04-18 03:34:31,626 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:34:31,626 [INFO] - HFI attention weights: {'vit': 0.19739951193332672, 'efficientnet': 0.1996089220046997, 'inception': 0.14816594123840332, 'mobilenet': 0.14447006583213806, 'resnet': 0.16472256183624268, 'densenet': 0.14563307166099548}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.62, acc=100.0%, ce=0.01, kl=0.06, feat=0.01, cal=0.00]\n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.02it/s]\n",
      "2025-04-18 03:41:02,456 [INFO] - Epoch 49 Results - Time: 391.03s, LR: 0.000001\n",
      "2025-04-18 03:41:02,457 [INFO] - Train - Loss: 0.6225, Acc: 99.96%\n",
      "2025-04-18 03:41:02,457 [INFO] -   CE: 0.0053, KL: 0.0550, Feat: 0.0147, Cal: 0.0007\n",
      "2025-04-18 03:41:02,458 [INFO] - Val - Loss: 0.0679, Acc: 97.98%, F1: 0.9796, ECE: 0.0101\n",
      "2025-04-18 03:41:02,717 [INFO] - GPU Memory: Current=840.72MB, Peak=4296.33MB, Reserved=4682.00MB\n",
      "2025-04-18 03:41:02,723 [INFO] - Epoch 50/50\n",
      "2025-04-18 03:41:02,909 [INFO] - GPU cache cleared: 840.72MB → 840.72MB (freed 0.00MB)\n",
      "2025-04-18 03:41:02,920 [INFO] - Current calibration weight: 0.1000\n",
      "2025-04-18 03:41:02,920 [INFO] - Updated teacher weights: {'vit': 0.12455639018674494, 'efficientnet': 0.24338208143486245, 'inception': 0.03611104132670407, 'mobilenet': 0.2434846228637375, 'resnet': 0.24117744071404948, 'densenet': 0.11128842347390162}\n",
      "2025-04-18 03:41:02,920 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:41:02,923 [INFO] - Current teacher temperatures: {'vit': 2.5501513481140137, 'efficientnet': 3.1344988346099854, 'inception': 7.964338779449463, 'mobilenet': 3.109036684036255, 'resnet': 3.1632065773010254, 'densenet': 3.0407755374908447}\n",
      "2025-04-18 03:41:02,923 [INFO] - Teacher gating status: {'vit': 1.0, 'efficientnet': 1.0, 'inception': 1.0, 'mobilenet': 1.0, 'resnet': 1.0, 'densenet': 1.0}\n",
      "2025-04-18 03:41:02,923 [INFO] - HFI attention weights: {'vit': 0.19738774001598358, 'efficientnet': 0.19962859153747559, 'inception': 0.14815714955329895, 'mobilenet': 0.14446580410003662, 'resnet': 0.16473743319511414, 'densenet': 0.1456233114004135}\n",
      "Training (cal_weight=0.1000): 100%|██████████| 704/704 [06:28<00:00,  1.81it/s, loss=0.63, acc=99.9%, ce=0.01, kl=0.06, feat=0.01, cal=0.00] \n",
      "Validating: 100%|██████████| 79/79 [00:02<00:00, 29.26it/s]\n",
      "2025-04-18 03:47:33,891 [INFO] - Epoch 50 Results - Time: 391.17s, LR: 0.000000\n",
      "2025-04-18 03:47:33,891 [INFO] - Train - Loss: 0.6293, Acc: 99.95%\n",
      "2025-04-18 03:47:33,891 [INFO] -   CE: 0.0054, KL: 0.0556, Feat: 0.0146, Cal: 0.0007\n",
      "2025-04-18 03:47:33,901 [INFO] - Val - Loss: 0.0684, Acc: 98.06%, F1: 0.9805, ECE: 0.0103\n",
      "2025-04-18 03:47:34,405 [INFO] - GPU Memory: Current=841.60MB, Peak=4296.33MB, Reserved=4684.00MB\n",
      "2025-04-18 03:47:34,406 [INFO] - Training completed. Best validation accuracy: 98.06%\n",
      "2025-04-18 03:47:34,425 [INFO] - Plotting training history...\n",
      "2025-04-18 03:47:37,867 [INFO] - Enhanced training history plot saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\plots\\training_history.png\n",
      "2025-04-18 03:47:39,141 [INFO] - Testing student model...\n",
      "2025-04-18 03:47:39,141 [INFO] - Testing student model on test set...\n",
      "Validating: 100%|██████████| 157/157 [00:21<00:00,  7.42it/s]\n",
      "2025-04-18 03:48:00,330 [INFO] - Test Results:\n",
      "2025-04-18 03:48:00,331 [INFO] - Loss: 0.0998\n",
      "2025-04-18 03:48:00,331 [INFO] - Accuracy: 97.17%\n",
      "2025-04-18 03:48:00,332 [INFO] - F1 Score: 0.9716\n",
      "2025-04-18 03:48:00,332 [INFO] - Precision: 0.9717\n",
      "2025-04-18 03:48:00,333 [INFO] - Recall: 0.9717\n",
      "2025-04-18 03:48:00,334 [INFO] - ECE: 0.0157\n",
      "2025-04-18 03:48:00,335 [INFO] - Plotting calibration curves for all models...\n",
      "Computing calibration for student: 100%|██████████| 157/157 [00:05<00:00, 30.59it/s]\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_14208\\1253639460.py:2058: UserWarning: linestyle is redundantly defined by the 'linestyle' keyword argument and the fmt string \"o-\" (-> linestyle='-'). The keyword argument will take precedence.\n",
      "  plt.plot(bin_confidences, bin_accuracies, 'o-', color=color, linestyle=line_style,\n",
      "Computing calibration for vit: 100%|██████████| 157/157 [00:23<00:00,  6.80it/s]\n",
      "Computing calibration for efficientnet: 100%|██████████| 157/157 [00:05<00:00, 30.11it/s]\n",
      "Computing calibration for inception: 100%|██████████| 157/157 [00:07<00:00, 20.29it/s]\n",
      "Computing calibration for mobilenet: 100%|██████████| 157/157 [00:04<00:00, 32.23it/s]\n",
      "Computing calibration for resnet: 100%|██████████| 157/157 [00:10<00:00, 15.18it/s]\n",
      "Computing calibration for densenet: 100%|██████████| 157/157 [00:11<00:00, 13.39it/s]\n",
      "2025-04-18 03:49:09,267 [INFO] - Teacher calibration curves saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\plots\\teacher_calibration_curves.png\n",
      "2025-04-18 03:49:09,267 [INFO] - ECE values: {'student': 0.04635745345015141, 'vit': 0.04546110738101256, 'efficientnet': 0.0848824145358763, 'inception': 0.2630447937035792, 'mobilenet': 0.059094877193580156, 'resnet': 0.08162392206789962, 'densenet': 0.11343221674889838}\n",
      "2025-04-18 03:49:09,267 [INFO] - Plotting calibration curve for student...\n",
      "Computing calibration data: 100%|██████████| 157/157 [00:05<00:00, 30.12it/s]\n",
      "2025-04-18 03:49:14,942 [INFO] - Calibration curve saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\EnsembleDistillation\\plots\\calibration_curve.png\n",
      "2025-04-18 03:49:14,951 [INFO] - Exporting final model...\n",
      "2025-04-18 03:49:15,000 [INFO] - Final model exported to C:\\Users\\Gading\\Downloads\\Research\\Models\\EnsembleDistillation\\exports\\cal_aware_distilled_model.pth\n",
      "2025-04-18 03:49:15,001 [INFO] - GPU Memory: Current=647.77MB, Peak=4296.33MB, Reserved=1370.00MB\n",
      "2025-04-18 03:49:15,001 [INFO] - Calibration-aware ensemble distillation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble Distillation Training Script for Six Teacher Models on CIFAR-10\n",
    "- Teachers: ViT-B16, EfficientNetB0, InceptionV3, MobileNetV3, ResNet50, DenseNet121\n",
    "- Student: Scaled EfficientNetB0\n",
    "\n",
    "Part of the research: \n",
    "\"Comparative Analysis of Ensemble Distillation and Mutual Learning: \n",
    "A Unified Framework for Uncertainty-Calibrated Vision Systems\"\n",
    "\n",
    "Target Hardware: RTX 3060 Laptop (6GB VRAM)\n",
    "Optimizations: AMP, gradient accumulation, memory-efficient techniques, GPU cache clearing\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from tensorboardX import SummaryWriter\n",
    "# Replace broad imports with specific model imports\n",
    "from torchvision.models import (\n",
    "    vit_b_16, ViT_B_16_Weights,\n",
    "    efficientnet_b0, EfficientNet_B0_Weights,\n",
    "    inception_v3, Inception_V3_Weights,\n",
    "    mobilenet_v3_large, MobileNet_V3_Large_Weights,\n",
    "    resnet50, ResNet50_Weights,\n",
    "    densenet121, DenseNet121_Weights\n",
    ")\n",
    "import timm\n",
    "from datetime import datetime\n",
    "import gc  # For explicit garbage collection\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Define base paths\n",
    "BASE_PATH = \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\"\n",
    "DATASET_PATH = os.path.join(BASE_PATH, \"Dataset\")\n",
    "RESULTS_PATH = os.path.join(BASE_PATH, \"Results\")\n",
    "MODELS_PATH = os.path.join(BASE_PATH, \"Models\")\n",
    "SCRIPTS_PATH = os.path.join(BASE_PATH, \"Scripts\")\n",
    "\n",
    "# Create model-specific paths\n",
    "MODEL_NAME = \"EnsembleDistillation\"\n",
    "MODEL_RESULTS_PATH = os.path.join(RESULTS_PATH, MODEL_NAME)\n",
    "MODEL_CHECKPOINT_PATH = os.path.join(MODELS_PATH, MODEL_NAME, \"checkpoints\")\n",
    "MODEL_EXPORT_PATH = os.path.join(MODELS_PATH, MODEL_NAME, \"exports\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_CHECKPOINT_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_EXPORT_PATH, exist_ok=True)\n",
    "os.makedirs(SCRIPTS_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_RESULTS_PATH, \"logs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_RESULTS_PATH, \"plots\"), exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "log_file = os.path.join(MODEL_RESULTS_PATH, \"logs\", \"ensemble_distillation.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Set up tensorboard writer\n",
    "writer = SummaryWriter(log_dir=os.path.join(MODEL_RESULTS_PATH, \"logs\"))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    # Enable cuDNN benchmark for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    logger.info(\"cuDNN benchmark mode enabled\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False  # Slightly faster with False\n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # General settings\n",
    "        self.seed = 42\n",
    "        self.model_name = \"ensemble_distillation\"\n",
    "        self.dataset = \"CIFAR-10\"\n",
    "        \n",
    "        # Hardware-specific optimizations - FIXED VALUES for RTX 3060 Laptop (6GB)\n",
    "        self.use_amp = True  # Automatic Mixed Precision\n",
    "        self.memory_efficient_attention = True  # Memory-efficient attention\n",
    "        self.prefetch_factor = 2  # DataLoader prefetch factor\n",
    "        self.pin_memory = True  # Pin memory for faster CPU->GPU transfers\n",
    "        self.persistent_workers = True  # Keep workers alive between epochs\n",
    "        \n",
    "        # RTX 3060 Laptop specific fixes\n",
    "        self.batch_size = 64  # Safe value based on testing\n",
    "        self.gradient_accumulation_steps = 8  # Accumulate for effective batch of 512\n",
    "        self.find_batch_size = False  # Disable auto-finding (using known values)\n",
    "        self.gpu_memory_fraction = 0.75  # More conservative memory usage\n",
    "        \n",
    "        # Data settings\n",
    "        self.input_size = 32  # Original CIFAR-10 image size\n",
    "        self.model_input_size = 224  # Required size for pretrained models\n",
    "        self.num_workers = 4  # For data loading\n",
    "        self.val_split = 0.1  # 10% validation split\n",
    "        self.dataset_path = DATASET_PATH\n",
    "        \n",
    "        # GPU cache clearing settings\n",
    "        self.clear_cache_every_n_epochs = 1  # Clear cache every epoch\n",
    "        \n",
    "        # Model settings\n",
    "        self.pretrained = True  # Use pretrained models\n",
    "        self.num_classes = 10  # CIFAR-10 has 10 classes\n",
    "        \n",
    "        # Teacher models\n",
    "        self.teacher_models = ['vit', 'efficientnet', 'inception', 'mobilenet', 'resnet', 'densenet']\n",
    "        self.teacher_finetune_epochs = 5  # Number of epochs to fine-tune each teacher\n",
    "        self.freeze_teacher_backbones = True  # Freeze teacher backbones during fine-tuning\n",
    "        \n",
    "        # Pre-trained teacher model paths\n",
    "        self.teacher_model_paths = {\n",
    "            'vit': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\ViT\\checkpoints\\vit_b16_teacher_20250321_053628_best.pth\",\n",
    "            'efficientnet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\EfficientNetB0\\checkpoints\\efficientnet_b0_teacher_20250325_132652_best.pth\",\n",
    "            'inception': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\InceptionV3\\checkpoints\\inception_v3_teacher_20250321_153825_best.pth\",\n",
    "            'mobilenet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MobileNetV3\\checkpoints\\mobilenetv3_20250326_035725_best.pth\",\n",
    "            'resnet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\ResNet50\\checkpoints\\resnet50_teacher_20250322_225032_best.pth\",\n",
    "            'densenet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\DenseNet121\\checkpoints\\densenet121_teacher_20250325_160534_best.pth\"\n",
    "        }\n",
    "        self.use_pretrained_teachers = True  # Flag to use pre-trained teacher models\n",
    "        \n",
    "        # Teacher calibration and accuracy metrics\n",
    "        self.teacher_accuracies = {\n",
    "            'densenet': 95.07,\n",
    "            'efficientnet': 94.94,\n",
    "            'inception': 74.30,\n",
    "            'mobilenet': 94.98,\n",
    "            'resnet': 94.08,\n",
    "            'vit': 93.89\n",
    "        }\n",
    "        \n",
    "        # Initial teacher weights (will be dynamically adjusted during training)\n",
    "        self.teacher_init_weights = {\n",
    "            'densenet': 1.0,\n",
    "            'efficientnet': 1.0,\n",
    "            'inception': 0.5,  # Lower initial weight due to lower accuracy\n",
    "            'mobilenet': 1.0,\n",
    "            'resnet': 1.0,\n",
    "            'vit': 1.0\n",
    "        }\n",
    "        \n",
    "        # Teacher temperature scaling\n",
    "        self.use_adaptive_temperature = True  # Use teacher-specific temperatures\n",
    "        self.teacher_temperatures = {\n",
    "            'densenet': 4.0,\n",
    "            'efficientnet': 4.0,\n",
    "            'inception': 5.0,  # Higher temperature for less confident predictions\n",
    "            'mobilenet': 4.0,\n",
    "            'resnet': 4.0,\n",
    "            'vit': 4.0\n",
    "        }\n",
    "        self.learn_temperatures = True  # Whether to learn temperatures during training\n",
    "        \n",
    "        # Teacher gating settings\n",
    "        self.use_teacher_gating = True  # Use dynamic teacher gating/pruning\n",
    "        self.gating_threshold = 0.2  # Minimum weight for teacher contribution\n",
    "        self.dynamic_weight_update = True  # Update weights during training\n",
    "        \n",
    "        # Weighting scheme options\n",
    "        self.weighting_scheme = 'adaptive'  # Options: 'fixed', 'accuracy', 'calibration', 'adaptive', 'learned'\n",
    "        self.weight_update_interval = 5  # Update weights every N batches\n",
    "        \n",
    "        # Temperature settings\n",
    "        self.soft_target_temp = 4.0  # Temperature for soft targets\n",
    "        \n",
    "        # Training settings\n",
    "        self.epochs = 50  # Total training epochs\n",
    "        self.lr = 1e-3  # Learning rate\n",
    "        self.weight_decay = 1e-5  # Weight decay\n",
    "        self.early_stop_patience = 10  # Early stopping patience\n",
    "        \n",
    "        # Loss weights\n",
    "        self.alpha = 0.7  # Weight of distillation loss vs hard-label loss\n",
    "        self.feature_loss_weight = 0.3  # Feature loss weight\n",
    "        self.cal_weight = 0.1  # Maximum calibration weight\n",
    "        \n",
    "        # Curriculum scheduling settings\n",
    "        self.use_curriculum = True  # Whether to use curriculum scheduling\n",
    "        self.curriculum_ramp_epochs = 30  # Epochs for ramping up calibration weight\n",
    "        \n",
    "        # Output settings\n",
    "        self.checkpoint_dir = MODEL_CHECKPOINT_PATH\n",
    "        self.results_dir = MODEL_RESULTS_PATH\n",
    "        self.export_dir = MODEL_EXPORT_PATH\n",
    "        \n",
    "        # Enhanced calibration settings\n",
    "        self.per_teacher_calibration = True  # Use per-teacher calibration loss\n",
    "        self.weight_by_calibration = True  # Weight losses by teacher calibration quality\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the configuration\"\"\"\n",
    "        return json.dumps(self.__dict__, indent=4)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save configuration to a JSON file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=4)\n",
    "\n",
    "    def get_calibration_weight(self, epoch):\n",
    "        \"\"\"\n",
    "        Calculate the calibration weight for the current epoch based on curriculum scheduling\n",
    "        \"\"\"\n",
    "        if not self.use_curriculum:\n",
    "            return self.cal_weight\n",
    "        \n",
    "        # Linear ramp-up of calibration weight\n",
    "        if epoch < self.curriculum_ramp_epochs:\n",
    "            return self.cal_weight * (epoch + 1) / self.curriculum_ramp_epochs\n",
    "        else:\n",
    "            return self.cal_weight\n",
    "\n",
    "# Memory utilities\n",
    "def print_gpu_memory_stats():\n",
    "    \"\"\"Print GPU memory usage statistics\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        current_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        max_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        reserved_mem = torch.cuda.memory_reserved() / 1024**2\n",
    "        logger.info(f\"GPU Memory: Current={current_mem:.2f}MB, Peak={max_mem:.2f}MB, Reserved={reserved_mem:.2f}MB\")\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache to free up memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        before_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()  # Explicit garbage collection\n",
    "        after_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        logger.info(f\"GPU cache cleared: {before_mem:.2f}MB → {after_mem:.2f}MB (freed {before_mem-after_mem:.2f}MB)\")\n",
    "\n",
    "# Calibration Metrics\n",
    "class CalibrationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_ece(probs, targets, n_bins=15):\n",
    "        \"\"\"Compute Expected Calibration Error (ECE)\"\"\"\n",
    "        # Get the confidence (max probability) and predictions\n",
    "        confidences, predictions = torch.max(probs, dim=1)\n",
    "        accuracies = (predictions == targets).float()\n",
    "        \n",
    "        # Sort by confidence\n",
    "        sorted_indices = torch.argsort(confidences)\n",
    "        sorted_confidences = confidences[sorted_indices]\n",
    "        sorted_accuracies = accuracies[sorted_indices]\n",
    "        \n",
    "        # Create bins\n",
    "        bin_size = 1.0 / n_bins\n",
    "        bins = torch.linspace(0, 1.0, n_bins+1)\n",
    "        ece = 0.0\n",
    "        \n",
    "        for i in range(n_bins):\n",
    "            # Determine bin boundaries\n",
    "            bin_start = bins[i]\n",
    "            bin_end = bins[i+1]\n",
    "            \n",
    "            # Find samples in bin\n",
    "            in_bin = (sorted_confidences >= bin_start) & (sorted_confidences < bin_end)\n",
    "            bin_count = in_bin.sum()\n",
    "            \n",
    "            if bin_count > 0:\n",
    "                bin_conf = sorted_confidences[in_bin].mean()\n",
    "                bin_acc = sorted_accuracies[in_bin].mean()\n",
    "                # Add weighted absolute difference to ECE\n",
    "                ece += (bin_count / len(confidences)) * torch.abs(bin_acc - bin_conf)\n",
    "        \n",
    "        return ece\n",
    "    \n",
    "    @staticmethod\n",
    "    def calibration_loss(logits, targets):\n",
    "        \"\"\"Compute a loss term that encourages better calibration\"\"\"\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(probs, dim=1)\n",
    "        accuracies = (predictions == targets).float()\n",
    "        \n",
    "        # MSE between confidence and accuracy\n",
    "        return torch.mean((confidences - accuracies) ** 2)\n",
    "\n",
    "# Data Preparation\n",
    "def get_cifar10_loaders(config):\n",
    "    \"\"\"Prepare CIFAR-10 dataset and dataloaders\"\"\"\n",
    "    # For pretrained models, we need to use ImageNet normalization\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Calculate padding required to bring 32x32 to config.model_input_size\n",
    "    pad_size = (config.model_input_size - config.input_size) // 2\n",
    "    \n",
    "    # Transform for training with data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        transforms.Resize(config.model_input_size, antialias=True)\n",
    "    ])\n",
    "    \n",
    "    # Transform for validation/test (no augmentation)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        transforms.Resize(config.model_input_size, antialias=True)\n",
    "    ])\n",
    "    \n",
    "    # Set CIFAR-10 dataset path\n",
    "    cifar10_path = os.path.join(config.dataset_path, \"CIFAR-10\")\n",
    "    \n",
    "    # Load CIFAR-10 dataset\n",
    "    full_train_dataset = datasets.CIFAR10(\n",
    "        root=cifar10_path, train=True, download=True, transform=train_transform\n",
    "    )\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root=cifar10_path, train=False, download=True, transform=test_transform\n",
    "    )\n",
    "    \n",
    "    # Split training set into train and validation\n",
    "    val_size = int(len(full_train_dataset) * config.val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(config.seed)\n",
    "    )\n",
    "    \n",
    "    # Create a custom dataset for validation to apply the test transform\n",
    "    val_dataset_with_transform = torch.utils.data.Subset(\n",
    "        datasets.CIFAR10(\n",
    "            root=cifar10_path, train=True, download=False, transform=test_transform\n",
    "        ),\n",
    "        val_dataset.indices\n",
    "    )\n",
    "    \n",
    "    # Create data loaders with optimized settings for RTX 3060\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=config.num_workers, \n",
    "        pin_memory=config.pin_memory,\n",
    "        persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "        prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset_with_transform, \n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False, \n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory,\n",
    "        persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "        prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False, \n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory,\n",
    "        persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "        prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "    logger.info(f\"Validation samples: {len(val_dataset)}\")\n",
    "    logger.info(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Teacher Models\n",
    "def load_teacher_models(config):\n",
    "    \"\"\"Load the six teacher models with pretrained weights\"\"\"\n",
    "    teachers = {}\n",
    "    \n",
    "    # ViT-B16 - Use torchvision implementation instead of timm\n",
    "    logger.info(\"Loading ViT-B16 model...\")\n",
    "    teachers['vit'] = vit_b_16(weights=None)\n",
    "    num_classes = config.num_classes\n",
    "    # Adjust the classifier head\n",
    "    if hasattr(teachers['vit'], 'heads'):\n",
    "        if hasattr(teachers['vit'].heads, 'head'):\n",
    "            in_features = teachers['vit'].heads.head.in_features\n",
    "            teachers['vit'].heads.head = nn.Linear(in_features, num_classes)\n",
    "        else:\n",
    "            logger.warning(\"ViT model structure differs from expected - trying alternative configuration\")\n",
    "            in_features = teachers['vit'].hidden_dim\n",
    "            teachers['vit'].heads = nn.Linear(in_features, num_classes)\n",
    "    elif hasattr(teachers['vit'], 'head'):\n",
    "        in_features = teachers['vit'].head.in_features\n",
    "        teachers['vit'].head = nn.Linear(in_features, num_classes)\n",
    "    else:\n",
    "        logger.error(\"Could not locate classification head of ViT model\")\n",
    "    \n",
    "    # EfficientNetB0 - Use torchvision implementation\n",
    "    logger.info(\"Loading EfficientNetB0 model...\")\n",
    "    teachers['efficientnet'] = efficientnet_b0(weights=None)\n",
    "    if hasattr(teachers['efficientnet'], 'classifier'):\n",
    "        in_features = teachers['efficientnet'].classifier[1].in_features\n",
    "        teachers['efficientnet'].classifier[1] = nn.Linear(in_features, config.num_classes)\n",
    "    else:\n",
    "        logger.warning(\"EfficientNet structure differs from expected\")\n",
    "    \n",
    "    # InceptionV3\n",
    "    logger.info(\"Loading InceptionV3 model...\")\n",
    "    teachers['inception'] = inception_v3(weights=None)\n",
    "    teachers['inception'].fc = nn.Linear(teachers['inception'].fc.in_features, config.num_classes)\n",
    "    teachers['inception'].aux_logits = False\n",
    "    \n",
    "    # MobileNetV3\n",
    "    logger.info(\"Loading MobileNetV3 model...\")\n",
    "    teachers['mobilenet'] = mobilenet_v3_large(weights=None)\n",
    "    teachers['mobilenet'].classifier[-1] = nn.Linear(teachers['mobilenet'].classifier[-1].in_features, config.num_classes)\n",
    "    \n",
    "    # ResNet50\n",
    "    logger.info(\"Loading ResNet50 model...\")\n",
    "    teachers['resnet'] = resnet50(weights=None)\n",
    "    teachers['resnet'].fc = nn.Linear(teachers['resnet'].fc.in_features, config.num_classes)\n",
    "    \n",
    "    # DenseNet121\n",
    "    logger.info(\"Loading DenseNet121 model...\")\n",
    "    teachers['densenet'] = densenet121(weights=None)\n",
    "    teachers['densenet'].classifier = nn.Linear(teachers['densenet'].classifier.in_features, config.num_classes)\n",
    "    \n",
    "    # Load fine-tuned weights if use_pretrained_teachers is enabled\n",
    "    if config.use_pretrained_teachers:\n",
    "        for name, model in teachers.items():\n",
    "            if name in config.teacher_model_paths:\n",
    "                checkpoint_path = config.teacher_model_paths[name]\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    logger.info(f\"Loading pre-trained weights for {name} from {checkpoint_path}\")\n",
    "                    try:\n",
    "                        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "                        \n",
    "                        # Handle different checkpoint formats\n",
    "                        if 'model_state_dict' in checkpoint:\n",
    "                            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        elif 'state_dict' in checkpoint:\n",
    "                            model.load_state_dict(checkpoint['state_dict'])\n",
    "                        else:\n",
    "                            model.load_state_dict(checkpoint)\n",
    "                            \n",
    "                        logger.info(f\"Successfully loaded pre-trained weights for {name}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error loading weights for {name}: {str(e)}\")\n",
    "                        logger.error(\"Attempting to continue with pretrained ImageNet weights\")\n",
    "                        # Fall back to ImageNet pretrained weights\n",
    "                        if name == 'vit':\n",
    "                            teachers[name] = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "                            if hasattr(teachers[name], 'heads') and hasattr(teachers[name].heads, 'head'):\n",
    "                                in_features = teachers[name].heads.head.in_features\n",
    "                                teachers[name].heads.head = nn.Linear(in_features, config.num_classes)\n",
    "                        elif name == 'efficientnet':\n",
    "                            teachers[name] = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "                            in_features = teachers[name].classifier[1].in_features\n",
    "                            teachers[name].classifier[1] = nn.Linear(in_features, config.num_classes)\n",
    "                        elif name == 'inception':\n",
    "                            teachers[name] = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
    "                            teachers[name].fc = nn.Linear(teachers[name].fc.in_features, config.num_classes)\n",
    "                            teachers[name].aux_logits = False\n",
    "                        elif name == 'mobilenet':\n",
    "                            teachers[name] = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "                            teachers[name].classifier[-1] = nn.Linear(teachers[name].classifier[-1].in_features, config.num_classes)\n",
    "                        elif name == 'resnet':\n",
    "                            teachers[name] = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "                            teachers[name].fc = nn.Linear(teachers[name].fc.in_features, config.num_classes)\n",
    "                        elif name == 'densenet':\n",
    "                            teachers[name] = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "                            teachers[name].classifier = nn.Linear(teachers[name].classifier.in_features, config.num_classes)\n",
    "                else:\n",
    "                    logger.warning(f\"Checkpoint file for {name} not found at {checkpoint_path}\")\n",
    "                    # Load ImageNet pretrained weights as fallback\n",
    "                    if name == 'vit':\n",
    "                        teachers[name] = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "                        if hasattr(teachers[name], 'heads') and hasattr(teachers[name].heads, 'head'):\n",
    "                            in_features = teachers[name].heads.head.in_features\n",
    "                            teachers[name].heads.head = nn.Linear(in_features, config.num_classes)\n",
    "                    # Add similar fallbacks for other models\n",
    "    \n",
    "    # Move all models to device\n",
    "    for name, model in teachers.items():\n",
    "        teachers[name] = model.to(device)\n",
    "        logger.info(f\"Model {name} loaded and moved to {device}\")\n",
    "        \n",
    "        # Set to evaluation mode since they're already trained\n",
    "        if config.use_pretrained_teachers:\n",
    "            teachers[name].eval()\n",
    "            logger.info(f\"Model {name} set to evaluation mode\")\n",
    "        \n",
    "    return teachers\n",
    "\n",
    "def freeze_teacher_backbone(teacher, model_name):\n",
    "    \"\"\"Freeze all layers except the classifier/output layer\"\"\"\n",
    "    if model_name == 'vit':\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze the classifier head based on model structure\n",
    "        if hasattr(teacher, 'heads') and hasattr(teacher.heads, 'head'):\n",
    "            for param in teacher.heads.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif hasattr(teacher, 'head'):\n",
    "            for param in teacher.head.parameters():\n",
    "                param.requires_grad = True\n",
    "    elif model_name == 'efficientnet':\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze classifier\n",
    "        for param in teacher.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif model_name == 'inception':\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in teacher.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif model_name == 'mobilenet':\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in teacher.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif model_name == 'resnet':\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in teacher.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif model_name == 'densenet':\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in teacher.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    return teacher\n",
    "\n",
    "# Student Model\n",
    "def create_student_model(config):\n",
    "    \"\"\"Create a student model based on EfficientNetB0\"\"\"\n",
    "    logger.info(f\"Creating scaled EfficientNet-B0 student model...\")\n",
    "    \n",
    "    # Initialize the model with ImageNet weights\n",
    "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Modify the classifier for our number of classes\n",
    "    if hasattr(model, 'classifier'):\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, config.num_classes)\n",
    "    \n",
    "    # Log model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"Student model created with {total_params/1e6:.2f}M parameters\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Loss Functions\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, temperature=2.0):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, student_logits, teacher_ensemble_logits, labels):\n",
    "        # Cross-entropy loss for hard labels\n",
    "        ce_loss = self.ce_loss(student_logits, labels)\n",
    "        \n",
    "        # KL divergence loss for soft targets (teacher ensemble)\n",
    "        soft_targets = F.softmax(teacher_ensemble_logits / self.temperature, dim=1)\n",
    "        soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        kl_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (self.temperature ** 2)\n",
    "        \n",
    "        # Combine losses using alpha as weight\n",
    "        loss = (1 - self.alpha) * ce_loss + self.alpha * kl_loss\n",
    "        \n",
    "        return loss, ce_loss, kl_loss\n",
    "\n",
    "class FeatureAlignmentLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureAlignmentLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.projections = {}  # Cache projections for efficiency\n",
    "        \n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Log shape information for debugging\n",
    "        student_shape = student_features.shape\n",
    "        teacher_shape = teacher_features.shape\n",
    "        \n",
    "        # If dimensions don't match, apply transformations\n",
    "        if student_shape != teacher_shape:\n",
    "            # Get shape information\n",
    "            if len(student_shape) == 4:  # 2D features (B, C, H, W)\n",
    "                batch_size, student_channels, student_h, student_w = student_shape\n",
    "                batch_size, teacher_channels, teacher_h, teacher_w = teacher_shape\n",
    "                \n",
    "                # Step 1: Handle spatial dimensions with adaptive pooling\n",
    "                if student_h != teacher_h or student_w != teacher_w:\n",
    "                    spatial_pool = nn.AdaptiveAvgPool2d((teacher_h, teacher_w))\n",
    "                    student_features = spatial_pool(student_features)\n",
    "                \n",
    "                # Step 2: Handle channel dimension mismatch with 1x1 convolution\n",
    "                if student_channels != teacher_channels:\n",
    "                    key = f\"{student_channels}_{teacher_channels}\"\n",
    "                    if key not in self.projections:\n",
    "                        # Create and cache a 1x1 convolution for channel projection\n",
    "                        self.projections[key] = nn.Conv2d(\n",
    "                            student_channels, teacher_channels, kernel_size=1, bias=False\n",
    "                        ).to(student_features.device)\n",
    "                        # Initialize with identity-like weights if possible\n",
    "                        if student_channels <= teacher_channels:\n",
    "                            # Partial identity initialization (first channels are copied)\n",
    "                            with torch.no_grad():\n",
    "                                self.projections[key].weight[:student_channels].fill_diagonal_(1.0)\n",
    "                    \n",
    "                    # Apply the channel projection\n",
    "                    student_features = self.projections[key](student_features)\n",
    "                \n",
    "            elif len(student_shape) == 2:  # 1D features (B, C)\n",
    "                batch_size, student_channels = student_shape\n",
    "                batch_size, teacher_channels = teacher_shape\n",
    "                \n",
    "                # Handle channel dimension mismatch with linear projection\n",
    "                if student_channels != teacher_channels:\n",
    "                    key = f\"{student_channels}_{teacher_channels}\"\n",
    "                    if key not in self.projections:\n",
    "                        # Create and cache a linear projection\n",
    "                        self.projections[key] = nn.Linear(\n",
    "                            student_channels, teacher_channels, bias=False\n",
    "                        ).to(student_features.device)\n",
    "                        # Initialize with partial identity if possible\n",
    "                        if student_channels <= teacher_channels:\n",
    "                            with torch.no_grad():\n",
    "                                torch.nn.init.eye_(self.projections[key].weight[:student_channels, :student_channels])\n",
    "                    \n",
    "                    # Apply the linear projection\n",
    "                    student_features = self.projections[key](student_features)\n",
    "                    \n",
    "            elif len(student_shape) == 3:  # Sequence features (B, L, C)\n",
    "                batch_size, student_len, student_channels = student_shape\n",
    "                batch_size, teacher_len, teacher_channels = teacher_shape\n",
    "                \n",
    "                # Handle sequence length mismatch\n",
    "                if student_len != teacher_len:\n",
    "                    # Use adaptive pooling along sequence dimension\n",
    "                    student_features = student_features.transpose(1, 2)  # (B, C, L)\n",
    "                    student_features = F.adaptive_avg_pool1d(student_features, teacher_len)\n",
    "                    student_features = student_features.transpose(1, 2)  # Back to (B, L, C)\n",
    "                \n",
    "                # Handle channel dimension mismatch\n",
    "                if student_channels != teacher_channels:\n",
    "                    key = f\"seq_{student_channels}_{teacher_channels}\"\n",
    "                    if key not in self.projections:\n",
    "                        # Create and cache a linear projection for this dimension\n",
    "                        self.projections[key] = nn.Linear(\n",
    "                            student_channels, teacher_channels, bias=False\n",
    "                        ).to(student_features.device)\n",
    "                    \n",
    "                    # Apply the channel projection\n",
    "                    student_features = self.projections[key](student_features)\n",
    "        \n",
    "        # Verify the shapes match after transformation\n",
    "        if student_features.shape != teacher_features.shape:\n",
    "            # If still not matching, try a last-resort reshape if dimensions are compatible\n",
    "            total_student_elements = student_features.numel() // student_features.size(0)\n",
    "            total_teacher_elements = teacher_features.numel() // teacher_features.size(0)\n",
    "            \n",
    "            if total_student_elements == total_teacher_elements:\n",
    "                student_features = student_features.view(teacher_features.shape)\n",
    "            else:\n",
    "                logger.warning(f\"Could not align features: student {student_features.shape} vs teacher {teacher_features.shape}\")\n",
    "                # Fall back to using the mean of each feature map to avoid crash\n",
    "                if len(student_features.shape) == 4:\n",
    "                    student_features = student_features.mean(dim=[2, 3], keepdim=True).expand_as(teacher_features)\n",
    "                elif len(student_features.shape) == 3:\n",
    "                    student_features = student_features.mean(dim=1, keepdim=True).expand_as(teacher_features)\n",
    "                elif len(student_features.shape) == 2:\n",
    "                    student_features = student_features.mean(dim=1, keepdim=True).expand_as(teacher_features)\n",
    "        \n",
    "        # Apply MSE loss on aligned features\n",
    "        return self.mse_loss(student_features, teacher_features)\n",
    "\n",
    "# Feature extraction\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.model = model\n",
    "        self.features = None\n",
    "        \n",
    "        # Flag to track if hook was registered successfully\n",
    "        self.hook_registered = False\n",
    "        \n",
    "        # Register hook to extract features\n",
    "        for name, module in model.named_modules():\n",
    "            if layer_name in name:  # More flexible matching\n",
    "                module.register_forward_hook(self.hook)\n",
    "                self.hook_registered = True\n",
    "                logger.info(f\"Hook registered for {name}\")\n",
    "                break\n",
    "        \n",
    "        if not self.hook_registered:\n",
    "            logger.warning(f\"Could not find layer {layer_name} in model, listing available layers:\")\n",
    "            for name, _ in model.named_modules():\n",
    "                logger.warning(f\"  - {name}\")\n",
    "                \n",
    "    def hook(self, module, input, output):\n",
    "        self.features = output\n",
    "        \n",
    "    def get_features(self, x):\n",
    "        _ = self.model(x)\n",
    "        return self.features\n",
    "\n",
    "# Heterogeneous Feature Integration (HFI)\n",
    "class HeterogeneousFeatureIntegrator(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Heterogeneous Feature Integration (HFI) mechanism described in the paper.\n",
    "    This module fuses features from multiple teacher models using learnable projections and attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, teacher_feature_shapes, student_feature_shape):\n",
    "        super(HeterogeneousFeatureIntegrator, self).__init__()\n",
    "        self.teacher_names = list(teacher_feature_shapes.keys())\n",
    "        self.K = len(self.teacher_names)\n",
    "        \n",
    "        # Step 2: Learnable projections (φ_j)\n",
    "        # Create projection networks for each teacher\n",
    "        self.projections = nn.ModuleDict()\n",
    "        \n",
    "        for teacher_name, feature_shape in teacher_feature_shapes.items():\n",
    "            # Different projection types based on tensor dimensions\n",
    "            if len(feature_shape) == 4:  # CNN features (B, C, H, W)\n",
    "                # 1x1 convolution to match student channels\n",
    "                self.projections[teacher_name] = nn.Conv2d(\n",
    "                    feature_shape[1], student_feature_shape[1], kernel_size=1, bias=False\n",
    "                )\n",
    "            elif len(feature_shape) == 3:  # Transformer features (B, L, D)\n",
    "                # Linear projection for sequence features\n",
    "                self.projections[teacher_name] = nn.Linear(\n",
    "                    feature_shape[2], student_feature_shape[1], bias=False\n",
    "                )\n",
    "            elif len(feature_shape) == 2:  # Vector features (B, D)\n",
    "                # Linear projection for vector features\n",
    "                self.projections[teacher_name] = nn.Linear(\n",
    "                    feature_shape[1], student_feature_shape[1], bias=False\n",
    "                )\n",
    "        \n",
    "        # Step 3: Learnable attention weights (W)\n",
    "        # Initialize with zeros, softmax will be applied during forward pass\n",
    "        self.attention_weights = nn.Parameter(torch.zeros(self.K))\n",
    "        \n",
    "        # Store shapes for spatial adaptations\n",
    "        self.student_feature_shape = student_feature_shape\n",
    "        self.teacher_feature_shapes = teacher_feature_shapes\n",
    "        \n",
    "        logger.info(f\"HFI module initialized with {self.K} teachers\")\n",
    "        logger.info(f\"Target student feature shape: {student_feature_shape}\")\n",
    "    \n",
    "    def forward(self, teacher_features):\n",
    "        \"\"\"\n",
    "        Fuse features from multiple teachers using learned projections and attention.\n",
    "        \n",
    "        Args:\n",
    "            teacher_features: Dict with teacher_name -> feature_tensor\n",
    "        \n",
    "        Returns:\n",
    "            Fused feature tensor with same shape as student features\n",
    "        \"\"\"\n",
    "        device = self.attention_weights.device\n",
    "        batch_size = list(teacher_features.values())[0].size(0)\n",
    "        \n",
    "        # Step 3: Calculate attention weights (α) using softmax\n",
    "        alpha = F.softmax(self.attention_weights, dim=0)\n",
    "        \n",
    "        # For logging/debugging\n",
    "        alpha_dict = {name: alpha[i].item() for i, name in enumerate(self.teacher_names)}\n",
    "        \n",
    "        # Step 4: Project and fuse teacher features\n",
    "        fused_features = None\n",
    "        \n",
    "        for i, teacher_name in enumerate(self.teacher_names):\n",
    "            if teacher_name not in teacher_features:\n",
    "                continue\n",
    "                \n",
    "            # Get teacher features\n",
    "            feat = teacher_features[teacher_name]\n",
    "            \n",
    "            # Get projection for this teacher\n",
    "            proj = self.projections[teacher_name]\n",
    "            \n",
    "            # Project features to common space (φ_j(f_j))\n",
    "            if len(feat.shape) == 4:  # CNN features\n",
    "                # Apply 1x1 convolution\n",
    "                projected = proj(feat)\n",
    "                # Ensure spatial dimensions match student's using adaptive pooling\n",
    "                if projected.shape[2:] != self.student_feature_shape[2:]:\n",
    "                    projected = F.adaptive_avg_pool2d(\n",
    "                        projected, output_size=self.student_feature_shape[2:]\n",
    "                    )\n",
    "            elif len(feat.shape) == 3:  # Transformer features\n",
    "                # For sequence features, we need special handling\n",
    "                if isinstance(proj, nn.Linear):\n",
    "                    # Apply linear projection along sequence dimension\n",
    "                    projected = proj(feat)\n",
    "                    # Reshape to match student's CNN format if needed\n",
    "                    if len(self.student_feature_shape) == 4:\n",
    "                        # Convert (B, L, D) to (B, D, H, W) format\n",
    "                        seq_len = projected.size(1)\n",
    "                        channels = projected.size(2)\n",
    "                        # Try to find factors for H,W that multiply to seq_len\n",
    "                        h = int(np.sqrt(seq_len))\n",
    "                        w = seq_len // h\n",
    "                        if h * w == seq_len:\n",
    "                            # Perfect square, reshape directly\n",
    "                            projected = projected.transpose(1, 2).reshape(\n",
    "                                batch_size, channels, h, w\n",
    "                            )\n",
    "                        else:\n",
    "                            # Not perfect square, use adaptive pooling\n",
    "                            projected = projected.transpose(1, 2).unsqueeze(-1)  # B, D, L, 1\n",
    "                            projected = F.adaptive_avg_pool2d(\n",
    "                                projected, output_size=self.student_feature_shape[2:]\n",
    "                            )\n",
    "            elif len(feat.shape) == 2:  # Vector features\n",
    "                # Project vector features\n",
    "                projected = proj(feat)\n",
    "                # Reshape to match student's format if needed\n",
    "                if len(self.student_feature_shape) == 4:\n",
    "                    # Reshape (B, D) to (B, D, 1, 1) then expand\n",
    "                    projected = projected.unsqueeze(-1).unsqueeze(-1)\n",
    "                    projected = projected.expand(\n",
    "                        -1, -1, self.student_feature_shape[2], self.student_feature_shape[3]\n",
    "                    )\n",
    "                elif len(self.student_feature_shape) == 3:\n",
    "                    # Reshape (B, D) to (B, 1, D) then expand\n",
    "                    projected = projected.unsqueeze(1)\n",
    "                    projected = projected.expand(-1, self.student_feature_shape[1], -1)\n",
    "            \n",
    "            # Step 4: Feature Fusion with attention weights\n",
    "            # α[j] * φ_j(f_j)\n",
    "            weighted = alpha[i] * projected\n",
    "            \n",
    "            if fused_features is None:\n",
    "                fused_features = weighted\n",
    "            else:\n",
    "                # Handle potential shape mismatch \n",
    "                if weighted.shape != fused_features.shape:\n",
    "                    # Try to adapt weighted feature to match fused shape\n",
    "                    if len(weighted.shape) == len(fused_features.shape):\n",
    "                        if len(weighted.shape) == 4:  # CNN features\n",
    "                            weighted = F.adaptive_avg_pool2d(\n",
    "                                weighted, output_size=fused_features.shape[2:]\n",
    "                            )\n",
    "                        elif len(weighted.shape) == 3:  # Sequence features\n",
    "                            weighted = F.adaptive_avg_pool1d(\n",
    "                                weighted.transpose(1, 2), fused_features.shape[1]\n",
    "                            ).transpose(1, 2)\n",
    "                \n",
    "                # Add to fusion\n",
    "                fused_features = fused_features + weighted\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "# Training and evaluation functions\n",
    "def fine_tune_teacher(teacher, model_name, train_loader, val_loader, config, epoch_callback=None):\n",
    "    \"\"\"Fine-tune a single teacher model on CIFAR-10\"\"\"\n",
    "    logger.info(f\"Fine-tuning {model_name} teacher model...\")\n",
    "    \n",
    "    # Freeze the backbone if configured\n",
    "    if config.freeze_teacher_backbones:\n",
    "        teacher = freeze_teacher_backbone(teacher, model_name)\n",
    "        \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.Adam([p for p in teacher.parameters() if p.requires_grad], \n",
    "                          lr=config.lr, weight_decay=config.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config.teacher_finetune_epochs)\n",
    "    scaler = GradScaler() if config.use_amp else None\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state_dict = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config.teacher_finetune_epochs):\n",
    "        # Training phase\n",
    "        teacher.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"{model_name} Epoch {epoch+1}/{config.teacher_finetune_epochs}\")\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision - ADD DEVICE TYPE\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                outputs = teacher(inputs)\n",
    "                \n",
    "                # Handle inception output format\n",
    "                if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                    \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "            # Backward pass with mixed precision\n",
    "            if config.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': train_loss / (pbar.n + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        teacher.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                    outputs = teacher(inputs)\n",
    "                    \n",
    "                    # Handle inception output format\n",
    "                    if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                        \n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        logger.info(f\"{model_name} Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "                   f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state_dict = teacher.state_dict()\n",
    "            logger.info(f\"New best model for {model_name} (Val Loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        writer.add_scalar(f'teacher/{model_name}/train_loss', train_loss, epoch)\n",
    "        writer.add_scalar(f'teacher/{model_name}/train_acc', train_acc, epoch)\n",
    "        writer.add_scalar(f'teacher/{model_name}/val_loss', val_loss, epoch)\n",
    "        writer.add_scalar(f'teacher/{model_name}/val_acc', val_acc, epoch)\n",
    "        \n",
    "        # Call epoch callback if provided\n",
    "        if epoch_callback:\n",
    "            epoch_callback(epoch, train_loss, train_acc, val_loss, val_acc)\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if epoch % config.clear_cache_every_n_epochs == 0:\n",
    "            clear_gpu_cache()\n",
    "    \n",
    "    # Restore best model and save\n",
    "    if best_state_dict is not None:\n",
    "        teacher.load_state_dict(best_state_dict)\n",
    "        save_path = os.path.join(config.checkpoint_dir, f\"{model_name}_teacher.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': best_state_dict,\n",
    "            'val_loss': best_val_loss,\n",
    "        }, save_path)\n",
    "        logger.info(f\"Saved best {model_name} teacher model to {save_path}\")\n",
    "    \n",
    "    return teacher\n",
    "\n",
    "# Teacher Weighting\n",
    "class TeacherWeighting:\n",
    "    def __init__(self, config, device):\n",
    "        \"\"\"Initialize teacher weighting mechanism\"\"\"\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.teacher_names = config.teacher_models\n",
    "        \n",
    "        # Initialize weights based on config\n",
    "        self.weights = {name: config.teacher_init_weights.get(name, 1.0) for name in self.teacher_names}\n",
    "        self.normalized_weights = self._normalize_weights()\n",
    "        \n",
    "        # Initialize temperatures based on config\n",
    "        self.temperatures = {name: config.teacher_temperatures.get(name, config.soft_target_temp) for name in self.teacher_names}\n",
    "        \n",
    "        # If temperatures are learnable, create parameters\n",
    "        self.learnable_temps = None\n",
    "        if config.learn_temperatures:\n",
    "            self.learnable_temps = nn.ParameterDict({\n",
    "                name: nn.Parameter(torch.tensor(temp).to(device))\n",
    "                for name, temp in self.temperatures.items()\n",
    "            })\n",
    "        \n",
    "        # Track metrics for each teacher\n",
    "        self.teacher_metrics = {\n",
    "            name: {\n",
    "                'accuracy': config.teacher_accuracies.get(name, 90.0),  # Default to 90% if not specified\n",
    "                'ece': 0.05,  # Initial ECE estimate\n",
    "                'batch_entropies': [],\n",
    "                'batch_accuracies': []\n",
    "            }\n",
    "            for name in self.teacher_names\n",
    "        }\n",
    "        \n",
    "        # Dynamic gating status (1 = active, 0 = gated/pruned)\n",
    "        self.gating_status = {name: 1.0 for name in self.teacher_names}\n",
    "        \n",
    "        logger.info(f\"Teacher weighting initialized with scheme: {config.weighting_scheme}\")\n",
    "        logger.info(f\"Initial teacher weights: {self.normalized_weights}\")\n",
    "        logger.info(f\"Initial teacher temperatures: {self.temperatures}\")\n",
    "    \n",
    "    def _normalize_weights(self):\n",
    "        \"\"\"Normalize weights to sum to 1\"\"\"\n",
    "        total = sum(self.weights.values())\n",
    "        if total > 0:\n",
    "            return {name: weight / total for name, weight in self.weights.items()}\n",
    "        return {name: 1.0 / len(self.weights) for name in self.weights}\n",
    "    \n",
    "    def get_temperature(self, teacher_name):\n",
    "        \"\"\"Get temperature for a specific teacher\"\"\"\n",
    "        if self.learnable_temps is not None:\n",
    "            # Use learned temperature (with positive constraint)\n",
    "            return torch.abs(self.learnable_temps[teacher_name]) + 1.0\n",
    "        else:\n",
    "            # Use fixed temperature\n",
    "            return self.temperatures[teacher_name]\n",
    "    \n",
    "    def update_metrics(self, teacher_outputs, labels, teacher_names):\n",
    "        \"\"\"Update accuracy and calibration metrics for teachers\"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        for i, name in enumerate(teacher_names):\n",
    "            outputs = teacher_outputs[i]\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            accuracy = (predicted == labels).float().mean().item() * 100\n",
    "            \n",
    "            # Calculate entropy\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "            entropy = -(probs * log_probs).sum(dim=1).mean().item()\n",
    "            \n",
    "            # Calculate ECE\n",
    "            ece = CalibrationMetrics.compute_ece(probs, labels).item()\n",
    "            \n",
    "            # Update metrics\n",
    "            self.teacher_metrics[name]['batch_accuracies'].append(accuracy)\n",
    "            self.teacher_metrics[name]['batch_entropies'].append(entropy)\n",
    "            self.teacher_metrics[name]['ece'] = ece\n",
    "    \n",
    "    def update_weights(self, validation=False):\n",
    "        \"\"\"Update teacher weights based on the selected scheme\"\"\"\n",
    "        if not self.config.dynamic_weight_update and not validation:\n",
    "            return\n",
    "        \n",
    "        scheme = self.config.weighting_scheme\n",
    "        \n",
    "        if scheme == 'fixed':\n",
    "            # Use fixed weights from config\n",
    "            pass\n",
    "        \n",
    "        elif scheme == 'accuracy':\n",
    "            # Weight by accuracy\n",
    "            for name in self.teacher_names:\n",
    "                acc = self.teacher_metrics[name]['accuracy']\n",
    "                self.weights[name] = acc / 100.0  # Normalize to [0, 1]\n",
    "        \n",
    "        elif scheme == 'calibration':\n",
    "            # Weight inversely by ECE (lower ECE = better calibration = higher weight)\n",
    "            for name in self.teacher_names:\n",
    "                ece = max(0.01, self.teacher_metrics[name]['ece'])  # Avoid division by zero\n",
    "                self.weights[name] = 1.0 / (ece * 10.0)  # Scale for reasonable values\n",
    "        \n",
    "        elif scheme == 'adaptive':\n",
    "            # Combine accuracy and calibration\n",
    "            for name in self.teacher_names:\n",
    "                acc = self.teacher_metrics[name]['accuracy'] / 100.0\n",
    "                ece = max(0.01, self.teacher_metrics[name]['ece'])\n",
    "                # Higher accuracy and lower ECE = higher weight\n",
    "                self.weights[name] = acc / (ece * 5.0 + 0.1)\n",
    "        \n",
    "        # Apply gating if enabled\n",
    "        if self.config.use_teacher_gating:\n",
    "            for name in self.teacher_names:\n",
    "                # Apply binary gating (on/off)\n",
    "                if self.weights[name] < self.config.gating_threshold:\n",
    "                    self.gating_status[name] = 0.0\n",
    "                else:\n",
    "                    self.gating_status[name] = 1.0\n",
    "                \n",
    "                # Apply gating to weights\n",
    "                self.weights[name] *= self.gating_status[name]\n",
    "        \n",
    "        # Normalize weights\n",
    "        self.normalized_weights = self._normalize_weights()\n",
    "        \n",
    "        # Log updated weights\n",
    "        if validation:\n",
    "            logger.info(f\"Updated teacher weights: {self.normalized_weights}\")\n",
    "            logger.info(f\"Teacher gating status: {self.gating_status}\")\n",
    "    \n",
    "    def get_weighted_ensemble(self, outputs, teacher_names):\n",
    "        \"\"\"Combine teacher outputs using current weights\"\"\"\n",
    "        weighted_outputs = []\n",
    "        \n",
    "        for i, name in enumerate(teacher_names):\n",
    "            # Apply temperature scaling\n",
    "            temp = self.get_temperature(name) if self.config.use_adaptive_temperature else self.config.soft_target_temp\n",
    "            scaled_output = outputs[i] / temp\n",
    "            \n",
    "            # Apply weight\n",
    "            weight = self.normalized_weights[name]\n",
    "            weighted_outputs.append(scaled_output * weight)\n",
    "        \n",
    "        # Sum weighted outputs\n",
    "        ensemble_output = sum(weighted_outputs)\n",
    "        \n",
    "        return ensemble_output\n",
    "\n",
    "def get_ensemble_predictions(teachers, inputs, config, teacher_weighting=None):\n",
    "    \"\"\"Get ensemble predictions from all teacher models with calibration-aware weighting\"\"\"\n",
    "    ensemble_outputs = []\n",
    "    teacher_raw_outputs = []\n",
    "    teacher_names = []\n",
    "    \n",
    "    for name, model in teachers.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Handle inception output format\n",
    "                if name == 'inception' and isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                \n",
    "                teacher_raw_outputs.append(outputs)\n",
    "                teacher_names.append(name)\n",
    "    \n",
    "    # If teacher weighting is provided, use it to get weighted predictions\n",
    "    if teacher_weighting is not None:\n",
    "        # Update teacher metrics\n",
    "        with torch.no_grad():\n",
    "            labels = torch.argmax(teacher_raw_outputs[0], dim=1)  # Use first teacher's predictions as pseudo-labels\n",
    "            teacher_weighting.update_metrics(teacher_raw_outputs, labels, teacher_names)\n",
    "        \n",
    "        # Get weighted ensemble\n",
    "        ensemble_pred = teacher_weighting.get_weighted_ensemble(teacher_raw_outputs, teacher_names)\n",
    "    else:\n",
    "        # Simple averaging (original method)\n",
    "        ensemble_pred = torch.mean(torch.stack(teacher_raw_outputs), dim=0)\n",
    "    \n",
    "    return ensemble_pred, teacher_raw_outputs, teacher_names\n",
    "\n",
    "class EnhancedDistillationLoss(nn.Module):\n",
    "    def __init__(self, config, teacher_weighting=None):\n",
    "        super(EnhancedDistillationLoss, self).__init__()\n",
    "        self.config = config\n",
    "        self.alpha = config.alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.teacher_weighting = teacher_weighting\n",
    "        \n",
    "    def forward(self, student_logits, teacher_ensemble_logits, teacher_individual_logits, \n",
    "                teacher_names, labels):\n",
    "        # Cross-entropy loss for hard labels\n",
    "        ce_loss = self.ce_loss(student_logits, labels)\n",
    "        \n",
    "        # KL divergence loss for soft targets (teacher ensemble)\n",
    "        # Use global temperature if teacher_weighting is not provided\n",
    "        if self.teacher_weighting is None or not self.config.use_adaptive_temperature:\n",
    "            temp = self.config.soft_target_temp\n",
    "            soft_targets = F.softmax(teacher_ensemble_logits / temp, dim=1)\n",
    "            soft_prob = F.log_softmax(student_logits / temp, dim=1)\n",
    "            kl_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (temp ** 2)\n",
    "        else:\n",
    "            # Use teacher-specific temperature scaling and weights\n",
    "            kl_losses = []\n",
    "            weights = []\n",
    "            \n",
    "            for i, name in enumerate(teacher_names):\n",
    "                # Get teacher-specific temperature\n",
    "                temp = self.teacher_weighting.get_temperature(name)\n",
    "                \n",
    "                # Get teacher weight\n",
    "                weight = self.teacher_weighting.normalized_weights[name]\n",
    "                \n",
    "                # Calculate KL divergence with this teacher\n",
    "                teacher_logits = teacher_individual_logits[i]\n",
    "                soft_targets = F.softmax(teacher_logits / temp, dim=1)\n",
    "                soft_prob = F.log_softmax(student_logits / temp, dim=1)\n",
    "                teacher_kl = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (temp ** 2)\n",
    "                \n",
    "                kl_losses.append(teacher_kl)\n",
    "                weights.append(weight)\n",
    "            \n",
    "            # Weight KL losses by teacher weights\n",
    "            if self.config.weight_by_calibration:\n",
    "                kl_loss = sum(w * l for w, l in zip(weights, kl_losses))\n",
    "            else:\n",
    "                # Simple average if not weighting by calibration\n",
    "                kl_loss = sum(kl_losses) / len(kl_losses)\n",
    "        \n",
    "        # Calculate per-teacher calibration loss if enabled\n",
    "        cal_loss = 0\n",
    "        if self.config.per_teacher_calibration and self.teacher_weighting is not None:\n",
    "            cal_losses = []\n",
    "            \n",
    "            for i, name in enumerate(teacher_names):\n",
    "                # Get teacher weight\n",
    "                weight = self.teacher_weighting.normalized_weights[name]\n",
    "                \n",
    "                # Calculate calibration loss for this teacher\n",
    "                teacher_cal_loss = CalibrationMetrics.calibration_loss(student_logits, labels)\n",
    "                cal_losses.append(teacher_cal_loss * weight)\n",
    "            \n",
    "            cal_loss = sum(cal_losses)\n",
    "        else:\n",
    "            # Use standard calibration loss\n",
    "            cal_loss = CalibrationMetrics.calibration_loss(student_logits, labels)\n",
    "        \n",
    "        # Combine losses using alpha as weight\n",
    "        loss = (1 - self.alpha) * ce_loss + self.alpha * kl_loss\n",
    "        \n",
    "        return loss, ce_loss, kl_loss, cal_loss\n",
    "\n",
    "def validate(model, val_loader, criterion, config):\n",
    "    \"\"\"Validate the model and compute metrics\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Store for metrics calculation\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probs.append(F.softmax(outputs, dim=1).cpu())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_probs = torch.cat(all_probs, dim=0)\n",
    "    all_targets_tensor = torch.tensor(all_targets)\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "    ece = CalibrationMetrics.compute_ece(all_probs, all_targets_tensor).item()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': val_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'ece': ece\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_student(student, teachers, train_loader, val_loader, config):\n",
    "    \"\"\"Train the student model with knowledge distillation from teachers\"\"\"\n",
    "    logger.info(\"Training student model with ensemble distillation...\")\n",
    "    \n",
    "    # Initialize teacher weighting mechanism\n",
    "    teacher_weighting = TeacherWeighting(config, device)\n",
    "    \n",
    "    # Configure losses\n",
    "    distil_loss_fn = EnhancedDistillationLoss(config, teacher_weighting)\n",
    "    feature_loss_fn = FeatureAlignmentLoss()\n",
    "    calibration_loss_fn = CalibrationMetrics.calibration_loss\n",
    "    \n",
    "    # Setup feature extractors for all models - Adapt layer names for torchvision models\n",
    "    teacher_feature_extractors = {}\n",
    "    teacher_feature_layers = {\n",
    "        'vit': 'encoder.ln',  # For torchvision ViT\n",
    "        'efficientnet': 'features.8',  # For torchvision EfficientNet\n",
    "        'inception': 'Mixed_7c',\n",
    "        'mobilenet': 'features',\n",
    "        'resnet': 'layer4',\n",
    "        'densenet': 'features'\n",
    "    }\n",
    "    \n",
    "    # Store teacher feature shapes for HFI initialization\n",
    "    teacher_feature_shapes = {}\n",
    "    \n",
    "    for name, model in teachers.items():\n",
    "        layer_name = teacher_feature_layers.get(name)\n",
    "        if layer_name:\n",
    "            teacher_feature_extractors[name] = FeatureExtractor(model, layer_name)\n",
    "            if teacher_feature_extractors[name].hook_registered:\n",
    "                logger.info(f\"Feature extractor registered for {name} at layer {layer_name}\")\n",
    "                # Log feature shape for debugging\n",
    "                with torch.no_grad():\n",
    "                    dummy_input = torch.randn(1, 3, config.model_input_size, config.model_input_size).to(device)\n",
    "                    _ = model(dummy_input)\n",
    "                    if teacher_feature_extractors[name].features is not None:\n",
    "                        feature_shape = teacher_feature_extractors[name].features.shape\n",
    "                        teacher_feature_shapes[name] = feature_shape\n",
    "                        logger.info(f\"Feature shape for {name}: {feature_shape}\")\n",
    "            else:\n",
    "                logger.warning(f\"Feature extractor failed for {name} at layer {layer_name}\")\n",
    "    \n",
    "    # Feature extractor for student - Use the torchvision model's feature layer\n",
    "    student_feature_extractor = FeatureExtractor(student, 'features.8')\n",
    "    if student_feature_extractor.hook_registered:\n",
    "        logger.info(\"Feature extractor registered for student model\")\n",
    "        # Log feature shape for debugging\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, config.model_input_size, config.model_input_size).to(device)\n",
    "            _ = student(dummy_input)\n",
    "            if student_feature_extractor.features is not None:\n",
    "                student_feature_shape = student_feature_extractor.features.shape\n",
    "                logger.info(f\"Feature shape for student: {student_feature_shape}\")\n",
    "    else:\n",
    "        logger.warning(\"Feature extractor failed for student model, listing available layers:\")\n",
    "        for name, _ in student.named_modules():\n",
    "            logger.info(f\"  - {name}\")\n",
    "    \n",
    "    # Initialize the HFI module\n",
    "    hfi_module = HeterogeneousFeatureIntegrator(\n",
    "        teacher_feature_shapes=teacher_feature_shapes,\n",
    "        student_feature_shape=student_feature_shape\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer and scheduler - Include HFI parameters\n",
    "    optimizer_params = list(student.parameters()) + list(hfi_module.parameters())\n",
    "    \n",
    "    # Add temperature parameters if they're learnable\n",
    "    if config.learn_temperatures and teacher_weighting.learnable_temps is not None:\n",
    "        logger.info(\"Adding teacher temperatures to optimizer parameters\")\n",
    "        optimizer_params += list(teacher_weighting.learnable_temps.parameters())\n",
    "    \n",
    "    optimizer = optim.Adam(optimizer_params, lr=config.lr, weight_decay=config.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "    scaler = GradScaler() if config.use_amp else None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], \n",
    "        'val_loss': [], 'val_acc': [], 'val_ece': [], 'val_f1': [],\n",
    "        'ce_loss': [], 'kl_loss': [], 'feature_loss': [], 'cal_loss': [],\n",
    "        'calibration_weights': [],\n",
    "        'teacher_weights': [],\n",
    "        'teacher_temperatures': [],\n",
    "        'teacher_gating': [],\n",
    "        'hfi_weights': [],  # Store HFI attention weights\n",
    "        'best_epoch': 0\n",
    "    }\n",
    "    \n",
    "    # Get timestamp for model naming\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = f\"{config.model_name}_{timestamp}\"\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = os.path.join(config.results_dir, f\"{model_name}_config.json\")\n",
    "    config.save(config_path)\n",
    "    logger.info(f\"Configuration saved to {config_path}\")\n",
    "    \n",
    "    # Initial GPU memory stats\n",
    "    print_gpu_memory_stats()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        logger.info(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        clear_gpu_cache()\n",
    "        \n",
    "        # Get current calibration weight\n",
    "        cal_weight = config.get_calibration_weight(epoch)\n",
    "        history['calibration_weights'].append(cal_weight)\n",
    "        logger.info(f\"Current calibration weight: {cal_weight:.4f}\")\n",
    "        \n",
    "        # Update teacher weights at epoch start\n",
    "        teacher_weighting.update_weights(validation=True)\n",
    "        \n",
    "        # Track teacher weights and temperatures\n",
    "        history['teacher_weights'].append(dict(teacher_weighting.normalized_weights))\n",
    "        \n",
    "        if config.learn_temperatures and teacher_weighting.learnable_temps is not None:\n",
    "            current_temps = {name: teacher_weighting.get_temperature(name).item() \n",
    "                            for name in config.teacher_models}\n",
    "            history['teacher_temperatures'].append(current_temps)\n",
    "            logger.info(f\"Current teacher temperatures: {current_temps}\")\n",
    "        \n",
    "        # Track teacher gating status\n",
    "        history['teacher_gating'].append(dict(teacher_weighting.gating_status))\n",
    "        logger.info(f\"Teacher gating status: {teacher_weighting.gating_status}\")\n",
    "        \n",
    "        # Track HFI attention weights\n",
    "        hfi_weights = F.softmax(hfi_module.attention_weights, dim=0).detach().cpu().numpy()\n",
    "        hfi_weight_dict = {name: float(hfi_weights[i]) for i, name in enumerate(hfi_module.teacher_names)}\n",
    "        history['hfi_weights'].append(hfi_weight_dict)\n",
    "        logger.info(f\"HFI attention weights: {hfi_weight_dict}\")\n",
    "        \n",
    "        # Set all models to appropriate modes\n",
    "        student.train()\n",
    "        hfi_module.train()\n",
    "        for teacher in teachers.values():\n",
    "            teacher.eval()\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss = 0.0\n",
    "        train_ce_loss = 0.0\n",
    "        train_kl_loss = 0.0\n",
    "        train_feature_loss = 0.0\n",
    "        train_cal_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Initialize gradient accumulation counter\n",
    "        steps_since_update = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Training (cal_weight={cal_weight:.4f})\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients if starting a new accumulation cycle\n",
    "            if config.gradient_accumulation_steps <= 1 or steps_since_update == 0:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                # Get teacher ensemble predictions with weights\n",
    "                teacher_outputs, teacher_individual, teacher_names = get_ensemble_predictions(\n",
    "                    teachers, inputs, config, teacher_weighting\n",
    "                )\n",
    "                \n",
    "                # Student forward pass\n",
    "                student_outputs = student(inputs)\n",
    "                \n",
    "                # Collect teacher features into a dictionary\n",
    "                teacher_feats = {}\n",
    "                for name, extractor in teacher_feature_extractors.items():\n",
    "                    if extractor.features is not None:\n",
    "                        teacher_feats[name] = extractor.features\n",
    "                \n",
    "                # Get student features\n",
    "                student_features = student_feature_extractor.features\n",
    "                \n",
    "                # Use HFI to fuse teacher features\n",
    "                fused_features = hfi_module(teacher_feats)\n",
    "                \n",
    "                # Calculate feature alignment loss using fused features\n",
    "                feature_loss = F.mse_loss(student_features, fused_features) if student_features is not None and fused_features is not None else torch.tensor(0.0).to(device)\n",
    "                \n",
    "                # Calculate distillation loss components\n",
    "                dist_loss, ce_loss, kl_loss, cal_loss = distil_loss_fn(\n",
    "                    student_outputs, teacher_outputs, teacher_individual, \n",
    "                    teacher_names, labels\n",
    "                )\n",
    "                \n",
    "                # Combine all losses with weights but normalize KL loss for display\n",
    "                loss = dist_loss + config.feature_loss_weight * feature_loss + cal_weight * cal_loss\n",
    "            \n",
    "            # Backward pass with mixed precision\n",
    "            if config.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            # Update weights if we've accumulated enough gradients\n",
    "            steps_since_update += 1\n",
    "            if config.gradient_accumulation_steps <= 1 or steps_since_update == config.gradient_accumulation_steps:\n",
    "                if config.use_amp:\n",
    "                    # Unscale before gradient clipping\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "                    torch.nn.utils.clip_grad_norm_(hfi_module.parameters(), max_norm=1.0)\n",
    "                    # Step optimizer and update scaler\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "                    torch.nn.utils.clip_grad_norm_(hfi_module.parameters(), max_norm=1.0)\n",
    "                    # Step optimizer\n",
    "                    optimizer.step()\n",
    "                \n",
    "                steps_since_update = 0\n",
    "            \n",
    "            # Update statistics for tracking\n",
    "            train_loss += loss.item()\n",
    "            train_ce_loss += ce_loss.item()\n",
    "            # Store normalized KL loss (divide by temperature squared) for more reasonable display\n",
    "            train_kl_loss += kl_loss.item() / (config.soft_target_temp ** 2) if not config.use_adaptive_temperature else kl_loss.item() / 16.0\n",
    "            train_feature_loss += feature_loss.item()\n",
    "            train_cal_loss += cal_loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = student_outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar with properly averaged values and color coding\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "            avg_ce = train_ce_loss / (batch_idx + 1)\n",
    "            avg_kl = train_kl_loss / (batch_idx + 1)\n",
    "            avg_feat = train_feature_loss / (batch_idx + 1)\n",
    "            avg_cal = train_cal_loss / (batch_idx + 1)\n",
    "            current_acc = 100. * correct / total\n",
    "            \n",
    "            # Format with fewer decimal places for cleaner display\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{avg_loss:.2f}\",\n",
    "                'acc': f\"{current_acc:.1f}%\", \n",
    "                'ce': f\"{avg_ce:.2f}\",\n",
    "                'kl': f\"{avg_kl:.2f}\",\n",
    "                'feat': f\"{avg_feat:.2f}\",\n",
    "                'cal': f\"{avg_cal:.2f}\"\n",
    "            })\n",
    "            \n",
    "            # Update teacher weights periodically if dynamic updates are enabled\n",
    "            batch_count += 1\n",
    "            if config.dynamic_weight_update and batch_count % config.weight_update_interval == 0:\n",
    "                teacher_weighting.update_weights()\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_ce_loss = train_ce_loss / len(train_loader)\n",
    "        train_kl_loss = train_kl_loss / len(train_loader)\n",
    "        train_feature_loss = train_feature_loss / len(train_loader)\n",
    "        train_cal_loss = train_cal_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        student.eval()\n",
    "        val_criterion = nn.CrossEntropyLoss()\n",
    "        val_metrics = validate(student, val_loader, val_criterion, config)\n",
    "        \n",
    "        val_loss = val_metrics['loss']\n",
    "        val_acc = val_metrics['accuracy']\n",
    "        val_f1 = val_metrics['f1_score']\n",
    "        val_ece = val_metrics['ece']\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Log results\n",
    "        logger.info(f\"Epoch {epoch+1} Results - Time: {epoch_time:.2f}s, LR: {current_lr:.6f}\")\n",
    "        logger.info(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        logger.info(f\"  CE: {train_ce_loss:.4f}, KL: {train_kl_loss:.4f}, Feat: {train_feature_loss:.4f}, Cal: {train_cal_loss:.4f}\")\n",
    "        logger.info(f\"Val - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%, F1: {val_f1:.4f}, ECE: {val_ece:.4f}\")\n",
    "        \n",
    "        # Save to history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_ece'].append(val_ece)\n",
    "        history['ce_loss'].append(train_ce_loss)\n",
    "        history['kl_loss'].append(train_kl_loss)\n",
    "        history['feature_loss'].append(train_feature_loss)\n",
    "        history['cal_loss'].append(train_cal_loss)\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        writer.add_scalar('student/train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('student/train_acc', train_acc, epoch)\n",
    "        writer.add_scalar('student/val_loss', val_loss, epoch)\n",
    "        writer.add_scalar('student/val_acc', val_acc, epoch)\n",
    "        writer.add_scalar('student/val_f1', val_f1, epoch)\n",
    "        writer.add_scalar('student/val_ece', val_ece, epoch)\n",
    "        writer.add_scalar('student/ce_loss', train_ce_loss, epoch)\n",
    "        writer.add_scalar('student/kl_loss', train_kl_loss, epoch)\n",
    "        writer.add_scalar('student/feature_loss', train_feature_loss, epoch)\n",
    "        writer.add_scalar('student/cal_loss', train_cal_loss, epoch)\n",
    "        writer.add_scalar('student/cal_weight', cal_weight, epoch)\n",
    "        writer.add_scalar('student/learning_rate', current_lr, epoch)\n",
    "        \n",
    "        # Log teacher weights to tensorboard\n",
    "        for name, weight in teacher_weighting.normalized_weights.items():\n",
    "            writer.add_scalar(f'teacher_weights/{name}', weight, epoch)\n",
    "        \n",
    "        # Log HFI attention weights to tensorboard\n",
    "        for i, name in enumerate(hfi_module.teacher_names):\n",
    "            weight_value = hfi_weights[i]\n",
    "            writer.add_scalar(f'hfi_weights/{name}', weight_value, epoch)\n",
    "        \n",
    "        # Log teacher gating status\n",
    "        for name, status in teacher_weighting.gating_status.items():\n",
    "            writer.add_scalar(f'teacher_gating/{name}', status, epoch)\n",
    "        \n",
    "        # Log teacher temperatures if learnable\n",
    "        if config.learn_temperatures and teacher_weighting.learnable_temps is not None:\n",
    "            for name in config.teacher_models:\n",
    "                temp = teacher_weighting.get_temperature(name).item()\n",
    "                writer.add_scalar(f'teacher_temp/{name}', temp, epoch)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': student.state_dict(),\n",
    "            'hfi_state_dict': hfi_module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1,\n",
    "            'val_ece': val_ece,\n",
    "            'history': history,\n",
    "            'config': config.__dict__,\n",
    "            'teacher_weights': teacher_weighting.normalized_weights,\n",
    "            'hfi_weights': hfi_weight_dict,\n",
    "            'teacher_gating': teacher_weighting.gating_status\n",
    "        }\n",
    "        \n",
    "        # Add teacher temperatures if learnable\n",
    "        if config.learn_temperatures and teacher_weighting.learnable_temps is not None:\n",
    "            checkpoint['teacher_temperatures'] = {\n",
    "                name: teacher_weighting.get_temperature(name).item() \n",
    "                for name in config.teacher_models\n",
    "            }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        latest_path = os.path.join(config.checkpoint_dir, f\"{model_name}_latest.pth\")\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_loss_path = os.path.join(config.checkpoint_dir, f\"{model_name}_best_loss.pth\")\n",
    "            torch.save(checkpoint, best_loss_path)\n",
    "            logger.info(f\"New best model saved (Val Loss: {val_loss:.4f})\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Save best model based on validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            history['best_epoch'] = epoch + 1\n",
    "            best_acc_path = os.path.join(config.checkpoint_dir, f\"{model_name}_best_acc.pth\")\n",
    "            torch.save(checkpoint, best_acc_path)\n",
    "            logger.info(f\"New best model saved (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        # Save model at specific epochs (every 10)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            epoch_path = os.path.join(config.checkpoint_dir, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "            torch.save(checkpoint, epoch_path)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= config.early_stop_patience:\n",
    "            logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "        \n",
    "        # Print memory stats\n",
    "        print_gpu_memory_stats()\n",
    "    \n",
    "    # End of training\n",
    "    logger.info(f\"Training completed. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    return student, history\n",
    "\n",
    "# Visualization Functions\n",
    "def plot_training_history(history, config):\n",
    "    \"\"\"Plot training history with multiple metrics\"\"\"\n",
    "    plt.figure(figsize=(20, 24))  # Even larger figure for more detailed plots\n",
    "    \n",
    "    # Set a consistent style for better visualizations\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    \n",
    "    # Create color palette for consistent coloring\n",
    "    main_colors = ['#2077B4', '#FF7F0E', '#2CA02C', '#D62728']\n",
    "    teacher_colors = plt.cm.tab10(np.linspace(0, 1, len(config.teacher_models)))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax1 = plt.subplot(5, 2, 1)\n",
    "    ax1.plot(history['train_loss'], label='Train', color=main_colors[0], linewidth=2)\n",
    "    ax1.plot(history['val_loss'], label='Validation', color=main_colors[1], linewidth=2)\n",
    "    if 'best_epoch' in history:\n",
    "        ax1.axvline(x=history['best_epoch']-1, color='r', linestyle='--', label='Best Model')\n",
    "    ax1.set_title('Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax2 = plt.subplot(5, 2, 2)\n",
    "    ax2.plot(history['train_acc'], label='Train', color=main_colors[0], linewidth=2)\n",
    "    ax2.plot(history['val_acc'], label='Validation', color=main_colors[1], linewidth=2)\n",
    "    if 'best_epoch' in history:\n",
    "        ax2.axvline(x=history['best_epoch']-1, color='r', linestyle='--', label='Best Model')\n",
    "    ax2.set_title('Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss components\n",
    "    ax3 = plt.subplot(5, 2, 3)\n",
    "    ax3.plot(history['ce_loss'], label='CE Loss', linewidth=2, color=main_colors[0])\n",
    "    ax3.plot(history['kl_loss'], label='KL Loss (normalized)', linewidth=2, color=main_colors[1])\n",
    "    ax3.plot(history['feature_loss'], label='Feature Loss', linewidth=2, color=main_colors[2])\n",
    "    ax3.plot(history['cal_loss'], label='Calibration Loss', linewidth=2, color=main_colors[3])\n",
    "    if 'best_epoch' in history:\n",
    "        ax3.axvline(x=history['best_epoch']-1, color='r', linestyle='--', label='Best Model')\n",
    "    ax3.set_title('Loss Components', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Loss', fontsize=12)\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot calibration metrics\n",
    "    ax4 = plt.subplot(5, 2, 4)\n",
    "    ax4.plot(history['val_ece'], label='ECE', linewidth=2.5, color=main_colors[0])\n",
    "    if 'best_epoch' in history:\n",
    "        ax4.axvline(x=history['best_epoch']-1, color='r', linestyle='--', label='Best Model')\n",
    "    ax4.set_title('Expected Calibration Error', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch', fontsize=12)\n",
    "    ax4.set_ylabel('ECE', fontsize=12)\n",
    "    ax4.legend(fontsize=10)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    ax5 = plt.subplot(5, 2, 5)\n",
    "    ax5.plot(history['val_f1'], label='F1 Score', linewidth=2.5, color=main_colors[1])\n",
    "    if 'best_epoch' in history:\n",
    "        ax5.axvline(x=history['best_epoch']-1, color='r', linestyle='--', label='Best Model')\n",
    "    ax5.set_title('F1 Score Progression', fontsize=14, fontweight='bold')\n",
    "    ax5.set_xlabel('Epoch', fontsize=12)\n",
    "    ax5.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax5.legend(fontsize=10)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot calibration weight curriculum\n",
    "    ax6 = plt.subplot(5, 2, 6)\n",
    "    ax6.plot(history['calibration_weights'], label='Calibration Weight', linewidth=2.5, color=main_colors[2])\n",
    "    if 'best_epoch' in history:\n",
    "        ax6.axvline(x=history['best_epoch']-1, color='r', linestyle='--', label='Best Model')\n",
    "    ax6.set_title('Calibration Weight Curriculum', fontsize=14, fontweight='bold')\n",
    "    ax6.set_xlabel('Epoch', fontsize=12)\n",
    "    ax6.set_ylabel('Weight', fontsize=12)\n",
    "    ax6.legend(fontsize=10)\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot teacher adaptive weights\n",
    "    ax7 = plt.subplot(5, 2, 7)\n",
    "    if 'teacher_weights' in history and history['teacher_weights']:\n",
    "        for i, teacher_name in enumerate(config.teacher_models):\n",
    "            weights = [epoch_weights.get(teacher_name, 0) for epoch_weights in history['teacher_weights']]\n",
    "            ax7.plot(weights, label=teacher_name, linewidth=2, color=teacher_colors[i])\n",
    "        ax7.set_title('Adaptive Teacher Weights', fontsize=14, fontweight='bold')\n",
    "        ax7.set_xlabel('Epoch', fontsize=12)\n",
    "        ax7.set_ylabel('Weight', fontsize=12)\n",
    "        ax7.legend(fontsize=10)\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot teacher temperatures\n",
    "    ax8 = plt.subplot(5, 2, 8)\n",
    "    if 'teacher_temperatures' in history and history['teacher_temperatures']:\n",
    "        for i, teacher_name in enumerate(config.teacher_models):\n",
    "            temps = [epoch_temps.get(teacher_name, 4.0) for epoch_temps in history['teacher_temperatures']]\n",
    "            ax8.plot(temps, label=teacher_name, linewidth=2, color=teacher_colors[i])\n",
    "        ax8.set_title('Adaptive Teacher Temperatures', fontsize=14, fontweight='bold')\n",
    "        ax8.set_xlabel('Epoch', fontsize=12)\n",
    "        ax8.set_ylabel('Temperature', fontsize=12)\n",
    "        ax8.legend(fontsize=10)\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot HFI attention weights\n",
    "    ax9 = plt.subplot(5, 2, 9)\n",
    "    if 'hfi_weights' in history and history['hfi_weights']:\n",
    "        for i, teacher_name in enumerate(config.teacher_models):\n",
    "            if teacher_name in history['hfi_weights'][0]:\n",
    "                weights = [epoch_weights.get(teacher_name, 0) for epoch_weights in history['hfi_weights']]\n",
    "                ax9.plot(weights, label=f\"HFI: {teacher_name}\", linewidth=2, color=teacher_colors[i])\n",
    "        ax9.set_title('HFI Attention Weights', fontsize=14, fontweight='bold')\n",
    "        ax9.set_xlabel('Epoch', fontsize=12)\n",
    "        ax9.set_ylabel('Weight', fontsize=12)\n",
    "        ax9.legend(fontsize=10)\n",
    "        ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot teacher gating status\n",
    "    ax10 = plt.subplot(5, 2, 10)\n",
    "    if 'teacher_gating' in history and history['teacher_gating']:\n",
    "        for i, teacher_name in enumerate(config.teacher_models):\n",
    "            status = [epoch_gating.get(teacher_name, 0) for epoch_gating in history['teacher_gating']]\n",
    "            ax10.plot(status, label=f\"Gate: {teacher_name}\", linewidth=2, color=teacher_colors[i], \n",
    "                     marker='o', markersize=4)\n",
    "        ax10.set_title('Teacher Gating Status (1=Active, 0=Pruned)', fontsize=14, fontweight='bold')\n",
    "        ax10.set_xlabel('Epoch', fontsize=12)\n",
    "        ax10.set_ylabel('Status', fontsize=12)\n",
    "        ax10.set_yticks([0, 1])\n",
    "        ax10.set_yticklabels(['Pruned', 'Active'])\n",
    "        ax10.legend(fontsize=10)\n",
    "        ax10.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Calibration-Aware Ensemble Distillation Training Analytics', fontsize=18, fontweight='bold', y=0.99)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    \n",
    "    # Add timestamp and config info\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    info_text = f\"Generated: {timestamp}\\nLearning Rate: {config.lr}, Alpha: {config.alpha}, Cal Weight: {config.cal_weight}\"\n",
    "    plt.figtext(0.01, 0.01, info_text, fontsize=8)\n",
    "    \n",
    "    # Save figure with high quality\n",
    "    plt.savefig(os.path.join(config.results_dir, 'plots', 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "    logger.info(f\"Enhanced training history plot saved to {os.path.join(config.results_dir, 'plots', 'training_history.png')}\")\n",
    "    \n",
    "    # Save a separate PDF version for publications\n",
    "    plt.savefig(os.path.join(config.results_dir, 'plots', 'training_history.pdf'), format='pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_calibration_curve(model, test_loader, config):\n",
    "    \"\"\"Plot calibration reliability diagram\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    confidences = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # Compute confidences and accuracies\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Computing calibration data\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            confidence, predictions = torch.max(probabilities, dim=1)\n",
    "            \n",
    "            accuracy = (predictions == targets).float()\n",
    "            \n",
    "            confidences.append(confidence.cpu())\n",
    "            accuracies.append(accuracy.cpu())\n",
    "    \n",
    "    # Concatenate lists\n",
    "    confidences = torch.cat(confidences)\n",
    "    accuracies = torch.cat(accuracies)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    n_bins = 10\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    bin_confidences = []\n",
    "    bin_accuracies = []\n",
    "    bin_sizes = []\n",
    "    \n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        bin_size = in_bin.sum().item()\n",
    "        \n",
    "        if bin_size > 0:\n",
    "            bin_confidence = confidences[in_bin].mean().item()\n",
    "            bin_accuracy = accuracies[in_bin].mean().item()\n",
    "        else:\n",
    "            bin_confidence = (bin_lower + bin_upper) / 2  # Use bin center if empty\n",
    "            bin_accuracy = 0\n",
    "            \n",
    "        bin_confidences.append(bin_confidence)\n",
    "        bin_accuracies.append(bin_accuracy)\n",
    "        bin_sizes.append(bin_size)\n",
    "    \n",
    "    bin_sizes = np.array(bin_sizes) / sum(bin_sizes)  # Normalize sizes\n",
    "    \n",
    "    # Plot reliability diagram\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Plot bins\n",
    "    plt.bar(bin_lowers, bin_accuracies, width=1/n_bins, align='edge', alpha=0.5, label='Accuracy in bin')\n",
    "    for i, (conf, acc) in enumerate(zip(bin_confidences, bin_accuracies)):\n",
    "        plt.plot([conf, conf], [0, acc], 'r--', alpha=0.3)\n",
    "    \n",
    "    # Add histogram of confidence distribution\n",
    "    twin_ax = plt.twinx()\n",
    "    twin_ax.bar(bin_lowers, bin_sizes, width=1/n_bins, align='edge', alpha=0.3, color='g', label='Samples')\n",
    "    twin_ax.set_ylabel('Proportion of Samples')\n",
    "    \n",
    "    # Calculate ECE\n",
    "    ece = sum(bin_sizes[i] * abs(bin_accuracies[i] - bin_confidences[i]) for i in range(len(bin_sizes)))\n",
    "    \n",
    "    plt.title(f'Calibration Reliability Diagram (ECE = {ece:.4f})')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(config.results_dir, 'plots', 'calibration_curve.png'), dpi=300)\n",
    "    logger.info(f\"Calibration curve saved to {os.path.join(config.results_dir, 'plots', 'calibration_curve.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    return ece\n",
    "\n",
    "def plot_teacher_calibration_curves(teachers, test_loader, student, config):\n",
    "    \"\"\"Plot calibration reliability diagrams for all teachers and the student\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Setup for plotting multiple reliability diagrams\n",
    "    n_bins = 10\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1].numpy()\n",
    "    bin_uppers = bin_boundaries[1:].numpy()\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Color mapping\n",
    "    colors = {'student': 'blue', 'densenet': 'green', 'efficientnet': 'red', \n",
    "              'inception': 'purple', 'mobilenet': 'orange', 'resnet': 'brown', 'vit': 'pink'}\n",
    "    \n",
    "    # Process each model\n",
    "    all_models = {'student': student}\n",
    "    all_models.update(teachers)\n",
    "    \n",
    "    # Track ECE values\n",
    "    ece_values = {}\n",
    "    \n",
    "    for name, model in all_models.items():\n",
    "        model.eval()\n",
    "        \n",
    "        confidences = []\n",
    "        accuracies = []\n",
    "        \n",
    "        # Compute confidences and accuracies\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(test_loader, desc=f\"Computing calibration for {name}\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # Handle inception output format\n",
    "                    if name == 'inception' and isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                \n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                confidence, predictions = torch.max(probabilities, dim=1)\n",
    "                \n",
    "                accuracy = (predictions == targets).float()\n",
    "                \n",
    "                confidences.append(confidence.cpu())\n",
    "                accuracies.append(accuracy.cpu())\n",
    "        \n",
    "        # Concatenate lists\n",
    "        confidences = torch.cat(confidences)\n",
    "        accuracies = torch.cat(accuracies)\n",
    "        \n",
    "        # Calculate bin statistics\n",
    "        bin_confidences = []\n",
    "        bin_accuracies = []\n",
    "        bin_sizes = []\n",
    "        \n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "            bin_size = in_bin.sum().item()\n",
    "            \n",
    "            if bin_size > 0:\n",
    "                bin_confidence = confidences[in_bin].mean().item()\n",
    "                bin_accuracy = accuracies[in_bin].mean().item()\n",
    "            else:\n",
    "                bin_confidence = (bin_lower + bin_upper) / 2\n",
    "                bin_accuracy = 0\n",
    "                \n",
    "            bin_confidences.append(bin_confidence)\n",
    "            bin_accuracies.append(bin_accuracy)\n",
    "            bin_sizes.append(bin_size)\n",
    "        \n",
    "        # Calculate ECE\n",
    "        bin_sizes_norm = np.array(bin_sizes) / sum(bin_sizes)\n",
    "        ece = sum(bin_sizes_norm[i] * abs(bin_accuracies[i] - bin_confidences[i]) for i in range(len(bin_sizes)))\n",
    "        ece_values[name] = ece\n",
    "        \n",
    "        # Plot reliability curve\n",
    "        color = colors.get(name, 'gray')\n",
    "        line_style = '-' if name == 'student' else '--'\n",
    "        line_width = 2 if name == 'student' else 1\n",
    "        \n",
    "        # Plot accuracy points\n",
    "        plt.plot(bin_confidences, bin_accuracies, 'o-', color=color, linestyle=line_style, \n",
    "                 linewidth=line_width, label=f\"{name} (ECE={ece:.4f})\")\n",
    "    \n",
    "    plt.title('Calibration Reliability Diagrams')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(config.results_dir, 'plots', 'teacher_calibration_curves.png'), dpi=300)\n",
    "    logger.info(f\"Teacher calibration curves saved to {os.path.join(config.results_dir, 'plots', 'teacher_calibration_curves.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    return ece_values\n",
    "\n",
    "def test_student_model(student, test_loader, config):\n",
    "    \"\"\"Evaluate the student model on the test set and log detailed metrics\"\"\"\n",
    "    logger.info(\"Testing student model on test set...\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metrics = validate(student, test_loader, criterion, config)\n",
    "    \n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {metrics['loss']:.4f}\")\n",
    "    logger.info(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "    logger.info(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    logger.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    logger.info(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    logger.info(f\"ECE: {metrics['ece']:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = os.path.join(config.results_dir, 'test_metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        config = Config()\n",
    "        logger.info(f\"Configuration: {config}\")\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        set_seed(config.seed)\n",
    "        \n",
    "        # Initial GPU memory stats\n",
    "        print_gpu_memory_stats()\n",
    "        \n",
    "        # Get data loaders\n",
    "        logger.info(\"Preparing data loaders...\")\n",
    "        train_loader, val_loader, test_loader = get_cifar10_loaders(config)\n",
    "        \n",
    "        # Load teacher models\n",
    "        logger.info(\"Loading pre-trained teacher models...\")\n",
    "        teachers = load_teacher_models(config)\n",
    "        \n",
    "        # Skip fine-tuning if we're using pre-trained teacher models\n",
    "        if not config.use_pretrained_teachers:\n",
    "            # Fine-tune each teacher model\n",
    "            for name in config.teacher_models:\n",
    "                logger.info(f\"Fine-tuning {name} model...\")\n",
    "                teachers[name] = fine_tune_teacher(\n",
    "                    teachers[name], name, train_loader, val_loader, config\n",
    "                )\n",
    "                \n",
    "                # Clear GPU cache after fine-tuning each teacher\n",
    "                clear_gpu_cache()\n",
    "        else:\n",
    "            logger.info(\"Skipping teacher fine-tuning as pre-trained models are being used\")\n",
    "        \n",
    "        # Create student model\n",
    "        logger.info(\"Creating student model...\")\n",
    "        student = create_student_model(config)\n",
    "        \n",
    "        # Train student model with calibration-aware ensemble distillation\n",
    "        logger.info(\"Training student with calibration-aware ensemble distillation...\")\n",
    "        student, history = train_student(\n",
    "            student, teachers, train_loader, val_loader, config\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        logger.info(\"Plotting training history...\")\n",
    "        plot_training_history(history, config)\n",
    "        \n",
    "        # Test student model\n",
    "        logger.info(\"Testing student model...\")\n",
    "        test_metrics = test_student_model(student, test_loader, config)\n",
    "        \n",
    "        # Plot calibration curves for all models\n",
    "        logger.info(\"Plotting calibration curves for all models...\")\n",
    "        ece_values = plot_teacher_calibration_curves(\n",
    "            teachers, test_loader, student, config\n",
    "        )\n",
    "        logger.info(f\"ECE values: {ece_values}\")\n",
    "        \n",
    "        # Plot standard calibration curve for student\n",
    "        logger.info(\"Plotting calibration curve for student...\")\n",
    "        plot_calibration_curve(student, test_loader, config)\n",
    "        \n",
    "        # Export final model\n",
    "        logger.info(\"Exporting final model...\")\n",
    "        final_model_path = os.path.join(config.export_dir, \"cal_aware_distilled_model.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': student.state_dict(),\n",
    "            'test_metrics': test_metrics,\n",
    "            'config': config.__dict__,\n",
    "            'teacher_weights': history['teacher_weights'][-1] if history['teacher_weights'] else None,\n",
    "            'teacher_temperatures': history['teacher_temperatures'][-1] if history['teacher_temperatures'] else None,\n",
    "            'ece_values': ece_values\n",
    "        }, final_model_path)\n",
    "        logger.info(f\"Final model exported to {final_model_path}\")\n",
    "        \n",
    "        # Final GPU memory stats\n",
    "        print_gpu_memory_stats()\n",
    "        \n",
    "        logger.info(\"Calibration-aware ensemble distillation completed successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
