{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52dd3a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 13:54:13,135 [INFO] - Using device: cuda\n",
      "2025-05-06 13:54:13,148 [INFO] - GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "2025-05-06 13:54:13,148 [INFO] - Available memory: 6.44 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Mutual Learning Models CIFAR-10 Evaluation Pipeline\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_18668\\1186199743.py:458: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "2025-05-06 13:54:13,339 [INFO] - Detected EfficientNet architecture in checkpoint\n",
      "2025-05-06 13:54:13,339 [INFO] - Creating efficientnet model architecture...\n",
      "2025-05-06 13:54:13,452 [INFO] - Student model state loaded from 'model_state_dict'\n",
      "2025-05-06 13:54:13,452 [INFO] - Previous test metrics found in checkpoint:\n",
      "2025-05-06 13:54:13,452 [INFO] -   - loss: 0.17353186165094375\n",
      "2025-05-06 13:54:13,452 [INFO] -   - accuracy: 94.77\n",
      "2025-05-06 13:54:13,452 [INFO] -   - f1_score: 0.9475858735959616\n",
      "2025-05-06 13:54:13,452 [INFO] -   - precision: 0.9478610547138802\n",
      "2025-05-06 13:54:13,452 [INFO] -   - recall: 0.9477\n",
      "2025-05-06 13:54:13,452 [INFO] -   - ece: 0.013810415752232075\n",
      "2025-05-06 13:54:13,497 [INFO] - Student model loaded successfully and set to evaluation mode\n",
      "2025-05-06 13:54:13,502 [INFO] - Loading vit model from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\vit_mutual_final_best.pth\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_18668\\1186199743.py:366: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "2025-05-06 13:54:13,852 [INFO] - Creating vit model architecture...\n",
      "2025-05-06 13:54:14,396 [INFO] - Detected ViT architecture in vit checkpoint\n",
      "2025-05-06 13:54:14,494 [INFO] - Model vit state loaded directly from checkpoint\n",
      "2025-05-06 13:54:14,577 [INFO] - Model vit loaded successfully and set to evaluation mode\n",
      "2025-05-06 13:54:14,577 [INFO] - Loading efficientnet model from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\efficientnet_mutual_final_best.pth\n",
      "2025-05-06 13:54:14,677 [INFO] - Creating efficientnet model architecture...\n",
      "2025-05-06 13:54:14,736 [INFO] - Detected EfficientNet architecture in efficientnet checkpoint\n",
      "2025-05-06 13:54:14,760 [INFO] - Model efficientnet state loaded directly from checkpoint\n",
      "2025-05-06 13:54:14,785 [INFO] - Model efficientnet loaded successfully and set to evaluation mode\n",
      "2025-05-06 13:54:14,787 [INFO] - Loading inception model from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\inception_mutual_final_best.pth\n",
      "2025-05-06 13:54:14,970 [INFO] - Creating inception model architecture...\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "2025-05-06 13:54:15,343 [INFO] - Detected InceptionV3 architecture in inception checkpoint\n",
      "2025-05-06 13:54:15,392 [ERROR] - Failed to load inception model: Error(s) in loading state_dict for InceptionV3Wrapper:\n",
      "\tMissing key(s) in state_dict: \"inception.Conv2d_1a_3x3.conv.weight\", \"inception.Conv2d_1a_3x3.bn.weight\", \"inception.Conv2d_1a_3x3.bn.bias\", \"inception.Conv2d_1a_3x3.bn.running_mean\", \"inception.Conv2d_1a_3x3.bn.running_var\", \"inception.Conv2d_2a_3x3.conv.weight\", \"inception.Conv2d_2a_3x3.bn.weight\", \"inception.Conv2d_2a_3x3.bn.bias\", \"inception.Conv2d_2a_3x3.bn.running_mean\", \"inception.Conv2d_2a_3x3.bn.running_var\", \"inception.Conv2d_2b_3x3.conv.weight\", \"inception.Conv2d_2b_3x3.bn.weight\", \"inception.Conv2d_2b_3x3.bn.bias\", \"inception.Conv2d_2b_3x3.bn.running_mean\", \"inception.Conv2d_2b_3x3.bn.running_var\", \"inception.Conv2d_3b_1x1.conv.weight\", \"inception.Conv2d_3b_1x1.bn.weight\", \"inception.Conv2d_3b_1x1.bn.bias\", \"inception.Conv2d_3b_1x1.bn.running_mean\", \"inception.Conv2d_3b_1x1.bn.running_var\", \"inception.Conv2d_4a_3x3.conv.weight\", \"inception.Conv2d_4a_3x3.bn.weight\", \"inception.Conv2d_4a_3x3.bn.bias\", \"inception.Conv2d_4a_3x3.bn.running_mean\", \"inception.Conv2d_4a_3x3.bn.running_var\", \"inception.Mixed_5b.branch1x1.conv.weight\", \"inception.Mixed_5b.branch1x1.bn.weight\", \"inception.Mixed_5b.branch1x1.bn.bias\", \"inception.Mixed_5b.branch1x1.bn.running_mean\", \"inception.Mixed_5b.branch1x1.bn.running_var\", \"inception.Mixed_5b.branch5x5_1.conv.weight\", \"inception.Mixed_5b.branch5x5_1.bn.weight\", \"inception.Mixed_5b.branch5x5_1.bn.bias\", \"inception.Mixed_5b.branch5x5_1.bn.running_mean\", \"inception.Mixed_5b.branch5x5_1.bn.running_var\", \"inception.Mixed_5b.branch5x5_2.conv.weight\", \"inception.Mixed_5b.branch5x5_2.bn.weight\", \"inception.Mixed_5b.branch5x5_2.bn.bias\", \"inception.Mixed_5b.branch5x5_2.bn.running_mean\", \"inception.Mixed_5b.branch5x5_2.bn.running_var\", \"inception.Mixed_5b.branch3x3dbl_1.conv.weight\", \"inception.Mixed_5b.branch3x3dbl_1.bn.weight\", \"inception.Mixed_5b.branch3x3dbl_1.bn.bias\", \"inception.Mixed_5b.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_5b.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_5b.branch3x3dbl_2.conv.weight\", \"inception.Mixed_5b.branch3x3dbl_2.bn.weight\", \"inception.Mixed_5b.branch3x3dbl_2.bn.bias\", \"inception.Mixed_5b.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_5b.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_5b.branch3x3dbl_3.conv.weight\", \"inception.Mixed_5b.branch3x3dbl_3.bn.weight\", \"inception.Mixed_5b.branch3x3dbl_3.bn.bias\", \"inception.Mixed_5b.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_5b.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_5b.branch_pool.conv.weight\", \"inception.Mixed_5b.branch_pool.bn.weight\", \"inception.Mixed_5b.branch_pool.bn.bias\", \"inception.Mixed_5b.branch_pool.bn.running_mean\", \"inception.Mixed_5b.branch_pool.bn.running_var\", \"inception.Mixed_5c.branch1x1.conv.weight\", \"inception.Mixed_5c.branch1x1.bn.weight\", \"inception.Mixed_5c.branch1x1.bn.bias\", \"inception.Mixed_5c.branch1x1.bn.running_mean\", \"inception.Mixed_5c.branch1x1.bn.running_var\", \"inception.Mixed_5c.branch5x5_1.conv.weight\", \"inception.Mixed_5c.branch5x5_1.bn.weight\", \"inception.Mixed_5c.branch5x5_1.bn.bias\", \"inception.Mixed_5c.branch5x5_1.bn.running_mean\", \"inception.Mixed_5c.branch5x5_1.bn.running_var\", \"inception.Mixed_5c.branch5x5_2.conv.weight\", \"inception.Mixed_5c.branch5x5_2.bn.weight\", \"inception.Mixed_5c.branch5x5_2.bn.bias\", \"inception.Mixed_5c.branch5x5_2.bn.running_mean\", \"inception.Mixed_5c.branch5x5_2.bn.running_var\", \"inception.Mixed_5c.branch3x3dbl_1.conv.weight\", \"inception.Mixed_5c.branch3x3dbl_1.bn.weight\", \"inception.Mixed_5c.branch3x3dbl_1.bn.bias\", \"inception.Mixed_5c.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_5c.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_5c.branch3x3dbl_2.conv.weight\", \"inception.Mixed_5c.branch3x3dbl_2.bn.weight\", \"inception.Mixed_5c.branch3x3dbl_2.bn.bias\", \"inception.Mixed_5c.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_5c.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_5c.branch3x3dbl_3.conv.weight\", \"inception.Mixed_5c.branch3x3dbl_3.bn.weight\", \"inception.Mixed_5c.branch3x3dbl_3.bn.bias\", \"inception.Mixed_5c.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_5c.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_5c.branch_pool.conv.weight\", \"inception.Mixed_5c.branch_pool.bn.weight\", \"inception.Mixed_5c.branch_pool.bn.bias\", \"inception.Mixed_5c.branch_pool.bn.running_mean\", \"inception.Mixed_5c.branch_pool.bn.running_var\", \"inception.Mixed_5d.branch1x1.conv.weight\", \"inception.Mixed_5d.branch1x1.bn.weight\", \"inception.Mixed_5d.branch1x1.bn.bias\", \"inception.Mixed_5d.branch1x1.bn.running_mean\", \"inception.Mixed_5d.branch1x1.bn.running_var\", \"inception.Mixed_5d.branch5x5_1.conv.weight\", \"inception.Mixed_5d.branch5x5_1.bn.weight\", \"inception.Mixed_5d.branch5x5_1.bn.bias\", \"inception.Mixed_5d.branch5x5_1.bn.running_mean\", \"inception.Mixed_5d.branch5x5_1.bn.running_var\", \"inception.Mixed_5d.branch5x5_2.conv.weight\", \"inception.Mixed_5d.branch5x5_2.bn.weight\", \"inception.Mixed_5d.branch5x5_2.bn.bias\", \"inception.Mixed_5d.branch5x5_2.bn.running_mean\", \"inception.Mixed_5d.branch5x5_2.bn.running_var\", \"inception.Mixed_5d.branch3x3dbl_1.conv.weight\", \"inception.Mixed_5d.branch3x3dbl_1.bn.weight\", \"inception.Mixed_5d.branch3x3dbl_1.bn.bias\", \"inception.Mixed_5d.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_5d.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_5d.branch3x3dbl_2.conv.weight\", \"inception.Mixed_5d.branch3x3dbl_2.bn.weight\", \"inception.Mixed_5d.branch3x3dbl_2.bn.bias\", \"inception.Mixed_5d.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_5d.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_5d.branch3x3dbl_3.conv.weight\", \"inception.Mixed_5d.branch3x3dbl_3.bn.weight\", \"inception.Mixed_5d.branch3x3dbl_3.bn.bias\", \"inception.Mixed_5d.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_5d.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_5d.branch_pool.conv.weight\", \"inception.Mixed_5d.branch_pool.bn.weight\", \"inception.Mixed_5d.branch_pool.bn.bias\", \"inception.Mixed_5d.branch_pool.bn.running_mean\", \"inception.Mixed_5d.branch_pool.bn.running_var\", \"inception.Mixed_6a.branch3x3.conv.weight\", \"inception.Mixed_6a.branch3x3.bn.weight\", \"inception.Mixed_6a.branch3x3.bn.bias\", \"inception.Mixed_6a.branch3x3.bn.running_mean\", \"inception.Mixed_6a.branch3x3.bn.running_var\", \"inception.Mixed_6a.branch3x3dbl_1.conv.weight\", \"inception.Mixed_6a.branch3x3dbl_1.bn.weight\", \"inception.Mixed_6a.branch3x3dbl_1.bn.bias\", \"inception.Mixed_6a.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_6a.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_6a.branch3x3dbl_2.conv.weight\", \"inception.Mixed_6a.branch3x3dbl_2.bn.weight\", \"inception.Mixed_6a.branch3x3dbl_2.bn.bias\", \"inception.Mixed_6a.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_6a.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_6a.branch3x3dbl_3.conv.weight\", \"inception.Mixed_6a.branch3x3dbl_3.bn.weight\", \"inception.Mixed_6a.branch3x3dbl_3.bn.bias\", \"inception.Mixed_6a.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_6a.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_6b.branch1x1.conv.weight\", \"inception.Mixed_6b.branch1x1.bn.weight\", \"inception.Mixed_6b.branch1x1.bn.bias\", \"inception.Mixed_6b.branch1x1.bn.running_mean\", \"inception.Mixed_6b.branch1x1.bn.running_var\", \"inception.Mixed_6b.branch7x7_1.conv.weight\", \"inception.Mixed_6b.branch7x7_1.bn.weight\", \"inception.Mixed_6b.branch7x7_1.bn.bias\", \"inception.Mixed_6b.branch7x7_1.bn.running_mean\", \"inception.Mixed_6b.branch7x7_1.bn.running_var\", \"inception.Mixed_6b.branch7x7_2.conv.weight\", \"inception.Mixed_6b.branch7x7_2.bn.weight\", \"inception.Mixed_6b.branch7x7_2.bn.bias\", \"inception.Mixed_6b.branch7x7_2.bn.running_mean\", \"inception.Mixed_6b.branch7x7_2.bn.running_var\", \"inception.Mixed_6b.branch7x7_3.conv.weight\", \"inception.Mixed_6b.branch7x7_3.bn.weight\", \"inception.Mixed_6b.branch7x7_3.bn.bias\", \"inception.Mixed_6b.branch7x7_3.bn.running_mean\", \"inception.Mixed_6b.branch7x7_3.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6b.branch_pool.conv.weight\", \"inception.Mixed_6b.branch_pool.bn.weight\", \"inception.Mixed_6b.branch_pool.bn.bias\", \"inception.Mixed_6b.branch_pool.bn.running_mean\", \"inception.Mixed_6b.branch_pool.bn.running_var\", \"inception.Mixed_6c.branch1x1.conv.weight\", \"inception.Mixed_6c.branch1x1.bn.weight\", \"inception.Mixed_6c.branch1x1.bn.bias\", \"inception.Mixed_6c.branch1x1.bn.running_mean\", \"inception.Mixed_6c.branch1x1.bn.running_var\", \"inception.Mixed_6c.branch7x7_1.conv.weight\", \"inception.Mixed_6c.branch7x7_1.bn.weight\", \"inception.Mixed_6c.branch7x7_1.bn.bias\", \"inception.Mixed_6c.branch7x7_1.bn.running_mean\", \"inception.Mixed_6c.branch7x7_1.bn.running_var\", \"inception.Mixed_6c.branch7x7_2.conv.weight\", \"inception.Mixed_6c.branch7x7_2.bn.weight\", \"inception.Mixed_6c.branch7x7_2.bn.bias\", \"inception.Mixed_6c.branch7x7_2.bn.running_mean\", \"inception.Mixed_6c.branch7x7_2.bn.running_var\", \"inception.Mixed_6c.branch7x7_3.conv.weight\", \"inception.Mixed_6c.branch7x7_3.bn.weight\", \"inception.Mixed_6c.branch7x7_3.bn.bias\", \"inception.Mixed_6c.branch7x7_3.bn.running_mean\", \"inception.Mixed_6c.branch7x7_3.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6c.branch_pool.conv.weight\", \"inception.Mixed_6c.branch_pool.bn.weight\", \"inception.Mixed_6c.branch_pool.bn.bias\", \"inception.Mixed_6c.branch_pool.bn.running_mean\", \"inception.Mixed_6c.branch_pool.bn.running_var\", \"inception.Mixed_6d.branch1x1.conv.weight\", \"inception.Mixed_6d.branch1x1.bn.weight\", \"inception.Mixed_6d.branch1x1.bn.bias\", \"inception.Mixed_6d.branch1x1.bn.running_mean\", \"inception.Mixed_6d.branch1x1.bn.running_var\", \"inception.Mixed_6d.branch7x7_1.conv.weight\", \"inception.Mixed_6d.branch7x7_1.bn.weight\", \"inception.Mixed_6d.branch7x7_1.bn.bias\", \"inception.Mixed_6d.branch7x7_1.bn.running_mean\", \"inception.Mixed_6d.branch7x7_1.bn.running_var\", \"inception.Mixed_6d.branch7x7_2.conv.weight\", \"inception.Mixed_6d.branch7x7_2.bn.weight\", \"inception.Mixed_6d.branch7x7_2.bn.bias\", \"inception.Mixed_6d.branch7x7_2.bn.running_mean\", \"inception.Mixed_6d.branch7x7_2.bn.running_var\", \"inception.Mixed_6d.branch7x7_3.conv.weight\", \"inception.Mixed_6d.branch7x7_3.bn.weight\", \"inception.Mixed_6d.branch7x7_3.bn.bias\", \"inception.Mixed_6d.branch7x7_3.bn.running_mean\", \"inception.Mixed_6d.branch7x7_3.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6d.branch_pool.conv.weight\", \"inception.Mixed_6d.branch_pool.bn.weight\", \"inception.Mixed_6d.branch_pool.bn.bias\", \"inception.Mixed_6d.branch_pool.bn.running_mean\", \"inception.Mixed_6d.branch_pool.bn.running_var\", \"inception.Mixed_6e.branch1x1.conv.weight\", \"inception.Mixed_6e.branch1x1.bn.weight\", \"inception.Mixed_6e.branch1x1.bn.bias\", \"inception.Mixed_6e.branch1x1.bn.running_mean\", \"inception.Mixed_6e.branch1x1.bn.running_var\", \"inception.Mixed_6e.branch7x7_1.conv.weight\", \"inception.Mixed_6e.branch7x7_1.bn.weight\", \"inception.Mixed_6e.branch7x7_1.bn.bias\", \"inception.Mixed_6e.branch7x7_1.bn.running_mean\", \"inception.Mixed_6e.branch7x7_1.bn.running_var\", \"inception.Mixed_6e.branch7x7_2.conv.weight\", \"inception.Mixed_6e.branch7x7_2.bn.weight\", \"inception.Mixed_6e.branch7x7_2.bn.bias\", \"inception.Mixed_6e.branch7x7_2.bn.running_mean\", \"inception.Mixed_6e.branch7x7_2.bn.running_var\", \"inception.Mixed_6e.branch7x7_3.conv.weight\", \"inception.Mixed_6e.branch7x7_3.bn.weight\", \"inception.Mixed_6e.branch7x7_3.bn.bias\", \"inception.Mixed_6e.branch7x7_3.bn.running_mean\", \"inception.Mixed_6e.branch7x7_3.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6e.branch_pool.conv.weight\", \"inception.Mixed_6e.branch_pool.bn.weight\", \"inception.Mixed_6e.branch_pool.bn.bias\", \"inception.Mixed_6e.branch_pool.bn.running_mean\", \"inception.Mixed_6e.branch_pool.bn.running_var\", \"inception.AuxLogits.conv0.conv.weight\", \"inception.AuxLogits.conv0.bn.weight\", \"inception.AuxLogits.conv0.bn.bias\", \"inception.AuxLogits.conv0.bn.running_mean\", \"inception.AuxLogits.conv0.bn.running_var\", \"inception.AuxLogits.conv1.conv.weight\", \"inception.AuxLogits.conv1.bn.weight\", \"inception.AuxLogits.conv1.bn.bias\", \"inception.AuxLogits.conv1.bn.running_mean\", \"inception.AuxLogits.conv1.bn.running_var\", \"inception.AuxLogits.fc.weight\", \"inception.AuxLogits.fc.bias\", \"inception.Mixed_7a.branch3x3_1.conv.weight\", \"inception.Mixed_7a.branch3x3_1.bn.weight\", \"inception.Mixed_7a.branch3x3_1.bn.bias\", \"inception.Mixed_7a.branch3x3_1.bn.running_mean\", \"inception.Mixed_7a.branch3x3_1.bn.running_var\", \"inception.Mixed_7a.branch3x3_2.conv.weight\", \"inception.Mixed_7a.branch3x3_2.bn.weight\", \"inception.Mixed_7a.branch3x3_2.bn.bias\", \"inception.Mixed_7a.branch3x3_2.bn.running_mean\", \"inception.Mixed_7a.branch3x3_2.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_1.conv.weight\", \"inception.Mixed_7a.branch7x7x3_1.bn.weight\", \"inception.Mixed_7a.branch7x7x3_1.bn.bias\", \"inception.Mixed_7a.branch7x7x3_1.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_1.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_2.conv.weight\", \"inception.Mixed_7a.branch7x7x3_2.bn.weight\", \"inception.Mixed_7a.branch7x7x3_2.bn.bias\", \"inception.Mixed_7a.branch7x7x3_2.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_2.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_3.conv.weight\", \"inception.Mixed_7a.branch7x7x3_3.bn.weight\", \"inception.Mixed_7a.branch7x7x3_3.bn.bias\", \"inception.Mixed_7a.branch7x7x3_3.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_3.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_4.conv.weight\", \"inception.Mixed_7a.branch7x7x3_4.bn.weight\", \"inception.Mixed_7a.branch7x7x3_4.bn.bias\", \"inception.Mixed_7a.branch7x7x3_4.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_4.bn.running_var\", \"inception.Mixed_7b.branch1x1.conv.weight\", \"inception.Mixed_7b.branch1x1.bn.weight\", \"inception.Mixed_7b.branch1x1.bn.bias\", \"inception.Mixed_7b.branch1x1.bn.running_mean\", \"inception.Mixed_7b.branch1x1.bn.running_var\", \"inception.Mixed_7b.branch3x3_1.conv.weight\", \"inception.Mixed_7b.branch3x3_1.bn.weight\", \"inception.Mixed_7b.branch3x3_1.bn.bias\", \"inception.Mixed_7b.branch3x3_1.bn.running_mean\", \"inception.Mixed_7b.branch3x3_1.bn.running_var\", \"inception.Mixed_7b.branch3x3_2a.conv.weight\", \"inception.Mixed_7b.branch3x3_2a.bn.weight\", \"inception.Mixed_7b.branch3x3_2a.bn.bias\", \"inception.Mixed_7b.branch3x3_2a.bn.running_mean\", \"inception.Mixed_7b.branch3x3_2a.bn.running_var\", \"inception.Mixed_7b.branch3x3_2b.conv.weight\", \"inception.Mixed_7b.branch3x3_2b.bn.weight\", \"inception.Mixed_7b.branch3x3_2b.bn.bias\", \"inception.Mixed_7b.branch3x3_2b.bn.running_mean\", \"inception.Mixed_7b.branch3x3_2b.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_1.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_1.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_1.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_2.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_2.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_2.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_3a.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_3b.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.running_var\", \"inception.Mixed_7b.branch_pool.conv.weight\", \"inception.Mixed_7b.branch_pool.bn.weight\", \"inception.Mixed_7b.branch_pool.bn.bias\", \"inception.Mixed_7b.branch_pool.bn.running_mean\", \"inception.Mixed_7b.branch_pool.bn.running_var\", \"inception.Mixed_7c.branch1x1.conv.weight\", \"inception.Mixed_7c.branch1x1.bn.weight\", \"inception.Mixed_7c.branch1x1.bn.bias\", \"inception.Mixed_7c.branch1x1.bn.running_mean\", \"inception.Mixed_7c.branch1x1.bn.running_var\", \"inception.Mixed_7c.branch3x3_1.conv.weight\", \"inception.Mixed_7c.branch3x3_1.bn.weight\", \"inception.Mixed_7c.branch3x3_1.bn.bias\", \"inception.Mixed_7c.branch3x3_1.bn.running_mean\", \"inception.Mixed_7c.branch3x3_1.bn.running_var\", \"inception.Mixed_7c.branch3x3_2a.conv.weight\", \"inception.Mixed_7c.branch3x3_2a.bn.weight\", \"inception.Mixed_7c.branch3x3_2a.bn.bias\", \"inception.Mixed_7c.branch3x3_2a.bn.running_mean\", \"inception.Mixed_7c.branch3x3_2a.bn.running_var\", \"inception.Mixed_7c.branch3x3_2b.conv.weight\", \"inception.Mixed_7c.branch3x3_2b.bn.weight\", \"inception.Mixed_7c.branch3x3_2b.bn.bias\", \"inception.Mixed_7c.branch3x3_2b.bn.running_mean\", \"inception.Mixed_7c.branch3x3_2b.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_1.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_1.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_1.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_2.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_2.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_2.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_3a.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_3b.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.running_var\", \"inception.Mixed_7c.branch_pool.conv.weight\", \"inception.Mixed_7c.branch_pool.bn.weight\", \"inception.Mixed_7c.branch_pool.bn.bias\", \"inception.Mixed_7c.branch_pool.bn.running_mean\", \"inception.Mixed_7c.branch_pool.bn.running_var\", \"inception.fc.weight\", \"inception.fc.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"AuxLogits.conv0.conv.weight\", \"AuxLogits.conv0.bn.weight\", \"AuxLogits.conv0.bn.bias\", \"AuxLogits.conv0.bn.running_mean\", \"AuxLogits.conv0.bn.running_var\", \"AuxLogits.conv0.bn.num_batches_tracked\", \"AuxLogits.conv1.conv.weight\", \"AuxLogits.conv1.bn.weight\", \"AuxLogits.conv1.bn.bias\", \"AuxLogits.conv1.bn.running_mean\", \"AuxLogits.conv1.bn.running_var\", \"AuxLogits.conv1.bn.num_batches_tracked\", \"AuxLogits.fc.weight\", \"AuxLogits.fc.bias\". \n",
      "2025-05-06 13:54:15,392 [ERROR] - Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_18668\\1186199743.py\", line 439, in load_model\n",
      "    model.load_state_dict(checkpoint)\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 2584, in load_state_dict\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Error(s) in loading state_dict for InceptionV3Wrapper:\n",
      "\tMissing key(s) in state_dict: \"inception.Conv2d_1a_3x3.conv.weight\", \"inception.Conv2d_1a_3x3.bn.weight\", \"inception.Conv2d_1a_3x3.bn.bias\", \"inception.Conv2d_1a_3x3.bn.running_mean\", \"inception.Conv2d_1a_3x3.bn.running_var\", \"inception.Conv2d_2a_3x3.conv.weight\", \"inception.Conv2d_2a_3x3.bn.weight\", \"inception.Conv2d_2a_3x3.bn.bias\", \"inception.Conv2d_2a_3x3.bn.running_mean\", \"inception.Conv2d_2a_3x3.bn.running_var\", \"inception.Conv2d_2b_3x3.conv.weight\", \"inception.Conv2d_2b_3x3.bn.weight\", \"inception.Conv2d_2b_3x3.bn.bias\", \"inception.Conv2d_2b_3x3.bn.running_mean\", \"inception.Conv2d_2b_3x3.bn.running_var\", \"inception.Conv2d_3b_1x1.conv.weight\", \"inception.Conv2d_3b_1x1.bn.weight\", \"inception.Conv2d_3b_1x1.bn.bias\", \"inception.Conv2d_3b_1x1.bn.running_mean\", \"inception.Conv2d_3b_1x1.bn.running_var\", \"inception.Conv2d_4a_3x3.conv.weight\", \"inception.Conv2d_4a_3x3.bn.weight\", \"inception.Conv2d_4a_3x3.bn.bias\", \"inception.Conv2d_4a_3x3.bn.running_mean\", \"inception.Conv2d_4a_3x3.bn.running_var\", \"inception.Mixed_5b.branch1x1.conv.weight\", \"inception.Mixed_5b.branch1x1.bn.weight\", \"inception.Mixed_5b.branch1x1.bn.bias\", \"inception.Mixed_5b.branch1x1.bn.running_mean\", \"inception.Mixed_5b.branch1x1.bn.running_var\", \"inception.Mixed_5b.branch5x5_1.conv.weight\", \"inception.Mixed_5b.branch5x5_1.bn.weight\", \"inception.Mixed_5b.branch5x5_1.bn.bias\", \"inception.Mixed_5b.branch5x5_1.bn.running_mean\", \"inception.Mixed_5b.branch5x5_1.bn.running_var\", \"inception.Mixed_5b.branch5x5_2.conv.weight\", \"inception.Mixed_5b.branch5x5_2.bn.weight\", \"inception.Mixed_5b.branch5x5_2.bn.bias\", \"inception.Mixed_5b.branch5x5_2.bn.running_mean\", \"inception.Mixed_5b.branch5x5_2.bn.running_var\", \"inception.Mixed_5b.branch3x3dbl_1.conv.weight\", \"inception.Mixed_5b.branch3x3dbl_1.bn.weight\", \"inception.Mixed_5b.branch3x3dbl_1.bn.bias\", \"inception.Mixed_5b.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_5b.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_5b.branch3x3dbl_2.conv.weight\", \"inception.Mixed_5b.branch3x3dbl_2.bn.weight\", \"inception.Mixed_5b.branch3x3dbl_2.bn.bias\", \"inception.Mixed_5b.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_5b.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_5b.branch3x3dbl_3.conv.weight\", \"inception.Mixed_5b.branch3x3dbl_3.bn.weight\", \"inception.Mixed_5b.branch3x3dbl_3.bn.bias\", \"inception.Mixed_5b.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_5b.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_5b.branch_pool.conv.weight\", \"inception.Mixed_5b.branch_pool.bn.weight\", \"inception.Mixed_5b.branch_pool.bn.bias\", \"inception.Mixed_5b.branch_pool.bn.running_mean\", \"inception.Mixed_5b.branch_pool.bn.running_var\", \"inception.Mixed_5c.branch1x1.conv.weight\", \"inception.Mixed_5c.branch1x1.bn.weight\", \"inception.Mixed_5c.branch1x1.bn.bias\", \"inception.Mixed_5c.branch1x1.bn.running_mean\", \"inception.Mixed_5c.branch1x1.bn.running_var\", \"inception.Mixed_5c.branch5x5_1.conv.weight\", \"inception.Mixed_5c.branch5x5_1.bn.weight\", \"inception.Mixed_5c.branch5x5_1.bn.bias\", \"inception.Mixed_5c.branch5x5_1.bn.running_mean\", \"inception.Mixed_5c.branch5x5_1.bn.running_var\", \"inception.Mixed_5c.branch5x5_2.conv.weight\", \"inception.Mixed_5c.branch5x5_2.bn.weight\", \"inception.Mixed_5c.branch5x5_2.bn.bias\", \"inception.Mixed_5c.branch5x5_2.bn.running_mean\", \"inception.Mixed_5c.branch5x5_2.bn.running_var\", \"inception.Mixed_5c.branch3x3dbl_1.conv.weight\", \"inception.Mixed_5c.branch3x3dbl_1.bn.weight\", \"inception.Mixed_5c.branch3x3dbl_1.bn.bias\", \"inception.Mixed_5c.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_5c.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_5c.branch3x3dbl_2.conv.weight\", \"inception.Mixed_5c.branch3x3dbl_2.bn.weight\", \"inception.Mixed_5c.branch3x3dbl_2.bn.bias\", \"inception.Mixed_5c.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_5c.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_5c.branch3x3dbl_3.conv.weight\", \"inception.Mixed_5c.branch3x3dbl_3.bn.weight\", \"inception.Mixed_5c.branch3x3dbl_3.bn.bias\", \"inception.Mixed_5c.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_5c.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_5c.branch_pool.conv.weight\", \"inception.Mixed_5c.branch_pool.bn.weight\", \"inception.Mixed_5c.branch_pool.bn.bias\", \"inception.Mixed_5c.branch_pool.bn.running_mean\", \"inception.Mixed_5c.branch_pool.bn.running_var\", \"inception.Mixed_5d.branch1x1.conv.weight\", \"inception.Mixed_5d.branch1x1.bn.weight\", \"inception.Mixed_5d.branch1x1.bn.bias\", \"inception.Mixed_5d.branch1x1.bn.running_mean\", \"inception.Mixed_5d.branch1x1.bn.running_var\", \"inception.Mixed_5d.branch5x5_1.conv.weight\", \"inception.Mixed_5d.branch5x5_1.bn.weight\", \"inception.Mixed_5d.branch5x5_1.bn.bias\", \"inception.Mixed_5d.branch5x5_1.bn.running_mean\", \"inception.Mixed_5d.branch5x5_1.bn.running_var\", \"inception.Mixed_5d.branch5x5_2.conv.weight\", \"inception.Mixed_5d.branch5x5_2.bn.weight\", \"inception.Mixed_5d.branch5x5_2.bn.bias\", \"inception.Mixed_5d.branch5x5_2.bn.running_mean\", \"inception.Mixed_5d.branch5x5_2.bn.running_var\", \"inception.Mixed_5d.branch3x3dbl_1.conv.weight\", \"inception.Mixed_5d.branch3x3dbl_1.bn.weight\", \"inception.Mixed_5d.branch3x3dbl_1.bn.bias\", \"inception.Mixed_5d.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_5d.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_5d.branch3x3dbl_2.conv.weight\", \"inception.Mixed_5d.branch3x3dbl_2.bn.weight\", \"inception.Mixed_5d.branch3x3dbl_2.bn.bias\", \"inception.Mixed_5d.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_5d.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_5d.branch3x3dbl_3.conv.weight\", \"inception.Mixed_5d.branch3x3dbl_3.bn.weight\", \"inception.Mixed_5d.branch3x3dbl_3.bn.bias\", \"inception.Mixed_5d.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_5d.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_5d.branch_pool.conv.weight\", \"inception.Mixed_5d.branch_pool.bn.weight\", \"inception.Mixed_5d.branch_pool.bn.bias\", \"inception.Mixed_5d.branch_pool.bn.running_mean\", \"inception.Mixed_5d.branch_pool.bn.running_var\", \"inception.Mixed_6a.branch3x3.conv.weight\", \"inception.Mixed_6a.branch3x3.bn.weight\", \"inception.Mixed_6a.branch3x3.bn.bias\", \"inception.Mixed_6a.branch3x3.bn.running_mean\", \"inception.Mixed_6a.branch3x3.bn.running_var\", \"inception.Mixed_6a.branch3x3dbl_1.conv.weight\", \"inception.Mixed_6a.branch3x3dbl_1.bn.weight\", \"inception.Mixed_6a.branch3x3dbl_1.bn.bias\", \"inception.Mixed_6a.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_6a.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_6a.branch3x3dbl_2.conv.weight\", \"inception.Mixed_6a.branch3x3dbl_2.bn.weight\", \"inception.Mixed_6a.branch3x3dbl_2.bn.bias\", \"inception.Mixed_6a.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_6a.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_6a.branch3x3dbl_3.conv.weight\", \"inception.Mixed_6a.branch3x3dbl_3.bn.weight\", \"inception.Mixed_6a.branch3x3dbl_3.bn.bias\", \"inception.Mixed_6a.branch3x3dbl_3.bn.running_mean\", \"inception.Mixed_6a.branch3x3dbl_3.bn.running_var\", \"inception.Mixed_6b.branch1x1.conv.weight\", \"inception.Mixed_6b.branch1x1.bn.weight\", \"inception.Mixed_6b.branch1x1.bn.bias\", \"inception.Mixed_6b.branch1x1.bn.running_mean\", \"inception.Mixed_6b.branch1x1.bn.running_var\", \"inception.Mixed_6b.branch7x7_1.conv.weight\", \"inception.Mixed_6b.branch7x7_1.bn.weight\", \"inception.Mixed_6b.branch7x7_1.bn.bias\", \"inception.Mixed_6b.branch7x7_1.bn.running_mean\", \"inception.Mixed_6b.branch7x7_1.bn.running_var\", \"inception.Mixed_6b.branch7x7_2.conv.weight\", \"inception.Mixed_6b.branch7x7_2.bn.weight\", \"inception.Mixed_6b.branch7x7_2.bn.bias\", \"inception.Mixed_6b.branch7x7_2.bn.running_mean\", \"inception.Mixed_6b.branch7x7_2.bn.running_var\", \"inception.Mixed_6b.branch7x7_3.conv.weight\", \"inception.Mixed_6b.branch7x7_3.bn.weight\", \"inception.Mixed_6b.branch7x7_3.bn.bias\", \"inception.Mixed_6b.branch7x7_3.bn.running_mean\", \"inception.Mixed_6b.branch7x7_3.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6b.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6b.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6b.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6b.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6b.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6b.branch_pool.conv.weight\", \"inception.Mixed_6b.branch_pool.bn.weight\", \"inception.Mixed_6b.branch_pool.bn.bias\", \"inception.Mixed_6b.branch_pool.bn.running_mean\", \"inception.Mixed_6b.branch_pool.bn.running_var\", \"inception.Mixed_6c.branch1x1.conv.weight\", \"inception.Mixed_6c.branch1x1.bn.weight\", \"inception.Mixed_6c.branch1x1.bn.bias\", \"inception.Mixed_6c.branch1x1.bn.running_mean\", \"inception.Mixed_6c.branch1x1.bn.running_var\", \"inception.Mixed_6c.branch7x7_1.conv.weight\", \"inception.Mixed_6c.branch7x7_1.bn.weight\", \"inception.Mixed_6c.branch7x7_1.bn.bias\", \"inception.Mixed_6c.branch7x7_1.bn.running_mean\", \"inception.Mixed_6c.branch7x7_1.bn.running_var\", \"inception.Mixed_6c.branch7x7_2.conv.weight\", \"inception.Mixed_6c.branch7x7_2.bn.weight\", \"inception.Mixed_6c.branch7x7_2.bn.bias\", \"inception.Mixed_6c.branch7x7_2.bn.running_mean\", \"inception.Mixed_6c.branch7x7_2.bn.running_var\", \"inception.Mixed_6c.branch7x7_3.conv.weight\", \"inception.Mixed_6c.branch7x7_3.bn.weight\", \"inception.Mixed_6c.branch7x7_3.bn.bias\", \"inception.Mixed_6c.branch7x7_3.bn.running_mean\", \"inception.Mixed_6c.branch7x7_3.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6c.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6c.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6c.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6c.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6c.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6c.branch_pool.conv.weight\", \"inception.Mixed_6c.branch_pool.bn.weight\", \"inception.Mixed_6c.branch_pool.bn.bias\", \"inception.Mixed_6c.branch_pool.bn.running_mean\", \"inception.Mixed_6c.branch_pool.bn.running_var\", \"inception.Mixed_6d.branch1x1.conv.weight\", \"inception.Mixed_6d.branch1x1.bn.weight\", \"inception.Mixed_6d.branch1x1.bn.bias\", \"inception.Mixed_6d.branch1x1.bn.running_mean\", \"inception.Mixed_6d.branch1x1.bn.running_var\", \"inception.Mixed_6d.branch7x7_1.conv.weight\", \"inception.Mixed_6d.branch7x7_1.bn.weight\", \"inception.Mixed_6d.branch7x7_1.bn.bias\", \"inception.Mixed_6d.branch7x7_1.bn.running_mean\", \"inception.Mixed_6d.branch7x7_1.bn.running_var\", \"inception.Mixed_6d.branch7x7_2.conv.weight\", \"inception.Mixed_6d.branch7x7_2.bn.weight\", \"inception.Mixed_6d.branch7x7_2.bn.bias\", \"inception.Mixed_6d.branch7x7_2.bn.running_mean\", \"inception.Mixed_6d.branch7x7_2.bn.running_var\", \"inception.Mixed_6d.branch7x7_3.conv.weight\", \"inception.Mixed_6d.branch7x7_3.bn.weight\", \"inception.Mixed_6d.branch7x7_3.bn.bias\", \"inception.Mixed_6d.branch7x7_3.bn.running_mean\", \"inception.Mixed_6d.branch7x7_3.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6d.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6d.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6d.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6d.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6d.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6d.branch_pool.conv.weight\", \"inception.Mixed_6d.branch_pool.bn.weight\", \"inception.Mixed_6d.branch_pool.bn.bias\", \"inception.Mixed_6d.branch_pool.bn.running_mean\", \"inception.Mixed_6d.branch_pool.bn.running_var\", \"inception.Mixed_6e.branch1x1.conv.weight\", \"inception.Mixed_6e.branch1x1.bn.weight\", \"inception.Mixed_6e.branch1x1.bn.bias\", \"inception.Mixed_6e.branch1x1.bn.running_mean\", \"inception.Mixed_6e.branch1x1.bn.running_var\", \"inception.Mixed_6e.branch7x7_1.conv.weight\", \"inception.Mixed_6e.branch7x7_1.bn.weight\", \"inception.Mixed_6e.branch7x7_1.bn.bias\", \"inception.Mixed_6e.branch7x7_1.bn.running_mean\", \"inception.Mixed_6e.branch7x7_1.bn.running_var\", \"inception.Mixed_6e.branch7x7_2.conv.weight\", \"inception.Mixed_6e.branch7x7_2.bn.weight\", \"inception.Mixed_6e.branch7x7_2.bn.bias\", \"inception.Mixed_6e.branch7x7_2.bn.running_mean\", \"inception.Mixed_6e.branch7x7_2.bn.running_var\", \"inception.Mixed_6e.branch7x7_3.conv.weight\", \"inception.Mixed_6e.branch7x7_3.bn.weight\", \"inception.Mixed_6e.branch7x7_3.bn.bias\", \"inception.Mixed_6e.branch7x7_3.bn.running_mean\", \"inception.Mixed_6e.branch7x7_3.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_1.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_1.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_1.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_1.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_1.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_2.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_2.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_2.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_2.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_2.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_3.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_3.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_3.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_3.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_3.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_4.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_4.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_4.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_4.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_4.bn.running_var\", \"inception.Mixed_6e.branch7x7dbl_5.conv.weight\", \"inception.Mixed_6e.branch7x7dbl_5.bn.weight\", \"inception.Mixed_6e.branch7x7dbl_5.bn.bias\", \"inception.Mixed_6e.branch7x7dbl_5.bn.running_mean\", \"inception.Mixed_6e.branch7x7dbl_5.bn.running_var\", \"inception.Mixed_6e.branch_pool.conv.weight\", \"inception.Mixed_6e.branch_pool.bn.weight\", \"inception.Mixed_6e.branch_pool.bn.bias\", \"inception.Mixed_6e.branch_pool.bn.running_mean\", \"inception.Mixed_6e.branch_pool.bn.running_var\", \"inception.AuxLogits.conv0.conv.weight\", \"inception.AuxLogits.conv0.bn.weight\", \"inception.AuxLogits.conv0.bn.bias\", \"inception.AuxLogits.conv0.bn.running_mean\", \"inception.AuxLogits.conv0.bn.running_var\", \"inception.AuxLogits.conv1.conv.weight\", \"inception.AuxLogits.conv1.bn.weight\", \"inception.AuxLogits.conv1.bn.bias\", \"inception.AuxLogits.conv1.bn.running_mean\", \"inception.AuxLogits.conv1.bn.running_var\", \"inception.AuxLogits.fc.weight\", \"inception.AuxLogits.fc.bias\", \"inception.Mixed_7a.branch3x3_1.conv.weight\", \"inception.Mixed_7a.branch3x3_1.bn.weight\", \"inception.Mixed_7a.branch3x3_1.bn.bias\", \"inception.Mixed_7a.branch3x3_1.bn.running_mean\", \"inception.Mixed_7a.branch3x3_1.bn.running_var\", \"inception.Mixed_7a.branch3x3_2.conv.weight\", \"inception.Mixed_7a.branch3x3_2.bn.weight\", \"inception.Mixed_7a.branch3x3_2.bn.bias\", \"inception.Mixed_7a.branch3x3_2.bn.running_mean\", \"inception.Mixed_7a.branch3x3_2.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_1.conv.weight\", \"inception.Mixed_7a.branch7x7x3_1.bn.weight\", \"inception.Mixed_7a.branch7x7x3_1.bn.bias\", \"inception.Mixed_7a.branch7x7x3_1.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_1.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_2.conv.weight\", \"inception.Mixed_7a.branch7x7x3_2.bn.weight\", \"inception.Mixed_7a.branch7x7x3_2.bn.bias\", \"inception.Mixed_7a.branch7x7x3_2.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_2.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_3.conv.weight\", \"inception.Mixed_7a.branch7x7x3_3.bn.weight\", \"inception.Mixed_7a.branch7x7x3_3.bn.bias\", \"inception.Mixed_7a.branch7x7x3_3.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_3.bn.running_var\", \"inception.Mixed_7a.branch7x7x3_4.conv.weight\", \"inception.Mixed_7a.branch7x7x3_4.bn.weight\", \"inception.Mixed_7a.branch7x7x3_4.bn.bias\", \"inception.Mixed_7a.branch7x7x3_4.bn.running_mean\", \"inception.Mixed_7a.branch7x7x3_4.bn.running_var\", \"inception.Mixed_7b.branch1x1.conv.weight\", \"inception.Mixed_7b.branch1x1.bn.weight\", \"inception.Mixed_7b.branch1x1.bn.bias\", \"inception.Mixed_7b.branch1x1.bn.running_mean\", \"inception.Mixed_7b.branch1x1.bn.running_var\", \"inception.Mixed_7b.branch3x3_1.conv.weight\", \"inception.Mixed_7b.branch3x3_1.bn.weight\", \"inception.Mixed_7b.branch3x3_1.bn.bias\", \"inception.Mixed_7b.branch3x3_1.bn.running_mean\", \"inception.Mixed_7b.branch3x3_1.bn.running_var\", \"inception.Mixed_7b.branch3x3_2a.conv.weight\", \"inception.Mixed_7b.branch3x3_2a.bn.weight\", \"inception.Mixed_7b.branch3x3_2a.bn.bias\", \"inception.Mixed_7b.branch3x3_2a.bn.running_mean\", \"inception.Mixed_7b.branch3x3_2a.bn.running_var\", \"inception.Mixed_7b.branch3x3_2b.conv.weight\", \"inception.Mixed_7b.branch3x3_2b.bn.weight\", \"inception.Mixed_7b.branch3x3_2b.bn.bias\", \"inception.Mixed_7b.branch3x3_2b.bn.running_mean\", \"inception.Mixed_7b.branch3x3_2b.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_1.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_1.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_1.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_2.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_2.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_2.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_3a.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_3a.bn.running_var\", \"inception.Mixed_7b.branch3x3dbl_3b.conv.weight\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.weight\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.bias\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.running_mean\", \"inception.Mixed_7b.branch3x3dbl_3b.bn.running_var\", \"inception.Mixed_7b.branch_pool.conv.weight\", \"inception.Mixed_7b.branch_pool.bn.weight\", \"inception.Mixed_7b.branch_pool.bn.bias\", \"inception.Mixed_7b.branch_pool.bn.running_mean\", \"inception.Mixed_7b.branch_pool.bn.running_var\", \"inception.Mixed_7c.branch1x1.conv.weight\", \"inception.Mixed_7c.branch1x1.bn.weight\", \"inception.Mixed_7c.branch1x1.bn.bias\", \"inception.Mixed_7c.branch1x1.bn.running_mean\", \"inception.Mixed_7c.branch1x1.bn.running_var\", \"inception.Mixed_7c.branch3x3_1.conv.weight\", \"inception.Mixed_7c.branch3x3_1.bn.weight\", \"inception.Mixed_7c.branch3x3_1.bn.bias\", \"inception.Mixed_7c.branch3x3_1.bn.running_mean\", \"inception.Mixed_7c.branch3x3_1.bn.running_var\", \"inception.Mixed_7c.branch3x3_2a.conv.weight\", \"inception.Mixed_7c.branch3x3_2a.bn.weight\", \"inception.Mixed_7c.branch3x3_2a.bn.bias\", \"inception.Mixed_7c.branch3x3_2a.bn.running_mean\", \"inception.Mixed_7c.branch3x3_2a.bn.running_var\", \"inception.Mixed_7c.branch3x3_2b.conv.weight\", \"inception.Mixed_7c.branch3x3_2b.bn.weight\", \"inception.Mixed_7c.branch3x3_2b.bn.bias\", \"inception.Mixed_7c.branch3x3_2b.bn.running_mean\", \"inception.Mixed_7c.branch3x3_2b.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_1.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_1.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_1.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_1.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_1.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_2.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_2.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_2.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_2.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_2.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_3a.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_3a.bn.running_var\", \"inception.Mixed_7c.branch3x3dbl_3b.conv.weight\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.weight\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.bias\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.running_mean\", \"inception.Mixed_7c.branch3x3dbl_3b.bn.running_var\", \"inception.Mixed_7c.branch_pool.conv.weight\", \"inception.Mixed_7c.branch_pool.bn.weight\", \"inception.Mixed_7c.branch_pool.bn.bias\", \"inception.Mixed_7c.branch_pool.bn.running_mean\", \"inception.Mixed_7c.branch_pool.bn.running_var\", \"inception.fc.weight\", \"inception.fc.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"AuxLogits.conv0.conv.weight\", \"AuxLogits.conv0.bn.weight\", \"AuxLogits.conv0.bn.bias\", \"AuxLogits.conv0.bn.running_mean\", \"AuxLogits.conv0.bn.running_var\", \"AuxLogits.conv0.bn.num_batches_tracked\", \"AuxLogits.conv1.conv.weight\", \"AuxLogits.conv1.bn.weight\", \"AuxLogits.conv1.bn.bias\", \"AuxLogits.conv1.bn.running_mean\", \"AuxLogits.conv1.bn.running_var\", \"AuxLogits.conv1.bn.num_batches_tracked\", \"AuxLogits.fc.weight\", \"AuxLogits.fc.bias\". \n",
      "\n",
      "2025-05-06 13:54:15,392 [INFO] - Loading mobilenet model from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\mobilenet_mutual_final_best.pth\n",
      "2025-05-06 13:54:15,492 [INFO] - Creating efficientnet model architecture...\n",
      "2025-05-06 13:54:15,556 [INFO] - Detected EfficientNet architecture in mobilenet checkpoint\n",
      "2025-05-06 13:54:15,556 [ERROR] - Failed to load mobilenet model: Error(s) in loading state_dict for EfficientNet:\n",
      "\tMissing key(s) in state_dict: \"features.1.0.block.0.0.weight\", \"features.1.0.block.0.1.weight\", \"features.1.0.block.0.1.bias\", \"features.1.0.block.0.1.running_mean\", \"features.1.0.block.0.1.running_var\", \"features.1.0.block.1.fc1.weight\", \"features.1.0.block.1.fc1.bias\", \"features.1.0.block.1.fc2.weight\", \"features.1.0.block.1.fc2.bias\", \"features.1.0.block.2.0.weight\", \"features.1.0.block.2.1.weight\", \"features.1.0.block.2.1.bias\", \"features.1.0.block.2.1.running_mean\", \"features.1.0.block.2.1.running_var\", \"features.2.0.block.0.0.weight\", \"features.2.0.block.0.1.weight\", \"features.2.0.block.0.1.bias\", \"features.2.0.block.0.1.running_mean\", \"features.2.0.block.0.1.running_var\", \"features.2.0.block.1.0.weight\", \"features.2.0.block.1.1.weight\", \"features.2.0.block.1.1.bias\", \"features.2.0.block.1.1.running_mean\", \"features.2.0.block.1.1.running_var\", \"features.2.0.block.2.fc1.weight\", \"features.2.0.block.2.fc1.bias\", \"features.2.0.block.2.fc2.weight\", \"features.2.0.block.2.fc2.bias\", \"features.2.0.block.3.0.weight\", \"features.2.0.block.3.1.weight\", \"features.2.0.block.3.1.bias\", \"features.2.0.block.3.1.running_mean\", \"features.2.0.block.3.1.running_var\", \"features.2.1.block.0.0.weight\", \"features.2.1.block.0.1.weight\", \"features.2.1.block.0.1.bias\", \"features.2.1.block.0.1.running_mean\", \"features.2.1.block.0.1.running_var\", \"features.2.1.block.1.0.weight\", \"features.2.1.block.1.1.weight\", \"features.2.1.block.1.1.bias\", \"features.2.1.block.1.1.running_mean\", \"features.2.1.block.1.1.running_var\", \"features.2.1.block.2.fc1.weight\", \"features.2.1.block.2.fc1.bias\", \"features.2.1.block.2.fc2.weight\", \"features.2.1.block.2.fc2.bias\", \"features.2.1.block.3.0.weight\", \"features.2.1.block.3.1.weight\", \"features.2.1.block.3.1.bias\", \"features.2.1.block.3.1.running_mean\", \"features.2.1.block.3.1.running_var\", \"features.3.0.block.0.0.weight\", \"features.3.0.block.0.1.weight\", \"features.3.0.block.0.1.bias\", \"features.3.0.block.0.1.running_mean\", \"features.3.0.block.0.1.running_var\", \"features.3.0.block.1.0.weight\", \"features.3.0.block.1.1.weight\", \"features.3.0.block.1.1.bias\", \"features.3.0.block.1.1.running_mean\", \"features.3.0.block.1.1.running_var\", \"features.3.0.block.2.fc1.weight\", \"features.3.0.block.2.fc1.bias\", \"features.3.0.block.2.fc2.weight\", \"features.3.0.block.2.fc2.bias\", \"features.3.0.block.3.0.weight\", \"features.3.0.block.3.1.weight\", \"features.3.0.block.3.1.bias\", \"features.3.0.block.3.1.running_mean\", \"features.3.0.block.3.1.running_var\", \"features.3.1.block.0.0.weight\", \"features.3.1.block.0.1.weight\", \"features.3.1.block.0.1.bias\", \"features.3.1.block.0.1.running_mean\", \"features.3.1.block.0.1.running_var\", \"features.3.1.block.1.0.weight\", \"features.3.1.block.1.1.weight\", \"features.3.1.block.1.1.bias\", \"features.3.1.block.1.1.running_mean\", \"features.3.1.block.1.1.running_var\", \"features.3.1.block.2.fc1.weight\", \"features.3.1.block.2.fc1.bias\", \"features.3.1.block.2.fc2.weight\", \"features.3.1.block.2.fc2.bias\", \"features.3.1.block.3.0.weight\", \"features.3.1.block.3.1.weight\", \"features.3.1.block.3.1.bias\", \"features.3.1.block.3.1.running_mean\", \"features.3.1.block.3.1.running_var\", \"features.4.0.block.0.0.weight\", \"features.4.0.block.0.1.weight\", \"features.4.0.block.0.1.bias\", \"features.4.0.block.0.1.running_mean\", \"features.4.0.block.0.1.running_var\", \"features.4.0.block.1.0.weight\", \"features.4.0.block.1.1.weight\", \"features.4.0.block.1.1.bias\", \"features.4.0.block.1.1.running_mean\", \"features.4.0.block.1.1.running_var\", \"features.4.0.block.2.fc1.weight\", \"features.4.0.block.2.fc1.bias\", \"features.4.0.block.2.fc2.weight\", \"features.4.0.block.2.fc2.bias\", \"features.4.0.block.3.0.weight\", \"features.4.0.block.3.1.weight\", \"features.4.0.block.3.1.bias\", \"features.4.0.block.3.1.running_mean\", \"features.4.0.block.3.1.running_var\", \"features.4.1.block.0.0.weight\", \"features.4.1.block.0.1.weight\", \"features.4.1.block.0.1.bias\", \"features.4.1.block.0.1.running_mean\", \"features.4.1.block.0.1.running_var\", \"features.4.1.block.1.0.weight\", \"features.4.1.block.1.1.weight\", \"features.4.1.block.1.1.bias\", \"features.4.1.block.1.1.running_mean\", \"features.4.1.block.1.1.running_var\", \"features.4.1.block.2.fc1.weight\", \"features.4.1.block.2.fc1.bias\", \"features.4.1.block.2.fc2.weight\", \"features.4.1.block.2.fc2.bias\", \"features.4.1.block.3.0.weight\", \"features.4.1.block.3.1.weight\", \"features.4.1.block.3.1.bias\", \"features.4.1.block.3.1.running_mean\", \"features.4.1.block.3.1.running_var\", \"features.4.2.block.0.0.weight\", \"features.4.2.block.0.1.weight\", \"features.4.2.block.0.1.bias\", \"features.4.2.block.0.1.running_mean\", \"features.4.2.block.0.1.running_var\", \"features.4.2.block.1.0.weight\", \"features.4.2.block.1.1.weight\", \"features.4.2.block.1.1.bias\", \"features.4.2.block.1.1.running_mean\", \"features.4.2.block.1.1.running_var\", \"features.4.2.block.2.fc1.weight\", \"features.4.2.block.2.fc1.bias\", \"features.4.2.block.2.fc2.weight\", \"features.4.2.block.2.fc2.bias\", \"features.4.2.block.3.0.weight\", \"features.4.2.block.3.1.weight\", \"features.4.2.block.3.1.bias\", \"features.4.2.block.3.1.running_mean\", \"features.4.2.block.3.1.running_var\", \"features.5.0.block.0.0.weight\", \"features.5.0.block.0.1.weight\", \"features.5.0.block.0.1.bias\", \"features.5.0.block.0.1.running_mean\", \"features.5.0.block.0.1.running_var\", \"features.5.0.block.1.0.weight\", \"features.5.0.block.1.1.weight\", \"features.5.0.block.1.1.bias\", \"features.5.0.block.1.1.running_mean\", \"features.5.0.block.1.1.running_var\", \"features.5.0.block.2.fc1.weight\", \"features.5.0.block.2.fc1.bias\", \"features.5.0.block.2.fc2.weight\", \"features.5.0.block.2.fc2.bias\", \"features.5.0.block.3.0.weight\", \"features.5.0.block.3.1.weight\", \"features.5.0.block.3.1.bias\", \"features.5.0.block.3.1.running_mean\", \"features.5.0.block.3.1.running_var\", \"features.5.1.block.0.0.weight\", \"features.5.1.block.0.1.weight\", \"features.5.1.block.0.1.bias\", \"features.5.1.block.0.1.running_mean\", \"features.5.1.block.0.1.running_var\", \"features.5.1.block.1.0.weight\", \"features.5.1.block.1.1.weight\", \"features.5.1.block.1.1.bias\", \"features.5.1.block.1.1.running_mean\", \"features.5.1.block.1.1.running_var\", \"features.5.1.block.2.fc1.weight\", \"features.5.1.block.2.fc1.bias\", \"features.5.1.block.2.fc2.weight\", \"features.5.1.block.2.fc2.bias\", \"features.5.1.block.3.0.weight\", \"features.5.1.block.3.1.weight\", \"features.5.1.block.3.1.bias\", \"features.5.1.block.3.1.running_mean\", \"features.5.1.block.3.1.running_var\", \"features.5.2.block.0.0.weight\", \"features.5.2.block.0.1.weight\", \"features.5.2.block.0.1.bias\", \"features.5.2.block.0.1.running_mean\", \"features.5.2.block.0.1.running_var\", \"features.5.2.block.1.0.weight\", \"features.5.2.block.1.1.weight\", \"features.5.2.block.1.1.bias\", \"features.5.2.block.1.1.running_mean\", \"features.5.2.block.1.1.running_var\", \"features.5.2.block.2.fc1.weight\", \"features.5.2.block.2.fc1.bias\", \"features.5.2.block.2.fc2.weight\", \"features.5.2.block.2.fc2.bias\", \"features.5.2.block.3.0.weight\", \"features.5.2.block.3.1.weight\", \"features.5.2.block.3.1.bias\", \"features.5.2.block.3.1.running_mean\", \"features.5.2.block.3.1.running_var\", \"features.6.0.block.0.0.weight\", \"features.6.0.block.0.1.weight\", \"features.6.0.block.0.1.bias\", \"features.6.0.block.0.1.running_mean\", \"features.6.0.block.0.1.running_var\", \"features.6.0.block.1.0.weight\", \"features.6.0.block.1.1.weight\", \"features.6.0.block.1.1.bias\", \"features.6.0.block.1.1.running_mean\", \"features.6.0.block.1.1.running_var\", \"features.6.0.block.2.fc1.weight\", \"features.6.0.block.2.fc1.bias\", \"features.6.0.block.2.fc2.weight\", \"features.6.0.block.2.fc2.bias\", \"features.6.0.block.3.0.weight\", \"features.6.0.block.3.1.weight\", \"features.6.0.block.3.1.bias\", \"features.6.0.block.3.1.running_mean\", \"features.6.0.block.3.1.running_var\", \"features.6.1.block.0.0.weight\", \"features.6.1.block.0.1.weight\", \"features.6.1.block.0.1.bias\", \"features.6.1.block.0.1.running_mean\", \"features.6.1.block.0.1.running_var\", \"features.6.1.block.1.0.weight\", \"features.6.1.block.1.1.weight\", \"features.6.1.block.1.1.bias\", \"features.6.1.block.1.1.running_mean\", \"features.6.1.block.1.1.running_var\", \"features.6.1.block.2.fc1.weight\", \"features.6.1.block.2.fc1.bias\", \"features.6.1.block.2.fc2.weight\", \"features.6.1.block.2.fc2.bias\", \"features.6.1.block.3.0.weight\", \"features.6.1.block.3.1.weight\", \"features.6.1.block.3.1.bias\", \"features.6.1.block.3.1.running_mean\", \"features.6.1.block.3.1.running_var\", \"features.6.2.block.0.0.weight\", \"features.6.2.block.0.1.weight\", \"features.6.2.block.0.1.bias\", \"features.6.2.block.0.1.running_mean\", \"features.6.2.block.0.1.running_var\", \"features.6.2.block.1.0.weight\", \"features.6.2.block.1.1.weight\", \"features.6.2.block.1.1.bias\", \"features.6.2.block.1.1.running_mean\", \"features.6.2.block.1.1.running_var\", \"features.6.2.block.2.fc1.weight\", \"features.6.2.block.2.fc1.bias\", \"features.6.2.block.2.fc2.weight\", \"features.6.2.block.2.fc2.bias\", \"features.6.2.block.3.0.weight\", \"features.6.2.block.3.1.weight\", \"features.6.2.block.3.1.bias\", \"features.6.2.block.3.1.running_mean\", \"features.6.2.block.3.1.running_var\", \"features.6.3.block.0.0.weight\", \"features.6.3.block.0.1.weight\", \"features.6.3.block.0.1.bias\", \"features.6.3.block.0.1.running_mean\", \"features.6.3.block.0.1.running_var\", \"features.6.3.block.1.0.weight\", \"features.6.3.block.1.1.weight\", \"features.6.3.block.1.1.bias\", \"features.6.3.block.1.1.running_mean\", \"features.6.3.block.1.1.running_var\", \"features.6.3.block.2.fc1.weight\", \"features.6.3.block.2.fc1.bias\", \"features.6.3.block.2.fc2.weight\", \"features.6.3.block.2.fc2.bias\", \"features.6.3.block.3.0.weight\", \"features.6.3.block.3.1.weight\", \"features.6.3.block.3.1.bias\", \"features.6.3.block.3.1.running_mean\", \"features.6.3.block.3.1.running_var\", \"features.7.0.block.0.0.weight\", \"features.7.0.block.0.1.weight\", \"features.7.0.block.0.1.bias\", \"features.7.0.block.0.1.running_mean\", \"features.7.0.block.0.1.running_var\", \"features.7.0.block.1.0.weight\", \"features.7.0.block.1.1.weight\", \"features.7.0.block.1.1.bias\", \"features.7.0.block.1.1.running_mean\", \"features.7.0.block.1.1.running_var\", \"features.7.0.block.2.fc1.weight\", \"features.7.0.block.2.fc1.bias\", \"features.7.0.block.2.fc2.weight\", \"features.7.0.block.2.fc2.bias\", \"features.7.0.block.3.0.weight\", \"features.7.0.block.3.1.weight\", \"features.7.0.block.3.1.bias\", \"features.7.0.block.3.1.running_mean\", \"features.7.0.block.3.1.running_var\", \"features.8.0.weight\", \"features.8.1.weight\", \"features.8.1.bias\", \"features.8.1.running_mean\", \"features.8.1.running_var\", \"classifier.1.weight\", \"classifier.1.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"features.9.block.0.0.weight\", \"features.9.block.0.1.weight\", \"features.9.block.0.1.bias\", \"features.9.block.0.1.running_mean\", \"features.9.block.0.1.running_var\", \"features.9.block.0.1.num_batches_tracked\", \"features.9.block.1.0.weight\", \"features.9.block.1.1.weight\", \"features.9.block.1.1.bias\", \"features.9.block.1.1.running_mean\", \"features.9.block.1.1.running_var\", \"features.9.block.1.1.num_batches_tracked\", \"features.9.block.2.0.weight\", \"features.9.block.2.1.weight\", \"features.9.block.2.1.bias\", \"features.9.block.2.1.running_mean\", \"features.9.block.2.1.running_var\", \"features.9.block.2.1.num_batches_tracked\", \"features.10.block.0.0.weight\", \"features.10.block.0.1.weight\", \"features.10.block.0.1.bias\", \"features.10.block.0.1.running_mean\", \"features.10.block.0.1.running_var\", \"features.10.block.0.1.num_batches_tracked\", \"features.10.block.1.0.weight\", \"features.10.block.1.1.weight\", \"features.10.block.1.1.bias\", \"features.10.block.1.1.running_mean\", \"features.10.block.1.1.running_var\", \"features.10.block.1.1.num_batches_tracked\", \"features.10.block.2.0.weight\", \"features.10.block.2.1.weight\", \"features.10.block.2.1.bias\", \"features.10.block.2.1.running_mean\", \"features.10.block.2.1.running_var\", \"features.10.block.2.1.num_batches_tracked\", \"features.11.block.0.0.weight\", \"features.11.block.0.1.weight\", \"features.11.block.0.1.bias\", \"features.11.block.0.1.running_mean\", \"features.11.block.0.1.running_var\", \"features.11.block.0.1.num_batches_tracked\", \"features.11.block.1.0.weight\", \"features.11.block.1.1.weight\", \"features.11.block.1.1.bias\", \"features.11.block.1.1.running_mean\", \"features.11.block.1.1.running_var\", \"features.11.block.1.1.num_batches_tracked\", \"features.11.block.2.fc1.weight\", \"features.11.block.2.fc1.bias\", \"features.11.block.2.fc2.weight\", \"features.11.block.2.fc2.bias\", \"features.11.block.3.0.weight\", \"features.11.block.3.1.weight\", \"features.11.block.3.1.bias\", \"features.11.block.3.1.running_mean\", \"features.11.block.3.1.running_var\", \"features.11.block.3.1.num_batches_tracked\", \"features.12.block.0.0.weight\", \"features.12.block.0.1.weight\", \"features.12.block.0.1.bias\", \"features.12.block.0.1.running_mean\", \"features.12.block.0.1.running_var\", \"features.12.block.0.1.num_batches_tracked\", \"features.12.block.1.0.weight\", \"features.12.block.1.1.weight\", \"features.12.block.1.1.bias\", \"features.12.block.1.1.running_mean\", \"features.12.block.1.1.running_var\", \"features.12.block.1.1.num_batches_tracked\", \"features.12.block.2.fc1.weight\", \"features.12.block.2.fc1.bias\", \"features.12.block.2.fc2.weight\", \"features.12.block.2.fc2.bias\", \"features.12.block.3.0.weight\", \"features.12.block.3.1.weight\", \"features.12.block.3.1.bias\", \"features.12.block.3.1.running_mean\", \"features.12.block.3.1.running_var\", \"features.12.block.3.1.num_batches_tracked\", \"features.13.block.0.0.weight\", \"features.13.block.0.1.weight\", \"features.13.block.0.1.bias\", \"features.13.block.0.1.running_mean\", \"features.13.block.0.1.running_var\", \"features.13.block.0.1.num_batches_tracked\", \"features.13.block.1.0.weight\", \"features.13.block.1.1.weight\", \"features.13.block.1.1.bias\", \"features.13.block.1.1.running_mean\", \"features.13.block.1.1.running_var\", \"features.13.block.1.1.num_batches_tracked\", \"features.13.block.2.fc1.weight\", \"features.13.block.2.fc1.bias\", \"features.13.block.2.fc2.weight\", \"features.13.block.2.fc2.bias\", \"features.13.block.3.0.weight\", \"features.13.block.3.1.weight\", \"features.13.block.3.1.bias\", \"features.13.block.3.1.running_mean\", \"features.13.block.3.1.running_var\", \"features.13.block.3.1.num_batches_tracked\", \"features.14.block.0.0.weight\", \"features.14.block.0.1.weight\", \"features.14.block.0.1.bias\", \"features.14.block.0.1.running_mean\", \"features.14.block.0.1.running_var\", \"features.14.block.0.1.num_batches_tracked\", \"features.14.block.1.0.weight\", \"features.14.block.1.1.weight\", \"features.14.block.1.1.bias\", \"features.14.block.1.1.running_mean\", \"features.14.block.1.1.running_var\", \"features.14.block.1.1.num_batches_tracked\", \"features.14.block.2.fc1.weight\", \"features.14.block.2.fc1.bias\", \"features.14.block.2.fc2.weight\", \"features.14.block.2.fc2.bias\", \"features.14.block.3.0.weight\", \"features.14.block.3.1.weight\", \"features.14.block.3.1.bias\", \"features.14.block.3.1.running_mean\", \"features.14.block.3.1.running_var\", \"features.14.block.3.1.num_batches_tracked\", \"features.15.block.0.0.weight\", \"features.15.block.0.1.weight\", \"features.15.block.0.1.bias\", \"features.15.block.0.1.running_mean\", \"features.15.block.0.1.running_var\", \"features.15.block.0.1.num_batches_tracked\", \"features.15.block.1.0.weight\", \"features.15.block.1.1.weight\", \"features.15.block.1.1.bias\", \"features.15.block.1.1.running_mean\", \"features.15.block.1.1.running_var\", \"features.15.block.1.1.num_batches_tracked\", \"features.15.block.2.fc1.weight\", \"features.15.block.2.fc1.bias\", \"features.15.block.2.fc2.weight\", \"features.15.block.2.fc2.bias\", \"features.15.block.3.0.weight\", \"features.15.block.3.1.weight\", \"features.15.block.3.1.bias\", \"features.15.block.3.1.running_mean\", \"features.15.block.3.1.running_var\", \"features.15.block.3.1.num_batches_tracked\", \"features.16.0.weight\", \"features.16.1.weight\", \"features.16.1.bias\", \"features.16.1.running_mean\", \"features.16.1.running_var\", \"features.16.1.num_batches_tracked\", \"features.1.block.0.0.weight\", \"features.1.block.0.1.weight\", \"features.1.block.0.1.bias\", \"features.1.block.0.1.running_mean\", \"features.1.block.0.1.running_var\", \"features.1.block.0.1.num_batches_tracked\", \"features.1.block.1.0.weight\", \"features.1.block.1.1.weight\", \"features.1.block.1.1.bias\", \"features.1.block.1.1.running_mean\", \"features.1.block.1.1.running_var\", \"features.1.block.1.1.num_batches_tracked\", \"features.2.block.0.0.weight\", \"features.2.block.0.1.weight\", \"features.2.block.0.1.bias\", \"features.2.block.0.1.running_mean\", \"features.2.block.0.1.running_var\", \"features.2.block.0.1.num_batches_tracked\", \"features.2.block.1.0.weight\", \"features.2.block.1.1.weight\", \"features.2.block.1.1.bias\", \"features.2.block.1.1.running_mean\", \"features.2.block.1.1.running_var\", \"features.2.block.1.1.num_batches_tracked\", \"features.2.block.2.0.weight\", \"features.2.block.2.1.weight\", \"features.2.block.2.1.bias\", \"features.2.block.2.1.running_mean\", \"features.2.block.2.1.running_var\", \"features.2.block.2.1.num_batches_tracked\", \"features.3.block.0.0.weight\", \"features.3.block.0.1.weight\", \"features.3.block.0.1.bias\", \"features.3.block.0.1.running_mean\", \"features.3.block.0.1.running_var\", \"features.3.block.0.1.num_batches_tracked\", \"features.3.block.1.0.weight\", \"features.3.block.1.1.weight\", \"features.3.block.1.1.bias\", \"features.3.block.1.1.running_mean\", \"features.3.block.1.1.running_var\", \"features.3.block.1.1.num_batches_tracked\", \"features.3.block.2.0.weight\", \"features.3.block.2.1.weight\", \"features.3.block.2.1.bias\", \"features.3.block.2.1.running_mean\", \"features.3.block.2.1.running_var\", \"features.3.block.2.1.num_batches_tracked\", \"features.4.block.0.0.weight\", \"features.4.block.0.1.weight\", \"features.4.block.0.1.bias\", \"features.4.block.0.1.running_mean\", \"features.4.block.0.1.running_var\", \"features.4.block.0.1.num_batches_tracked\", \"features.4.block.1.0.weight\", \"features.4.block.1.1.weight\", \"features.4.block.1.1.bias\", \"features.4.block.1.1.running_mean\", \"features.4.block.1.1.running_var\", \"features.4.block.1.1.num_batches_tracked\", \"features.4.block.2.fc1.weight\", \"features.4.block.2.fc1.bias\", \"features.4.block.2.fc2.weight\", \"features.4.block.2.fc2.bias\", \"features.4.block.3.0.weight\", \"features.4.block.3.1.weight\", \"features.4.block.3.1.bias\", \"features.4.block.3.1.running_mean\", \"features.4.block.3.1.running_var\", \"features.4.block.3.1.num_batches_tracked\", \"features.5.block.0.0.weight\", \"features.5.block.0.1.weight\", \"features.5.block.0.1.bias\", \"features.5.block.0.1.running_mean\", \"features.5.block.0.1.running_var\", \"features.5.block.0.1.num_batches_tracked\", \"features.5.block.1.0.weight\", \"features.5.block.1.1.weight\", \"features.5.block.1.1.bias\", \"features.5.block.1.1.running_mean\", \"features.5.block.1.1.running_var\", \"features.5.block.1.1.num_batches_tracked\", \"features.5.block.2.fc1.weight\", \"features.5.block.2.fc1.bias\", \"features.5.block.2.fc2.weight\", \"features.5.block.2.fc2.bias\", \"features.5.block.3.0.weight\", \"features.5.block.3.1.weight\", \"features.5.block.3.1.bias\", \"features.5.block.3.1.running_mean\", \"features.5.block.3.1.running_var\", \"features.5.block.3.1.num_batches_tracked\", \"features.6.block.0.0.weight\", \"features.6.block.0.1.weight\", \"features.6.block.0.1.bias\", \"features.6.block.0.1.running_mean\", \"features.6.block.0.1.running_var\", \"features.6.block.0.1.num_batches_tracked\", \"features.6.block.1.0.weight\", \"features.6.block.1.1.weight\", \"features.6.block.1.1.bias\", \"features.6.block.1.1.running_mean\", \"features.6.block.1.1.running_var\", \"features.6.block.1.1.num_batches_tracked\", \"features.6.block.2.fc1.weight\", \"features.6.block.2.fc1.bias\", \"features.6.block.2.fc2.weight\", \"features.6.block.2.fc2.bias\", \"features.6.block.3.0.weight\", \"features.6.block.3.1.weight\", \"features.6.block.3.1.bias\", \"features.6.block.3.1.running_mean\", \"features.6.block.3.1.running_var\", \"features.6.block.3.1.num_batches_tracked\", \"features.7.block.0.0.weight\", \"features.7.block.0.1.weight\", \"features.7.block.0.1.bias\", \"features.7.block.0.1.running_mean\", \"features.7.block.0.1.running_var\", \"features.7.block.0.1.num_batches_tracked\", \"features.7.block.1.0.weight\", \"features.7.block.1.1.weight\", \"features.7.block.1.1.bias\", \"features.7.block.1.1.running_mean\", \"features.7.block.1.1.running_var\", \"features.7.block.1.1.num_batches_tracked\", \"features.7.block.2.0.weight\", \"features.7.block.2.1.weight\", \"features.7.block.2.1.bias\", \"features.7.block.2.1.running_mean\", \"features.7.block.2.1.running_var\", \"features.7.block.2.1.num_batches_tracked\", \"features.8.block.0.0.weight\", \"features.8.block.0.1.weight\", \"features.8.block.0.1.bias\", \"features.8.block.0.1.running_mean\", \"features.8.block.0.1.running_var\", \"features.8.block.0.1.num_batches_tracked\", \"features.8.block.1.0.weight\", \"features.8.block.1.1.weight\", \"features.8.block.1.1.bias\", \"features.8.block.1.1.running_mean\", \"features.8.block.1.1.running_var\", \"features.8.block.1.1.num_batches_tracked\", \"features.8.block.2.0.weight\", \"features.8.block.2.1.weight\", \"features.8.block.2.1.bias\", \"features.8.block.2.1.running_mean\", \"features.8.block.2.1.running_var\", \"features.8.block.2.1.num_batches_tracked\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.0.weight\", \"classifier.0.bias\". \n",
      "\tsize mismatch for features.0.0.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 3, 3, 3]).\n",
      "\tsize mismatch for features.0.1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for features.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for features.0.1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for features.0.1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "2025-05-06 13:54:15,565 [ERROR] - Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_18668\\1186199743.py\", line 439, in load_model\n",
      "    model.load_state_dict(checkpoint)\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 2584, in load_state_dict\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Error(s) in loading state_dict for EfficientNet:\n",
      "\tMissing key(s) in state_dict: \"features.1.0.block.0.0.weight\", \"features.1.0.block.0.1.weight\", \"features.1.0.block.0.1.bias\", \"features.1.0.block.0.1.running_mean\", \"features.1.0.block.0.1.running_var\", \"features.1.0.block.1.fc1.weight\", \"features.1.0.block.1.fc1.bias\", \"features.1.0.block.1.fc2.weight\", \"features.1.0.block.1.fc2.bias\", \"features.1.0.block.2.0.weight\", \"features.1.0.block.2.1.weight\", \"features.1.0.block.2.1.bias\", \"features.1.0.block.2.1.running_mean\", \"features.1.0.block.2.1.running_var\", \"features.2.0.block.0.0.weight\", \"features.2.0.block.0.1.weight\", \"features.2.0.block.0.1.bias\", \"features.2.0.block.0.1.running_mean\", \"features.2.0.block.0.1.running_var\", \"features.2.0.block.1.0.weight\", \"features.2.0.block.1.1.weight\", \"features.2.0.block.1.1.bias\", \"features.2.0.block.1.1.running_mean\", \"features.2.0.block.1.1.running_var\", \"features.2.0.block.2.fc1.weight\", \"features.2.0.block.2.fc1.bias\", \"features.2.0.block.2.fc2.weight\", \"features.2.0.block.2.fc2.bias\", \"features.2.0.block.3.0.weight\", \"features.2.0.block.3.1.weight\", \"features.2.0.block.3.1.bias\", \"features.2.0.block.3.1.running_mean\", \"features.2.0.block.3.1.running_var\", \"features.2.1.block.0.0.weight\", \"features.2.1.block.0.1.weight\", \"features.2.1.block.0.1.bias\", \"features.2.1.block.0.1.running_mean\", \"features.2.1.block.0.1.running_var\", \"features.2.1.block.1.0.weight\", \"features.2.1.block.1.1.weight\", \"features.2.1.block.1.1.bias\", \"features.2.1.block.1.1.running_mean\", \"features.2.1.block.1.1.running_var\", \"features.2.1.block.2.fc1.weight\", \"features.2.1.block.2.fc1.bias\", \"features.2.1.block.2.fc2.weight\", \"features.2.1.block.2.fc2.bias\", \"features.2.1.block.3.0.weight\", \"features.2.1.block.3.1.weight\", \"features.2.1.block.3.1.bias\", \"features.2.1.block.3.1.running_mean\", \"features.2.1.block.3.1.running_var\", \"features.3.0.block.0.0.weight\", \"features.3.0.block.0.1.weight\", \"features.3.0.block.0.1.bias\", \"features.3.0.block.0.1.running_mean\", \"features.3.0.block.0.1.running_var\", \"features.3.0.block.1.0.weight\", \"features.3.0.block.1.1.weight\", \"features.3.0.block.1.1.bias\", \"features.3.0.block.1.1.running_mean\", \"features.3.0.block.1.1.running_var\", \"features.3.0.block.2.fc1.weight\", \"features.3.0.block.2.fc1.bias\", \"features.3.0.block.2.fc2.weight\", \"features.3.0.block.2.fc2.bias\", \"features.3.0.block.3.0.weight\", \"features.3.0.block.3.1.weight\", \"features.3.0.block.3.1.bias\", \"features.3.0.block.3.1.running_mean\", \"features.3.0.block.3.1.running_var\", \"features.3.1.block.0.0.weight\", \"features.3.1.block.0.1.weight\", \"features.3.1.block.0.1.bias\", \"features.3.1.block.0.1.running_mean\", \"features.3.1.block.0.1.running_var\", \"features.3.1.block.1.0.weight\", \"features.3.1.block.1.1.weight\", \"features.3.1.block.1.1.bias\", \"features.3.1.block.1.1.running_mean\", \"features.3.1.block.1.1.running_var\", \"features.3.1.block.2.fc1.weight\", \"features.3.1.block.2.fc1.bias\", \"features.3.1.block.2.fc2.weight\", \"features.3.1.block.2.fc2.bias\", \"features.3.1.block.3.0.weight\", \"features.3.1.block.3.1.weight\", \"features.3.1.block.3.1.bias\", \"features.3.1.block.3.1.running_mean\", \"features.3.1.block.3.1.running_var\", \"features.4.0.block.0.0.weight\", \"features.4.0.block.0.1.weight\", \"features.4.0.block.0.1.bias\", \"features.4.0.block.0.1.running_mean\", \"features.4.0.block.0.1.running_var\", \"features.4.0.block.1.0.weight\", \"features.4.0.block.1.1.weight\", \"features.4.0.block.1.1.bias\", \"features.4.0.block.1.1.running_mean\", \"features.4.0.block.1.1.running_var\", \"features.4.0.block.2.fc1.weight\", \"features.4.0.block.2.fc1.bias\", \"features.4.0.block.2.fc2.weight\", \"features.4.0.block.2.fc2.bias\", \"features.4.0.block.3.0.weight\", \"features.4.0.block.3.1.weight\", \"features.4.0.block.3.1.bias\", \"features.4.0.block.3.1.running_mean\", \"features.4.0.block.3.1.running_var\", \"features.4.1.block.0.0.weight\", \"features.4.1.block.0.1.weight\", \"features.4.1.block.0.1.bias\", \"features.4.1.block.0.1.running_mean\", \"features.4.1.block.0.1.running_var\", \"features.4.1.block.1.0.weight\", \"features.4.1.block.1.1.weight\", \"features.4.1.block.1.1.bias\", \"features.4.1.block.1.1.running_mean\", \"features.4.1.block.1.1.running_var\", \"features.4.1.block.2.fc1.weight\", \"features.4.1.block.2.fc1.bias\", \"features.4.1.block.2.fc2.weight\", \"features.4.1.block.2.fc2.bias\", \"features.4.1.block.3.0.weight\", \"features.4.1.block.3.1.weight\", \"features.4.1.block.3.1.bias\", \"features.4.1.block.3.1.running_mean\", \"features.4.1.block.3.1.running_var\", \"features.4.2.block.0.0.weight\", \"features.4.2.block.0.1.weight\", \"features.4.2.block.0.1.bias\", \"features.4.2.block.0.1.running_mean\", \"features.4.2.block.0.1.running_var\", \"features.4.2.block.1.0.weight\", \"features.4.2.block.1.1.weight\", \"features.4.2.block.1.1.bias\", \"features.4.2.block.1.1.running_mean\", \"features.4.2.block.1.1.running_var\", \"features.4.2.block.2.fc1.weight\", \"features.4.2.block.2.fc1.bias\", \"features.4.2.block.2.fc2.weight\", \"features.4.2.block.2.fc2.bias\", \"features.4.2.block.3.0.weight\", \"features.4.2.block.3.1.weight\", \"features.4.2.block.3.1.bias\", \"features.4.2.block.3.1.running_mean\", \"features.4.2.block.3.1.running_var\", \"features.5.0.block.0.0.weight\", \"features.5.0.block.0.1.weight\", \"features.5.0.block.0.1.bias\", \"features.5.0.block.0.1.running_mean\", \"features.5.0.block.0.1.running_var\", \"features.5.0.block.1.0.weight\", \"features.5.0.block.1.1.weight\", \"features.5.0.block.1.1.bias\", \"features.5.0.block.1.1.running_mean\", \"features.5.0.block.1.1.running_var\", \"features.5.0.block.2.fc1.weight\", \"features.5.0.block.2.fc1.bias\", \"features.5.0.block.2.fc2.weight\", \"features.5.0.block.2.fc2.bias\", \"features.5.0.block.3.0.weight\", \"features.5.0.block.3.1.weight\", \"features.5.0.block.3.1.bias\", \"features.5.0.block.3.1.running_mean\", \"features.5.0.block.3.1.running_var\", \"features.5.1.block.0.0.weight\", \"features.5.1.block.0.1.weight\", \"features.5.1.block.0.1.bias\", \"features.5.1.block.0.1.running_mean\", \"features.5.1.block.0.1.running_var\", \"features.5.1.block.1.0.weight\", \"features.5.1.block.1.1.weight\", \"features.5.1.block.1.1.bias\", \"features.5.1.block.1.1.running_mean\", \"features.5.1.block.1.1.running_var\", \"features.5.1.block.2.fc1.weight\", \"features.5.1.block.2.fc1.bias\", \"features.5.1.block.2.fc2.weight\", \"features.5.1.block.2.fc2.bias\", \"features.5.1.block.3.0.weight\", \"features.5.1.block.3.1.weight\", \"features.5.1.block.3.1.bias\", \"features.5.1.block.3.1.running_mean\", \"features.5.1.block.3.1.running_var\", \"features.5.2.block.0.0.weight\", \"features.5.2.block.0.1.weight\", \"features.5.2.block.0.1.bias\", \"features.5.2.block.0.1.running_mean\", \"features.5.2.block.0.1.running_var\", \"features.5.2.block.1.0.weight\", \"features.5.2.block.1.1.weight\", \"features.5.2.block.1.1.bias\", \"features.5.2.block.1.1.running_mean\", \"features.5.2.block.1.1.running_var\", \"features.5.2.block.2.fc1.weight\", \"features.5.2.block.2.fc1.bias\", \"features.5.2.block.2.fc2.weight\", \"features.5.2.block.2.fc2.bias\", \"features.5.2.block.3.0.weight\", \"features.5.2.block.3.1.weight\", \"features.5.2.block.3.1.bias\", \"features.5.2.block.3.1.running_mean\", \"features.5.2.block.3.1.running_var\", \"features.6.0.block.0.0.weight\", \"features.6.0.block.0.1.weight\", \"features.6.0.block.0.1.bias\", \"features.6.0.block.0.1.running_mean\", \"features.6.0.block.0.1.running_var\", \"features.6.0.block.1.0.weight\", \"features.6.0.block.1.1.weight\", \"features.6.0.block.1.1.bias\", \"features.6.0.block.1.1.running_mean\", \"features.6.0.block.1.1.running_var\", \"features.6.0.block.2.fc1.weight\", \"features.6.0.block.2.fc1.bias\", \"features.6.0.block.2.fc2.weight\", \"features.6.0.block.2.fc2.bias\", \"features.6.0.block.3.0.weight\", \"features.6.0.block.3.1.weight\", \"features.6.0.block.3.1.bias\", \"features.6.0.block.3.1.running_mean\", \"features.6.0.block.3.1.running_var\", \"features.6.1.block.0.0.weight\", \"features.6.1.block.0.1.weight\", \"features.6.1.block.0.1.bias\", \"features.6.1.block.0.1.running_mean\", \"features.6.1.block.0.1.running_var\", \"features.6.1.block.1.0.weight\", \"features.6.1.block.1.1.weight\", \"features.6.1.block.1.1.bias\", \"features.6.1.block.1.1.running_mean\", \"features.6.1.block.1.1.running_var\", \"features.6.1.block.2.fc1.weight\", \"features.6.1.block.2.fc1.bias\", \"features.6.1.block.2.fc2.weight\", \"features.6.1.block.2.fc2.bias\", \"features.6.1.block.3.0.weight\", \"features.6.1.block.3.1.weight\", \"features.6.1.block.3.1.bias\", \"features.6.1.block.3.1.running_mean\", \"features.6.1.block.3.1.running_var\", \"features.6.2.block.0.0.weight\", \"features.6.2.block.0.1.weight\", \"features.6.2.block.0.1.bias\", \"features.6.2.block.0.1.running_mean\", \"features.6.2.block.0.1.running_var\", \"features.6.2.block.1.0.weight\", \"features.6.2.block.1.1.weight\", \"features.6.2.block.1.1.bias\", \"features.6.2.block.1.1.running_mean\", \"features.6.2.block.1.1.running_var\", \"features.6.2.block.2.fc1.weight\", \"features.6.2.block.2.fc1.bias\", \"features.6.2.block.2.fc2.weight\", \"features.6.2.block.2.fc2.bias\", \"features.6.2.block.3.0.weight\", \"features.6.2.block.3.1.weight\", \"features.6.2.block.3.1.bias\", \"features.6.2.block.3.1.running_mean\", \"features.6.2.block.3.1.running_var\", \"features.6.3.block.0.0.weight\", \"features.6.3.block.0.1.weight\", \"features.6.3.block.0.1.bias\", \"features.6.3.block.0.1.running_mean\", \"features.6.3.block.0.1.running_var\", \"features.6.3.block.1.0.weight\", \"features.6.3.block.1.1.weight\", \"features.6.3.block.1.1.bias\", \"features.6.3.block.1.1.running_mean\", \"features.6.3.block.1.1.running_var\", \"features.6.3.block.2.fc1.weight\", \"features.6.3.block.2.fc1.bias\", \"features.6.3.block.2.fc2.weight\", \"features.6.3.block.2.fc2.bias\", \"features.6.3.block.3.0.weight\", \"features.6.3.block.3.1.weight\", \"features.6.3.block.3.1.bias\", \"features.6.3.block.3.1.running_mean\", \"features.6.3.block.3.1.running_var\", \"features.7.0.block.0.0.weight\", \"features.7.0.block.0.1.weight\", \"features.7.0.block.0.1.bias\", \"features.7.0.block.0.1.running_mean\", \"features.7.0.block.0.1.running_var\", \"features.7.0.block.1.0.weight\", \"features.7.0.block.1.1.weight\", \"features.7.0.block.1.1.bias\", \"features.7.0.block.1.1.running_mean\", \"features.7.0.block.1.1.running_var\", \"features.7.0.block.2.fc1.weight\", \"features.7.0.block.2.fc1.bias\", \"features.7.0.block.2.fc2.weight\", \"features.7.0.block.2.fc2.bias\", \"features.7.0.block.3.0.weight\", \"features.7.0.block.3.1.weight\", \"features.7.0.block.3.1.bias\", \"features.7.0.block.3.1.running_mean\", \"features.7.0.block.3.1.running_var\", \"features.8.0.weight\", \"features.8.1.weight\", \"features.8.1.bias\", \"features.8.1.running_mean\", \"features.8.1.running_var\", \"classifier.1.weight\", \"classifier.1.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"features.9.block.0.0.weight\", \"features.9.block.0.1.weight\", \"features.9.block.0.1.bias\", \"features.9.block.0.1.running_mean\", \"features.9.block.0.1.running_var\", \"features.9.block.0.1.num_batches_tracked\", \"features.9.block.1.0.weight\", \"features.9.block.1.1.weight\", \"features.9.block.1.1.bias\", \"features.9.block.1.1.running_mean\", \"features.9.block.1.1.running_var\", \"features.9.block.1.1.num_batches_tracked\", \"features.9.block.2.0.weight\", \"features.9.block.2.1.weight\", \"features.9.block.2.1.bias\", \"features.9.block.2.1.running_mean\", \"features.9.block.2.1.running_var\", \"features.9.block.2.1.num_batches_tracked\", \"features.10.block.0.0.weight\", \"features.10.block.0.1.weight\", \"features.10.block.0.1.bias\", \"features.10.block.0.1.running_mean\", \"features.10.block.0.1.running_var\", \"features.10.block.0.1.num_batches_tracked\", \"features.10.block.1.0.weight\", \"features.10.block.1.1.weight\", \"features.10.block.1.1.bias\", \"features.10.block.1.1.running_mean\", \"features.10.block.1.1.running_var\", \"features.10.block.1.1.num_batches_tracked\", \"features.10.block.2.0.weight\", \"features.10.block.2.1.weight\", \"features.10.block.2.1.bias\", \"features.10.block.2.1.running_mean\", \"features.10.block.2.1.running_var\", \"features.10.block.2.1.num_batches_tracked\", \"features.11.block.0.0.weight\", \"features.11.block.0.1.weight\", \"features.11.block.0.1.bias\", \"features.11.block.0.1.running_mean\", \"features.11.block.0.1.running_var\", \"features.11.block.0.1.num_batches_tracked\", \"features.11.block.1.0.weight\", \"features.11.block.1.1.weight\", \"features.11.block.1.1.bias\", \"features.11.block.1.1.running_mean\", \"features.11.block.1.1.running_var\", \"features.11.block.1.1.num_batches_tracked\", \"features.11.block.2.fc1.weight\", \"features.11.block.2.fc1.bias\", \"features.11.block.2.fc2.weight\", \"features.11.block.2.fc2.bias\", \"features.11.block.3.0.weight\", \"features.11.block.3.1.weight\", \"features.11.block.3.1.bias\", \"features.11.block.3.1.running_mean\", \"features.11.block.3.1.running_var\", \"features.11.block.3.1.num_batches_tracked\", \"features.12.block.0.0.weight\", \"features.12.block.0.1.weight\", \"features.12.block.0.1.bias\", \"features.12.block.0.1.running_mean\", \"features.12.block.0.1.running_var\", \"features.12.block.0.1.num_batches_tracked\", \"features.12.block.1.0.weight\", \"features.12.block.1.1.weight\", \"features.12.block.1.1.bias\", \"features.12.block.1.1.running_mean\", \"features.12.block.1.1.running_var\", \"features.12.block.1.1.num_batches_tracked\", \"features.12.block.2.fc1.weight\", \"features.12.block.2.fc1.bias\", \"features.12.block.2.fc2.weight\", \"features.12.block.2.fc2.bias\", \"features.12.block.3.0.weight\", \"features.12.block.3.1.weight\", \"features.12.block.3.1.bias\", \"features.12.block.3.1.running_mean\", \"features.12.block.3.1.running_var\", \"features.12.block.3.1.num_batches_tracked\", \"features.13.block.0.0.weight\", \"features.13.block.0.1.weight\", \"features.13.block.0.1.bias\", \"features.13.block.0.1.running_mean\", \"features.13.block.0.1.running_var\", \"features.13.block.0.1.num_batches_tracked\", \"features.13.block.1.0.weight\", \"features.13.block.1.1.weight\", \"features.13.block.1.1.bias\", \"features.13.block.1.1.running_mean\", \"features.13.block.1.1.running_var\", \"features.13.block.1.1.num_batches_tracked\", \"features.13.block.2.fc1.weight\", \"features.13.block.2.fc1.bias\", \"features.13.block.2.fc2.weight\", \"features.13.block.2.fc2.bias\", \"features.13.block.3.0.weight\", \"features.13.block.3.1.weight\", \"features.13.block.3.1.bias\", \"features.13.block.3.1.running_mean\", \"features.13.block.3.1.running_var\", \"features.13.block.3.1.num_batches_tracked\", \"features.14.block.0.0.weight\", \"features.14.block.0.1.weight\", \"features.14.block.0.1.bias\", \"features.14.block.0.1.running_mean\", \"features.14.block.0.1.running_var\", \"features.14.block.0.1.num_batches_tracked\", \"features.14.block.1.0.weight\", \"features.14.block.1.1.weight\", \"features.14.block.1.1.bias\", \"features.14.block.1.1.running_mean\", \"features.14.block.1.1.running_var\", \"features.14.block.1.1.num_batches_tracked\", \"features.14.block.2.fc1.weight\", \"features.14.block.2.fc1.bias\", \"features.14.block.2.fc2.weight\", \"features.14.block.2.fc2.bias\", \"features.14.block.3.0.weight\", \"features.14.block.3.1.weight\", \"features.14.block.3.1.bias\", \"features.14.block.3.1.running_mean\", \"features.14.block.3.1.running_var\", \"features.14.block.3.1.num_batches_tracked\", \"features.15.block.0.0.weight\", \"features.15.block.0.1.weight\", \"features.15.block.0.1.bias\", \"features.15.block.0.1.running_mean\", \"features.15.block.0.1.running_var\", \"features.15.block.0.1.num_batches_tracked\", \"features.15.block.1.0.weight\", \"features.15.block.1.1.weight\", \"features.15.block.1.1.bias\", \"features.15.block.1.1.running_mean\", \"features.15.block.1.1.running_var\", \"features.15.block.1.1.num_batches_tracked\", \"features.15.block.2.fc1.weight\", \"features.15.block.2.fc1.bias\", \"features.15.block.2.fc2.weight\", \"features.15.block.2.fc2.bias\", \"features.15.block.3.0.weight\", \"features.15.block.3.1.weight\", \"features.15.block.3.1.bias\", \"features.15.block.3.1.running_mean\", \"features.15.block.3.1.running_var\", \"features.15.block.3.1.num_batches_tracked\", \"features.16.0.weight\", \"features.16.1.weight\", \"features.16.1.bias\", \"features.16.1.running_mean\", \"features.16.1.running_var\", \"features.16.1.num_batches_tracked\", \"features.1.block.0.0.weight\", \"features.1.block.0.1.weight\", \"features.1.block.0.1.bias\", \"features.1.block.0.1.running_mean\", \"features.1.block.0.1.running_var\", \"features.1.block.0.1.num_batches_tracked\", \"features.1.block.1.0.weight\", \"features.1.block.1.1.weight\", \"features.1.block.1.1.bias\", \"features.1.block.1.1.running_mean\", \"features.1.block.1.1.running_var\", \"features.1.block.1.1.num_batches_tracked\", \"features.2.block.0.0.weight\", \"features.2.block.0.1.weight\", \"features.2.block.0.1.bias\", \"features.2.block.0.1.running_mean\", \"features.2.block.0.1.running_var\", \"features.2.block.0.1.num_batches_tracked\", \"features.2.block.1.0.weight\", \"features.2.block.1.1.weight\", \"features.2.block.1.1.bias\", \"features.2.block.1.1.running_mean\", \"features.2.block.1.1.running_var\", \"features.2.block.1.1.num_batches_tracked\", \"features.2.block.2.0.weight\", \"features.2.block.2.1.weight\", \"features.2.block.2.1.bias\", \"features.2.block.2.1.running_mean\", \"features.2.block.2.1.running_var\", \"features.2.block.2.1.num_batches_tracked\", \"features.3.block.0.0.weight\", \"features.3.block.0.1.weight\", \"features.3.block.0.1.bias\", \"features.3.block.0.1.running_mean\", \"features.3.block.0.1.running_var\", \"features.3.block.0.1.num_batches_tracked\", \"features.3.block.1.0.weight\", \"features.3.block.1.1.weight\", \"features.3.block.1.1.bias\", \"features.3.block.1.1.running_mean\", \"features.3.block.1.1.running_var\", \"features.3.block.1.1.num_batches_tracked\", \"features.3.block.2.0.weight\", \"features.3.block.2.1.weight\", \"features.3.block.2.1.bias\", \"features.3.block.2.1.running_mean\", \"features.3.block.2.1.running_var\", \"features.3.block.2.1.num_batches_tracked\", \"features.4.block.0.0.weight\", \"features.4.block.0.1.weight\", \"features.4.block.0.1.bias\", \"features.4.block.0.1.running_mean\", \"features.4.block.0.1.running_var\", \"features.4.block.0.1.num_batches_tracked\", \"features.4.block.1.0.weight\", \"features.4.block.1.1.weight\", \"features.4.block.1.1.bias\", \"features.4.block.1.1.running_mean\", \"features.4.block.1.1.running_var\", \"features.4.block.1.1.num_batches_tracked\", \"features.4.block.2.fc1.weight\", \"features.4.block.2.fc1.bias\", \"features.4.block.2.fc2.weight\", \"features.4.block.2.fc2.bias\", \"features.4.block.3.0.weight\", \"features.4.block.3.1.weight\", \"features.4.block.3.1.bias\", \"features.4.block.3.1.running_mean\", \"features.4.block.3.1.running_var\", \"features.4.block.3.1.num_batches_tracked\", \"features.5.block.0.0.weight\", \"features.5.block.0.1.weight\", \"features.5.block.0.1.bias\", \"features.5.block.0.1.running_mean\", \"features.5.block.0.1.running_var\", \"features.5.block.0.1.num_batches_tracked\", \"features.5.block.1.0.weight\", \"features.5.block.1.1.weight\", \"features.5.block.1.1.bias\", \"features.5.block.1.1.running_mean\", \"features.5.block.1.1.running_var\", \"features.5.block.1.1.num_batches_tracked\", \"features.5.block.2.fc1.weight\", \"features.5.block.2.fc1.bias\", \"features.5.block.2.fc2.weight\", \"features.5.block.2.fc2.bias\", \"features.5.block.3.0.weight\", \"features.5.block.3.1.weight\", \"features.5.block.3.1.bias\", \"features.5.block.3.1.running_mean\", \"features.5.block.3.1.running_var\", \"features.5.block.3.1.num_batches_tracked\", \"features.6.block.0.0.weight\", \"features.6.block.0.1.weight\", \"features.6.block.0.1.bias\", \"features.6.block.0.1.running_mean\", \"features.6.block.0.1.running_var\", \"features.6.block.0.1.num_batches_tracked\", \"features.6.block.1.0.weight\", \"features.6.block.1.1.weight\", \"features.6.block.1.1.bias\", \"features.6.block.1.1.running_mean\", \"features.6.block.1.1.running_var\", \"features.6.block.1.1.num_batches_tracked\", \"features.6.block.2.fc1.weight\", \"features.6.block.2.fc1.bias\", \"features.6.block.2.fc2.weight\", \"features.6.block.2.fc2.bias\", \"features.6.block.3.0.weight\", \"features.6.block.3.1.weight\", \"features.6.block.3.1.bias\", \"features.6.block.3.1.running_mean\", \"features.6.block.3.1.running_var\", \"features.6.block.3.1.num_batches_tracked\", \"features.7.block.0.0.weight\", \"features.7.block.0.1.weight\", \"features.7.block.0.1.bias\", \"features.7.block.0.1.running_mean\", \"features.7.block.0.1.running_var\", \"features.7.block.0.1.num_batches_tracked\", \"features.7.block.1.0.weight\", \"features.7.block.1.1.weight\", \"features.7.block.1.1.bias\", \"features.7.block.1.1.running_mean\", \"features.7.block.1.1.running_var\", \"features.7.block.1.1.num_batches_tracked\", \"features.7.block.2.0.weight\", \"features.7.block.2.1.weight\", \"features.7.block.2.1.bias\", \"features.7.block.2.1.running_mean\", \"features.7.block.2.1.running_var\", \"features.7.block.2.1.num_batches_tracked\", \"features.8.block.0.0.weight\", \"features.8.block.0.1.weight\", \"features.8.block.0.1.bias\", \"features.8.block.0.1.running_mean\", \"features.8.block.0.1.running_var\", \"features.8.block.0.1.num_batches_tracked\", \"features.8.block.1.0.weight\", \"features.8.block.1.1.weight\", \"features.8.block.1.1.bias\", \"features.8.block.1.1.running_mean\", \"features.8.block.1.1.running_var\", \"features.8.block.1.1.num_batches_tracked\", \"features.8.block.2.0.weight\", \"features.8.block.2.1.weight\", \"features.8.block.2.1.bias\", \"features.8.block.2.1.running_mean\", \"features.8.block.2.1.running_var\", \"features.8.block.2.1.num_batches_tracked\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.0.weight\", \"classifier.0.bias\". \n",
      "\tsize mismatch for features.0.0.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 3, 3, 3]).\n",
      "\tsize mismatch for features.0.1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for features.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for features.0.1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for features.0.1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\n",
      "2025-05-06 13:54:15,565 [INFO] - Loading resnet model from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\resnet_mutual_final_best.pth\n",
      "2025-05-06 13:54:15,717 [INFO] - Creating resnet model architecture...\n",
      "2025-05-06 13:54:16,068 [INFO] - Could not determine model architecture from checkpoint, using resnet architecture\n",
      "2025-05-06 13:54:16,115 [INFO] - Model resnet state loaded directly from checkpoint\n",
      "2025-05-06 13:54:16,155 [INFO] - Model resnet loaded successfully and set to evaluation mode\n",
      "2025-05-06 13:54:16,155 [INFO] - Loading densenet model from: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\densenet_mutual_final_best.pth\n",
      "2025-05-06 13:54:16,319 [INFO] - Creating densenet model architecture...\n",
      "2025-05-06 13:54:16,403 [INFO] - Could not determine model architecture from checkpoint, using densenet architecture\n",
      "2025-05-06 13:54:16,443 [INFO] - Model densenet state loaded directly from checkpoint\n",
      "2025-05-06 13:54:16,477 [INFO] - Model densenet loaded successfully and set to evaluation mode\n",
      "2025-05-06 13:54:16,477 [INFO] - Loaded 5 models for evaluation\n",
      "2025-05-06 13:54:16,477 [INFO] - Preparing test dataset for student model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 13:54:17,077 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-06 13:54:17,077 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-06 13:54:17,077 [INFO] - Running inference for student model...\n",
      "2025-05-06 13:54:17,235 [INFO] - GPU cache cleared: 476.98MB  476.98MB (freed 0.00MB)\n",
      "Evaluating student: 100%|| 1250/1250 [00:34<00:00, 36.09it/s]\n",
      "2025-05-06 13:54:51,876 [INFO] - Inference complete on 10000 samples for student\n",
      "2025-05-06 13:54:51,878 [INFO] - Analyzing student model performance...\n",
      "2025-05-06 13:54:51,878 [INFO] - [student] Test Accuracy: 94.77%\n",
      "2025-05-06 13:54:51,889 [INFO] - [student] F1 Score (macro): 94.76%\n",
      "2025-05-06 13:54:51,890 [INFO] - [student] Precision (macro): 94.79%\n",
      "2025-05-06 13:54:51,890 [INFO] - [student] Recall (macro): 94.77%\n",
      "2025-05-06 13:54:51,894 [INFO] - [student] Expected Calibration Error: 0.0138\n",
      "2025-05-06 13:54:51,894 [INFO] - [student] Maximum Calibration Error: 0.0942\n",
      "2025-05-06 13:54:51,894 [INFO] - [student] Average Calibration Error: 0.0472\n",
      "2025-05-06 13:54:51,894 [INFO] - [student] Root Mean Square Calibration Error: 0.0568\n",
      "2025-05-06 13:54:52,524 [INFO] - \n",
      "[student] Classification Report:\n",
      "2025-05-06 13:54:52,534 [INFO] -               precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.938     0.967     0.952      1000\n",
      "  automobile      0.950     0.981     0.965      1000\n",
      "        bird      0.919     0.955     0.937      1000\n",
      "         cat      0.901     0.887     0.894      1000\n",
      "        deer      0.959     0.968     0.964      1000\n",
      "         dog      0.929     0.892     0.910      1000\n",
      "        frog      0.958     0.967     0.963      1000\n",
      "       horse      0.971     0.969     0.970      1000\n",
      "        ship      0.974     0.960     0.967      1000\n",
      "       truck      0.979     0.931     0.954      1000\n",
      "\n",
      "    accuracy                          0.948     10000\n",
      "   macro avg      0.948     0.948     0.948     10000\n",
      "weighted avg      0.948     0.948     0.948     10000\n",
      "\n",
      "2025-05-06 13:54:53,428 [INFO] - Metrics saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\MutualLearning\\evaluation\\student/metrics.json\n",
      "2025-05-06 13:54:53,435 [INFO] - [student] Evaluation results saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\MutualLearning\\evaluation\\student\n",
      "2025-05-06 13:54:53,436 [INFO] - Generating prediction visualizations for student...\n",
      "2025-05-06 13:54:55,936 [INFO] - Prediction visualizations saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\MutualLearning\\evaluation\\student/prediction_examples.png\n",
      "2025-05-06 13:54:55,936 [INFO] - Generating GradCAM visualizations for student...\n",
      "Finding class samples for student:   0%|          | 25/10000 [00:00<00:09, 1056.28it/s]\n",
      "2025-05-06 13:54:55,964 [INFO] - Detected architecture for student: efficientnet\n",
      "2025-05-06 13:54:55,964 [INFO] - Using target layer for student: Conv2dNormActivation(\n",
      "  (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): SiLU(inplace=True)\n",
      ")\n",
      "2025-05-06 13:54:55,964 [INFO] - [student] Generating GradCAM for class 'airplane'\n",
      "2025-05-06 13:54:57,178 [INFO] - [student] Generating GradCAM for class 'automobile'\n",
      "2025-05-06 13:54:57,629 [INFO] - [student] Generating GradCAM for class 'bird'\n",
      "2025-05-06 13:54:58,017 [INFO] - [student] Generating GradCAM for class 'cat'\n",
      "2025-05-06 13:54:58,400 [INFO] - [student] Generating GradCAM for class 'deer'\n",
      "2025-05-06 13:54:58,825 [INFO] - [student] Generating GradCAM for class 'dog'\n",
      "2025-05-06 13:54:59,193 [INFO] - [student] Generating GradCAM for class 'frog'\n",
      "2025-05-06 13:54:59,562 [INFO] - [student] Generating GradCAM for class 'horse'\n",
      "2025-05-06 13:54:59,927 [INFO] - [student] Generating GradCAM for class 'ship'\n",
      "2025-05-06 13:55:00,293 [INFO] - [student] Generating GradCAM for class 'truck'\n",
      "2025-05-06 13:55:04,280 [INFO] - GradCAM visualizations saved to C:\\Users\\Gading\\Downloads\\Research\\Results\\MutualLearning\\evaluation\\student/gradcam_visualization.png\n",
      "2025-05-06 13:55:04,490 [INFO] - GPU cache cleared: 510.88MB  510.88MB (freed 0.00MB)\n",
      "2025-05-06 13:55:04,490 [INFO] - Preparing test dataset for vit model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 13:55:05,002 [INFO] - Test dataset loaded with 10000 samples\n",
      "2025-05-06 13:55:05,002 [INFO] - Creating DataLoader with batch size 8...\n",
      "2025-05-06 13:55:05,010 [INFO] - Running inference for vit model...\n",
      "2025-05-06 13:55:05,239 [INFO] - GPU cache cleared: 510.88MB  510.88MB (freed 0.00MB)\n",
      "Evaluating vit:  18%|        | 228/1250 [00:58<04:24,  3.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1631\u001b[0m\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1631\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 1595\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1592\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m create_data_loader(test_dataset, config)\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m-> 1595\u001b[0m targets, predictions, probabilities \u001b[38;5;241m=\u001b[39m run_inference(model, test_loader, config, device, model_name)\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m# Generate metrics\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m metrics \u001b[38;5;241m=\u001b[39m analyze_results(targets, predictions, probabilities, config\u001b[38;5;241m.\u001b[39mclasses, config, model_name)\n",
      "Cell \u001b[1;32mIn[1], line 607\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(model, loader, config, device, model_name)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# Store results (on CPU to save GPU memory)\u001b[39;00m\n\u001b[0;32m    606\u001b[0m all_targets\u001b[38;5;241m.\u001b[39mextend(targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m--> 607\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    608\u001b[0m all_probs\u001b[38;5;241m.\u001b[39mappend(probs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Free memory\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Import specific model classes for proper model loading\n",
    "from torchvision.models import (\n",
    "    vit_b_16, \n",
    "    efficientnet_b0, \n",
    "    inception_v3, \n",
    "    mobilenet_v3_large, \n",
    "    resnet50, \n",
    "    densenet121\n",
    ")\n",
    "\n",
    "# Set the style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set environment variables for better performance\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Optimize CPU threading\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # Limit memory fragmentation\n",
    "\n",
    "# Setup logging\n",
    "log_file = os.path.join(r\"C:\\Users\\Gading\\Downloads\\Research\\Results\\MutualLearning\\logs\", \"mutual_test.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "####################################\n",
    "# 1. Configuration Class\n",
    "####################################\n",
    "class MutualEvalConfig:\n",
    "    def __init__(self):\n",
    "        # Base paths\n",
    "        self.base_path = r\"C:\\Users\\Gading\\Downloads\\Research\"\n",
    "        \n",
    "        # Dataset path\n",
    "        self.dataset_path = os.path.join(self.base_path, \"Dataset\", \"CIFAR-10\")\n",
    "        \n",
    "        # Model paths - student model and all teacher models\n",
    "        self.models_base_path = os.path.join(self.base_path, \"Models\")\n",
    "        self.student_model_path = os.path.join(self.models_base_path, \"MutualLearning\", \"exports\", \"mutual_learning_20250502_213701_final_student.pth\")\n",
    "        \n",
    "        # Teacher model paths (for comparison)\n",
    "        self.teacher_model_paths = {\n",
    "            'vit': os.path.join(self.models_base_path, \"MutualLearning\", \"checkpoints\", \"vit_mutual_final_best.pth\"),\n",
    "            'efficientnet': os.path.join(self.models_base_path, \"MutualLearning\", \"checkpoints\", \"efficientnet_mutual_final_best.pth\"),\n",
    "            'inception': os.path.join(self.models_base_path, \"MutualLearning\", \"checkpoints\", \"inception_mutual_final_best.pth\"),\n",
    "            'mobilenet': os.path.join(self.models_base_path, \"MutualLearning\", \"checkpoints\", \"mobilenet_mutual_final_best.pth\"),\n",
    "            'resnet': os.path.join(self.models_base_path, \"MutualLearning\", \"checkpoints\", \"resnet_mutual_final_best.pth\"),\n",
    "            'densenet': os.path.join(self.models_base_path, \"MutualLearning\", \"checkpoints\", \"densenet_mutual_final_best.pth\")\n",
    "        }\n",
    "        \n",
    "        # Output directory for evaluation results\n",
    "        self.output_dir = os.path.join(self.base_path, \"Results\", \"MutualLearning\", \"evaluation\")\n",
    "        \n",
    "        # Hardware settings - optimized for stability\n",
    "        self.batch_size = 8  # Reduced for stability\n",
    "        self.num_workers = 0  # Start with 0 workers to avoid hanging\n",
    "        self.use_amp = True   # Use mixed precision for faster evaluation\n",
    "        self.pin_memory = True\n",
    "        \n",
    "        # Evaluation options\n",
    "        self.compare_with_teachers = True  # Compare student with teachers\n",
    "        self.evaluate_teacher_ensemble = True  # Evaluate the ensemble of teachers\n",
    "        self.n_bins_calibration = 15  # Number of bins for calibration metrics\n",
    "        \n",
    "        # CIFAR-10 classes\n",
    "        self.classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "        \n",
    "        # ImageNet normalization (used by pretrained models)\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        # Model-specific input sizes\n",
    "        self.model_input_sizes = {\n",
    "            'vit': 224,\n",
    "            'efficientnet': 224,\n",
    "            'inception': 299,  # InceptionV3 requires 299x299 input\n",
    "            'mobilenet': 224,\n",
    "            'resnet': 224,\n",
    "            'densenet': 224,\n",
    "            'student': 224\n",
    "        }\n",
    "        \n",
    "        # Plots configuration\n",
    "        self.plot_dpi = 300\n",
    "        self.plot_format = 'png'  # Use 'pdf' for publication-quality\n",
    "        self.ieee_style = True  # Use IEEE conference/journal style guidelines\n",
    "        \n",
    "        # Mutual learning specific metrics\n",
    "        self.calibration_metrics = ['ece', 'mce', 'ace', 'rmsce']  # Expected, Maximum, Average, Root Mean Square Calibration Errors\n",
    "        self.knowledge_transfer_analysis = True  # Analyze how knowledge was transferred\n",
    "        self.soft_target_temp = 4.0  # Temperature used in mutual learning (for visualization)\n",
    "        \n",
    "        # Teacher weights (equal for mutual learning - different than distillation)\n",
    "        self.teacher_weights = {\n",
    "            'vit': 1.0,\n",
    "            'efficientnet': 1.0,\n",
    "            'inception': 1.0,\n",
    "            'mobilenet': 1.0,\n",
    "            'resnet': 1.0,\n",
    "            'densenet': 1.0\n",
    "        }\n",
    "\n",
    "    def get_input_size(self, model_name):\n",
    "        \"\"\"Get model-specific input size\"\"\"\n",
    "        if model_name in self.model_input_sizes:\n",
    "            return self.model_input_sizes[model_name]\n",
    "        return 224  # Default size\n",
    "\n",
    "\n",
    "####################################\n",
    "# 2. Utilities\n",
    "####################################\n",
    "def setup_environment():\n",
    "    \"\"\"Setup environment and output directory\"\"\"\n",
    "    # Create output directory\n",
    "    config = MutualEvalConfig()\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Show GPU info if available\n",
    "    if device.type == 'cuda':\n",
    "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    return config, device\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache to free up memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        before_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()  # Explicit garbage collection\n",
    "        after_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        logger.info(f\"GPU cache cleared: {before_mem:.2f}MB  {after_mem:.2f}MB (freed {before_mem-after_mem:.2f}MB)\")\n",
    "\n",
    "\n",
    "####################################\n",
    "# 3. Dataset and DataLoader\n",
    "####################################\n",
    "def get_transform(config, model_name):\n",
    "    \"\"\"Get model-specific transforms for CIFAR-10 test dataset\"\"\"\n",
    "    input_size = config.get_input_size(model_name)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=config.mean, std=config.std),\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def get_test_dataset(config, model_name):\n",
    "    \"\"\"Create a CIFAR-10 test dataset with model-specific transformations\"\"\"\n",
    "    logger.info(f\"Preparing test dataset for {model_name} model...\")\n",
    "    \n",
    "    transform = get_transform(config, model_name)\n",
    "    \n",
    "    # Load the dataset\n",
    "    try:\n",
    "        test_dataset = datasets.CIFAR10(\n",
    "            root=config.dataset_path,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        )\n",
    "        logger.info(f\"Test dataset loaded with {len(test_dataset)} samples\")\n",
    "        return test_dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_original_images(config, indices):\n",
    "    \"\"\"Get original 32x32 images for display purposes\"\"\"\n",
    "    # Load dataset without transformations\n",
    "    orig_dataset = datasets.CIFAR10(\n",
    "        root=config.dataset_path,\n",
    "        train=False,\n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    originals = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        img, label = orig_dataset.data[idx], orig_dataset.targets[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        originals.append(img_tensor)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return originals, labels\n",
    "\n",
    "def create_data_loader(dataset, config):\n",
    "    \"\"\"Create a DataLoader with optimized settings\"\"\"\n",
    "    logger.info(f\"Creating DataLoader with batch size {config.batch_size}...\")\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory,\n",
    "        persistent_workers=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "####################################\n",
    "# 4. Model Loading\n",
    "####################################\n",
    "class InceptionV3Wrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for InceptionV3 that safely handles auxiliary outputs for small inputs.\n",
    "    This prevents the \"Kernel size can't be greater than actual input size\" error.\n",
    "    \"\"\"\n",
    "    def __init__(self, inception_model):\n",
    "        super(InceptionV3Wrapper, self).__init__()\n",
    "        self.inception = inception_model\n",
    "        # Directly access the internal model components we need\n",
    "        self.Conv2d_1a_3x3 = inception_model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = inception_model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = inception_model.Conv2d_2b_3x3\n",
    "        self.maxpool1 = inception_model.maxpool1\n",
    "        self.Conv2d_3b_1x1 = inception_model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = inception_model.Conv2d_4a_3x3\n",
    "        self.maxpool2 = inception_model.maxpool2\n",
    "        self.Mixed_5b = inception_model.Mixed_5b\n",
    "        self.Mixed_5c = inception_model.Mixed_5c\n",
    "        self.Mixed_5d = inception_model.Mixed_5d\n",
    "        self.Mixed_6a = inception_model.Mixed_6a\n",
    "        self.Mixed_6b = inception_model.Mixed_6b\n",
    "        self.Mixed_6c = inception_model.Mixed_6c\n",
    "        self.Mixed_6d = inception_model.Mixed_6d\n",
    "        self.Mixed_6e = inception_model.Mixed_6e\n",
    "        self.Mixed_7a = inception_model.Mixed_7a\n",
    "        self.Mixed_7b = inception_model.Mixed_7b\n",
    "        self.Mixed_7c = inception_model.Mixed_7c\n",
    "        self.avgpool = inception_model.avgpool\n",
    "        self.dropout = inception_model.dropout\n",
    "        self.fc = inception_model.fc\n",
    "        \n",
    "        # Important: mark that this is a wrapper\n",
    "        self.is_wrapper = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get the batch size for reshaping later\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Basic stem\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.Mixed_5b(x)\n",
    "        x = self.Mixed_5c(x)\n",
    "        x = self.Mixed_5d(x)\n",
    "        x = self.Mixed_6a(x)\n",
    "        x = self.Mixed_6b(x)\n",
    "        x = self.Mixed_6c(x)\n",
    "        x = self.Mixed_6d(x)\n",
    "        x = self.Mixed_6e(x)\n",
    "        \n",
    "        # No auxiliary classifier usage - skip those layers that cause issues\n",
    "        \n",
    "        x = self.Mixed_7a(x)\n",
    "        x = self.Mixed_7b(x)\n",
    "        x = self.Mixed_7c(x)\n",
    "        \n",
    "        # Final pooling and prediction\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def create_model_architecture(model_name, num_classes=10):\n",
    "    \"\"\"Create a model architecture based on the model name\"\"\"\n",
    "    logger.info(f\"Creating {model_name} model architecture...\")\n",
    "    \n",
    "    if model_name == 'vit':\n",
    "        model = vit_b_16(weights=None)\n",
    "        if hasattr(model, 'heads'):\n",
    "            input_dim = model.heads.head.in_features\n",
    "            model.heads.head = torch.nn.Linear(input_dim, num_classes)\n",
    "        else:\n",
    "            input_dim = model.head.in_features\n",
    "            model.head = torch.nn.Linear(input_dim, num_classes)\n",
    "            \n",
    "    elif model_name == 'efficientnet' or model_name == 'student':\n",
    "        model = efficientnet_b0(weights=None)\n",
    "        if hasattr(model, 'classifier'):\n",
    "            in_features = model.classifier[1].in_features\n",
    "            model.classifier[1] = torch.nn.Linear(in_features, num_classes)\n",
    "            \n",
    "    elif model_name == 'inception':\n",
    "        base_inception = inception_v3(weights=None)\n",
    "        base_inception.fc = torch.nn.Linear(base_inception.fc.in_features, num_classes)\n",
    "        model = InceptionV3Wrapper(base_inception)\n",
    "        \n",
    "    elif model_name == 'mobilenet':\n",
    "        model = mobilenet_v3_large(weights=None)\n",
    "        model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'resnet':\n",
    "        model = resnet50(weights=None)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'densenet':\n",
    "        model = densenet121(weights=None)\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_model(config, device, model_name):\n",
    "    \"\"\"Load a model from checkpoint\"\"\"\n",
    "    checkpoint_path = config.teacher_model_paths.get(model_name) if model_name != 'student' else config.student_model_path\n",
    "    \n",
    "    if not checkpoint_path or not os.path.exists(checkpoint_path):\n",
    "        logger.warning(f\"Checkpoint for {model_name} not found at {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Loading {model_name} model from: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(\n",
    "            checkpoint_path, \n",
    "            map_location=device\n",
    "        )\n",
    "        \n",
    "        # Initialize model variable\n",
    "        model = None\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            # First, check the keys to determine the actual model architecture\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            \n",
    "            # Detect model architecture from keys pattern\n",
    "            if any('class_token' in key for key in state_dict.keys()):\n",
    "                logger.info(f\"Detected ViT architecture in {model_name} checkpoint\")\n",
    "                model = create_model_architecture('vit')\n",
    "            elif any('features.0.1.weight' in key for key in state_dict.keys()):\n",
    "                logger.info(f\"Detected EfficientNet architecture in {model_name} checkpoint\")\n",
    "                model = create_model_architecture('efficientnet')\n",
    "            elif any('Conv2d_1a_3x3' in key for key in state_dict.keys()):\n",
    "                logger.info(f\"Detected InceptionV3 architecture in {model_name} checkpoint\")\n",
    "                model = create_model_architecture('inception')\n",
    "            elif any('inverted_bottleneck' in key for key in state_dict.keys()):\n",
    "                logger.info(f\"Detected MobileNet architecture in {model_name} checkpoint\")\n",
    "                model = create_model_architecture('mobilenet')\n",
    "            elif any('layer1.0.conv1.weight' in key for key in state_dict.keys()):\n",
    "                logger.info(f\"Detected ResNet architecture in {model_name} checkpoint\")\n",
    "                model = create_model_architecture('resnet')\n",
    "            elif any('denseblock' in key for key in state_dict.keys()):\n",
    "                logger.info(f\"Detected DenseNet architecture in {model_name} checkpoint\")\n",
    "                model = create_model_architecture('densenet')\n",
    "            else:\n",
    "                # Use default architecture based on model name if detection fails\n",
    "                logger.info(f\"Could not determine model architecture from checkpoint, using {model_name} architecture\")\n",
    "                model = create_model_architecture(model_name)\n",
    "            \n",
    "            # Now load the state dict into the appropriate model\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            logger.info(f\"Model {model_name} state loaded from 'model_state_dict'\")\n",
    "            \n",
    "            # Print additional metadata if available\n",
    "            if 'test_metrics' in checkpoint:\n",
    "                logger.info(f\"Previous test metrics found for {model_name}:\")\n",
    "                for k, v in checkpoint['test_metrics'].items():\n",
    "                    logger.info(f\"  - {k}: {v}\")\n",
    "        else:\n",
    "            # For direct state dict, check the first key to determine model type\n",
    "            first_keys = list(checkpoint.keys())[:5]  # Look at first few keys\n",
    "            \n",
    "            if any('class_token' in key for key in first_keys):\n",
    "                model = create_model_architecture('vit')\n",
    "                logger.info(f\"Detected ViT architecture in {model_name} checkpoint\")\n",
    "            elif any('features.0.1.weight' in key for key in first_keys):\n",
    "                model = create_model_architecture('efficientnet')\n",
    "                logger.info(f\"Detected EfficientNet architecture in {model_name} checkpoint\")\n",
    "            elif any('Conv2d_1a_3x3' in key for key in first_keys):\n",
    "                model = create_model_architecture('inception')\n",
    "                logger.info(f\"Detected InceptionV3 architecture in {model_name} checkpoint\")\n",
    "            elif any('inverted_bottleneck' in key for key in first_keys):\n",
    "                model = create_model_architecture('mobilenet')\n",
    "                logger.info(f\"Detected MobileNet architecture in {model_name} checkpoint\")\n",
    "            elif any('layer1.0.conv1.weight' in key for key in first_keys):\n",
    "                model = create_model_architecture('resnet')\n",
    "                logger.info(f\"Detected ResNet architecture in {model_name} checkpoint\")\n",
    "            elif any('denseblock' in key for key in first_keys):\n",
    "                model = create_model_architecture('densenet')\n",
    "                logger.info(f\"Detected DenseNet architecture in {model_name} checkpoint\")\n",
    "            else:\n",
    "                # Use default architecture based on model name if detection fails\n",
    "                model = create_model_architecture(model_name)\n",
    "                logger.info(f\"Could not determine model architecture from checkpoint, using {model_name} architecture\")\n",
    "                \n",
    "            model.load_state_dict(checkpoint)\n",
    "            logger.info(f\"Model {model_name} state loaded directly from checkpoint\")\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        logger.info(f\"Model {model_name} loaded successfully and set to evaluation mode\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load {model_name} model: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def load_student_model(config, device):\n",
    "        \n",
    "    try:\n",
    "                \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(\n",
    "            config.student_model_path, \n",
    "            map_location=device\n",
    "        )\n",
    "        \n",
    "        # Initialize model variable before conditional blocks\n",
    "        model = None\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            # First, check the keys to determine the model architecture\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            \n",
    "            # Detect model architecture from keys pattern\n",
    "            if any('class_token' in key for key in state_dict.keys()):\n",
    "                logger.info(\"Detected ViT architecture in checkpoint\")\n",
    "                model = create_model_architecture('vit')\n",
    "            elif any('features.0.1.weight' in key for key in state_dict.keys()):\n",
    "                logger.info(\"Detected EfficientNet architecture in checkpoint\")\n",
    "                model = create_model_architecture('efficientnet')\n",
    "            elif any('Conv2d_1a_3x3' in key for key in state_dict.keys()):\n",
    "                logger.info(\"Detected InceptionV3 architecture in checkpoint\")\n",
    "                model = create_model_architecture('inception')\n",
    "            elif any('inverted_bottleneck' in key for key in state_dict.keys()):\n",
    "                logger.info(\"Detected MobileNet architecture in checkpoint\")\n",
    "                model = create_model_architecture('mobilenet')\n",
    "            elif any('layer1.0.conv1.weight' in key for key in state_dict.keys()):\n",
    "                logger.info(\"Detected ResNet architecture in checkpoint\")\n",
    "                model = create_model_architecture('resnet')\n",
    "            elif any('denseblock' in key for key in state_dict.keys()):\n",
    "                logger.info(\"Detected DenseNet architecture in checkpoint\")\n",
    "                model = create_model_architecture('densenet')\n",
    "            else:\n",
    "                # Default to EfficientNet if architecture can't be determined\n",
    "                logger.info(\"Could not determine model architecture, defaulting to EfficientNet\")\n",
    "                model = create_model_architecture('efficientnet')\n",
    "            \n",
    "            # Now load the state dict into the appropriate model\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            logger.info(f\"Student model state loaded from 'model_state_dict'\")\n",
    "            \n",
    "            # Load teacher weights if available\n",
    "            if 'teacher_weights' in checkpoint:\n",
    "                config.teacher_weights = checkpoint['teacher_weights']\n",
    "                logger.info(f\"Teacher weights loaded from checkpoint: {config.teacher_weights}\")\n",
    "            \n",
    "            # Load teacher temperatures if available\n",
    "            if 'teacher_temperatures' in checkpoint:\n",
    "                config.teacher_temperatures = checkpoint['teacher_temperatures']\n",
    "                logger.info(f\"Teacher temperatures loaded from checkpoint: {config.teacher_temperatures}\")\n",
    "                \n",
    "            # Print additional metadata if available\n",
    "            if 'test_metrics' in checkpoint:\n",
    "                logger.info(f\"Previous test metrics found in checkpoint:\")\n",
    "                for k, v in checkpoint['test_metrics'].items():\n",
    "                    logger.info(f\"  - {k}: {v}\")\n",
    "        else:\n",
    "            # For direct state dict, check the first key to determine model type\n",
    "            first_keys = list(checkpoint.keys())[:5]  # Look at first few keys\n",
    "            \n",
    "            if any('class_token' in key for key in first_keys):\n",
    "                model = create_model_architecture('vit')\n",
    "            elif any('features.0.1.weight' in key for key in first_keys):\n",
    "                model = create_model_architecture('efficientnet')\n",
    "            elif any('Conv2d_1a_3x3' in key for key in first_keys):\n",
    "                model = create_model_architecture('inception')\n",
    "            elif any('inverted_bottleneck' in key for key in first_keys):\n",
    "                model = create_model_architecture('mobilenet')\n",
    "            elif any('layer1.0.conv1.weight' in key for key in first_keys):\n",
    "                model = create_model_architecture('resnet')\n",
    "            elif any('denseblock' in key for key in first_keys):\n",
    "                model = create_model_architecture('densenet')\n",
    "            else:\n",
    "                model = create_model_architecture('efficientnet')\n",
    "                \n",
    "            model.load_state_dict(checkpoint)\n",
    "            logger.info(f\"Student model state loaded directly from checkpoint\")\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        logger.info(f\"Student model loaded successfully and set to evaluation mode\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load student model: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def load_models(config, device):\n",
    "    \"\"\"Load all models for evaluation\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Define which models to load\n",
    "    model_names = ['student'] + list(config.teacher_model_paths.keys())\n",
    "    \n",
    "    # Load each model\n",
    "    for model_name in model_names:\n",
    "        if model_name == 'student':\n",
    "            model = load_student_model(config, device)\n",
    "        else:\n",
    "            model = load_model(config, device, model_name)\n",
    "        if model is not None:\n",
    "            models[model_name] = model\n",
    "    \n",
    "    logger.info(f\"Loaded {len(models)} models for evaluation\")\n",
    "    return models\n",
    "\n",
    "\n",
    "####################################\n",
    "# 5. Inference\n",
    "####################################\n",
    "def run_inference(model, loader, config, device, model_name):\n",
    "    \"\"\"Run inference on the test set\"\"\"\n",
    "    logger.info(f\"Running inference for {model_name} model...\")\n",
    "    \n",
    "    # Store predictions, targets and probabilities\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    clear_gpu_cache()\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=f\"Evaluating {model_name}\"):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Use mixed precision if available and enabled\n",
    "            if config.use_amp and device.type == 'cuda':\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(images)\n",
    "                    # Handle inception output format\n",
    "                    if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                # Handle inception output format\n",
    "                if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(probs, dim=1)\n",
    "            \n",
    "            # Store results (on CPU to save GPU memory)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "            # Free memory\n",
    "            del images, outputs, probs, preds\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    \n",
    "    logger.info(f\"Inference complete on {len(all_targets)} samples for {model_name}\")\n",
    "    return np.array(all_targets), np.array(all_preds), all_probs\n",
    "\n",
    "\n",
    "####################################\n",
    "# 6. Evaluation Metrics\n",
    "####################################\n",
    "def compute_ece(probs, targets, n_bins=15):\n",
    "    \"\"\"Compute Expected Calibration Error (ECE)\"\"\"\n",
    "    # Convert targets to numpy array if it's not already\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.numpy()\n",
    "    \n",
    "    # Get the predicted class and its confidence\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = (predictions == targets).astype(np.float32)\n",
    "    \n",
    "    # Sort by confidence\n",
    "    sorted_indices = np.argsort(confidences)\n",
    "    sorted_confidences = confidences[sorted_indices]\n",
    "    sorted_accuracies = accuracies[sorted_indices]\n",
    "    \n",
    "    # Create bins\n",
    "    bin_size = 1.0 / n_bins\n",
    "    bins = np.linspace(0, 1.0, n_bins+1)\n",
    "    ece = 0.0\n",
    "    mce = 0.0  # Maximum Calibration Error\n",
    "    ace = 0.0  # Average Calibration Error (unweighted)\n",
    "    rmsce = 0.0  # Root Mean Square Calibration Error\n",
    "    \n",
    "    bin_confidences = []\n",
    "    bin_accuracies = []\n",
    "    bin_counts = []\n",
    "    cal_errors = []  # Store calibration errors for each bin\n",
    "    \n",
    "    valid_bins = 0  # Count bins with samples\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        # Determine bin boundaries\n",
    "        bin_start = bins[i]\n",
    "        bin_end = bins[i+1]\n",
    "        \n",
    "        # Find samples in bin\n",
    "        in_bin = (sorted_confidences >= bin_start) & (sorted_confidences < bin_end)\n",
    "        bin_count = np.sum(in_bin)\n",
    "        bin_counts.append(bin_count)\n",
    "        \n",
    "        if bin_count > 0:\n",
    "            bin_conf = np.mean(sorted_confidences[in_bin])\n",
    "            bin_acc = np.mean(sorted_accuracies[in_bin])\n",
    "            bin_confidences.append(bin_conf)\n",
    "            bin_accuracies.append(bin_acc)\n",
    "            \n",
    "            # Calculate calibration error for this bin\n",
    "            cal_error = np.abs(bin_acc - bin_conf)\n",
    "            cal_errors.append(cal_error)\n",
    "            \n",
    "            # Add weighted error to ECE\n",
    "            ece += (bin_count / len(confidences)) * cal_error\n",
    "            \n",
    "            # Update MCE (maximum error)\n",
    "            mce = max(mce, cal_error)\n",
    "            \n",
    "            # Add to unweighted average (ACE)\n",
    "            ace += cal_error\n",
    "            \n",
    "            # Add to root mean square calculation\n",
    "            rmsce += cal_error ** 2\n",
    "            \n",
    "            valid_bins += 1\n",
    "        else:\n",
    "            bin_confidences.append((bin_start + bin_end) / 2)\n",
    "            bin_accuracies.append(0)\n",
    "    \n",
    "    # Finalize ACE and RMSCE calculations\n",
    "    ace = ace / valid_bins if valid_bins > 0 else 0\n",
    "    rmsce = np.sqrt(rmsce / valid_bins) if valid_bins > 0 else 0\n",
    "    \n",
    "    return ece, mce, ace, rmsce, bin_confidences, bin_accuracies, bin_counts\n",
    "\n",
    "def analyze_results(y_true, y_pred, y_probs, class_names, config, model_name):\n",
    "    \"\"\"Generate and save evaluation metrics for a single model\"\"\"\n",
    "    logger.info(f\"Analyzing {model_name} model performance...\")\n",
    "    \n",
    "    # Create output directory for this model\n",
    "    model_output_dir = os.path.join(config.output_dir, model_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Calculate and print accuracy\n",
    "    accuracy = np.mean(y_true == y_pred) * 100\n",
    "    logger.info(f\"[{model_name}] Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # 2. Calculate F1 score, precision, and recall\n",
    "    f1 = f1_score(y_true, y_pred, average='macro') * 100\n",
    "    precision = precision_score(y_true, y_pred, average='macro') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='macro') * 100\n",
    "    logger.info(f\"[{model_name}] F1 Score (macro): {f1:.2f}%\")\n",
    "    logger.info(f\"[{model_name}] Precision (macro): {precision:.2f}%\")\n",
    "    logger.info(f\"[{model_name}] Recall (macro): {recall:.2f}%\")\n",
    "    \n",
    "    # 3. Calculate Expected Calibration Error and other calibration metrics\n",
    "    ece, mce, ace, rmsce, bin_confs, bin_accs, bin_counts = compute_ece(y_probs, y_true, config.n_bins_calibration)\n",
    "    logger.info(f\"[{model_name}] Expected Calibration Error: {ece:.4f}\")\n",
    "    logger.info(f\"[{model_name}] Maximum Calibration Error: {mce:.4f}\")\n",
    "    logger.info(f\"[{model_name}] Average Calibration Error: {ace:.4f}\")\n",
    "    logger.info(f\"[{model_name}] Root Mean Square Calibration Error: {rmsce:.4f}\")\n",
    "    \n",
    "    # 4. Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"Confusion Matrix - {model_name} (Accuracy: {accuracy:.2f}%)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_output_dir}/confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Generate classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, digits=3)\n",
    "    logger.info(f\"\\n[{model_name}] Classification Report:\")\n",
    "    logger.info(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open(f\"{model_output_dir}/classification_report.txt\", \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Test Accuracy: {accuracy:.2f}%\\n\")\n",
    "        f.write(f\"F1 Score (macro): {f1:.2f}%\\n\")\n",
    "        f.write(f\"Precision (macro): {precision:.2f}%\\n\")\n",
    "        f.write(f\"Recall (macro): {recall:.2f}%\\n\")\n",
    "        f.write(f\"Expected Calibration Error: {ece:.4f}\\n\")\n",
    "        f.write(f\"Maximum Calibration Error: {mce:.4f}\\n\")\n",
    "        f.write(f\"Average Calibration Error: {ace:.4f}\\n\")\n",
    "        f.write(f\"Root Mean Square Calibration Error: {rmsce:.4f}\\n\\n\")\n",
    "        f.write(report)\n",
    "    \n",
    "    # 6. Per-class accuracy\n",
    "    class_acc = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(class_names), y=class_acc)\n",
    "    plt.title(f\"{model_name}: Per-Class Accuracy\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_output_dir}/per_class_accuracy.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Plot calibration reliability diagram\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Plot actual calibration\n",
    "    bin_edges = np.linspace(0, 1, config.n_bins_calibration + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Plot bins with their accuracies\n",
    "    bin_counts_norm = np.array(bin_counts) / sum(bin_counts)\n",
    "    plt.bar(bin_centers, bin_accs, width=1/config.n_bins_calibration, alpha=0.3, label='Accuracy in bin')\n",
    "    \n",
    "    # Add histogram of confidence distribution\n",
    "    twin_ax = plt.twinx()\n",
    "    twin_ax.bar(bin_centers, bin_counts_norm, width=1/config.n_bins_calibration, alpha=0.2, color='g', label='Proportion of samples')\n",
    "    twin_ax.set_ylabel('Proportion of Samples')\n",
    "    \n",
    "    # Connect actual calibration points\n",
    "    plt.plot(bin_confs, bin_accs, 'ro-', label=f'Actual Calibration (ECE={ece:.4f}, MCE={mce:.4f})')\n",
    "    \n",
    "    plt.title(f'{model_name} - Calibration Reliability Diagram')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_output_dir}/calibration_curve.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save all metrics as a dictionary\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_score': float(f1),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'ece': float(ece),\n",
    "        'mce': float(mce),\n",
    "        'ace': float(ace),\n",
    "        'rmsce': float(rmsce),\n",
    "        'per_class_accuracy': [float(acc) for acc in class_acc.tolist()]\n",
    "    }\n",
    "    \n",
    "    # Save metrics as JSON\n",
    "    try:\n",
    "        with open(f\"{model_output_dir}/metrics.json\", \"w\") as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "        logger.info(f\"Metrics saved to {model_output_dir}/metrics.json\")\n",
    "    except TypeError as e:\n",
    "        logger.error(f\"Error saving metrics JSON: {str(e)}\")\n",
    "        # Debug which values are causing issues\n",
    "        for key, value in metrics.items():\n",
    "            try:\n",
    "                json.dumps({key: value})\n",
    "            except TypeError:\n",
    "                logger.error(f\"Key {key} with value type {type(value)} is not JSON serializable\")\n",
    "        # Try a more robust approach\n",
    "        import json as simplejson\n",
    "        with open(f\"{model_output_dir}/metrics.json\", \"w\") as f:\n",
    "            simplejson.dump(metrics, f, indent=4, ignore_nan=True)\n",
    "    \n",
    "    logger.info(f\"[{model_name}] Evaluation results saved to {model_output_dir}\")\n",
    "    return metrics\n",
    "\n",
    "def compare_models(all_metrics, config):\n",
    "    \"\"\"Create comparison visualizations for all models\"\"\"\n",
    "    logger.info(\"Generating model comparison visualizations...\")\n",
    "    \n",
    "    if len(all_metrics) <= 1:\n",
    "        logger.info(\"Not enough models to compare.\")\n",
    "        return\n",
    "    \n",
    "    # Extract model names and metrics\n",
    "    model_names = [metrics['model_name'] for metrics in all_metrics]\n",
    "    accuracies = [metrics['accuracy'] for metrics in all_metrics]\n",
    "    f1_scores = [metrics['f1_score'] for metrics in all_metrics]\n",
    "    precisions = [metrics['precision'] for metrics in all_metrics]\n",
    "    recalls = [metrics['recall'] for metrics in all_metrics]\n",
    "    eces = [metrics['ece'] for metrics in all_metrics]\n",
    "    \n",
    "    # Set colors - make student model stand out\n",
    "    colors = ['#1f77b4' if name != 'student' else '#d62728' for name in model_names]\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    ax = plt.subplot(111)\n",
    "    bars = ax.bar(model_names, accuracies, color=colors)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f\"{height:.2f}%\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title('Accuracy Comparison Across Models')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, max(accuracies) + 5)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/accuracy_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. F1, Precision, Recall comparison\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    bars1 = ax.bar(x - width, f1_scores, width, label='F1 Score', alpha=0.7)\n",
    "    bars2 = ax.bar(x, precisions, width, label='Precision', alpha=0.7)\n",
    "    bars3 = ax.bar(x + width, recalls, width, label='Recall', alpha=0.7)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.set_ylabel('Score (%)')\n",
    "    ax.set_title('F1, Precision, and Recall Comparison')\n",
    "    ax.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/f1_precision_recall_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. ECE comparison (lower is better)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    ax = plt.subplot(111)\n",
    "    bars = ax.bar(model_names, eces, color=colors)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "                f\"{height:.4f}\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title('Calibration (ECE) Comparison Across Models (Lower is Better)')\n",
    "    plt.ylabel('Expected Calibration Error')\n",
    "    plt.ylim(0, max(eces) + 0.01)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/ece_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Combined radar chart for all metrics\n",
    "    metrics_names = ['Accuracy (%)', 'F1 Score (%)', 'Precision (%)', 'Recall (%)', \n",
    "                      'Calibration (1-ECE)']\n",
    "    \n",
    "    # Normalize ECE to be between 0-100 like other metrics, but inverted (1-ECE)*100\n",
    "    ece_normalized = [(1 - ece) * 100 for ece in eces]\n",
    "    \n",
    "    # For each model, create a radar chart\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, polar=True)\n",
    "        \n",
    "        # Prepare data - all metrics should be 0-100 scale\n",
    "        values = [accuracies[i], f1_scores[i], precisions[i], recalls[i], ece_normalized[i]]\n",
    "        \n",
    "        # Duplicate the first value to close the loop\n",
    "        values.append(values[0])\n",
    "        metrics_with_first = metrics_names + [metrics_names[0]]\n",
    "        \n",
    "        # Plot radar chart\n",
    "        angles = np.linspace(0, 2*np.pi, len(metrics_names), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "        \n",
    "        # Set labels and grid\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metrics_names)\n",
    "        ax.set_yticks([20, 40, 60, 80, 100])\n",
    "        ax.set_title(f\"{model_name} Performance Metrics\", size=15, pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{config.output_dir}/{model_name}_radar_chart.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Combined radar chart for all models\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Plot for each model\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        values = [accuracies[i], f1_scores[i], precisions[i], recalls[i], ece_normalized[i]]\n",
    "        values.append(values[0])  # Close the loop\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model_name)\n",
    "        \n",
    "    # Set labels and grid\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.set_yticks([20, 40, 60, 80, 100])\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_title(\"Model Performance Comparison\", size=15, pad=20)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/all_models_radar_chart.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save comparison metrics as JSON\n",
    "    comparison = {\n",
    "        'models': model_names,\n",
    "        'accuracy': accuracies,\n",
    "        'f1_score': f1_scores,\n",
    "        'precision': precisions,\n",
    "        'recall': recalls,\n",
    "        'ece': eces\n",
    "    }\n",
    "    \n",
    "    with open(f\"{config.output_dir}/model_comparison.json\", \"w\") as f:\n",
    "        json.dump(comparison, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Model comparison visualizations saved to {config.output_dir}\")\n",
    "\n",
    "\n",
    "####################################\n",
    "# 7. Visualization Helpers\n",
    "####################################\n",
    "def visualize_predictions(model, test_dataset, config, device, model_name, num_examples=5):\n",
    "    \"\"\"Visualize random predictions with original CIFAR-10 images\"\"\"\n",
    "    logger.info(f\"Generating prediction visualizations for {model_name}...\")\n",
    "    \n",
    "    # Create output directory for this model\n",
    "    model_output_dir = os.path.join(config.output_dir, model_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use a professional style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "        'font.size': 9,\n",
    "        'axes.titlesize': 10,\n",
    "        'axes.labelsize': 9\n",
    "    })\n",
    "    \n",
    "    # Define colors for correct and incorrect predictions\n",
    "    correct_color = '#1f77b4'  # Professional blue\n",
    "    incorrect_color = '#d62728'  # Professional red\n",
    "    \n",
    "    # Select random indices\n",
    "    indices = np.random.choice(len(test_dataset), size=num_examples*len(config.classes), replace=False)\n",
    "    \n",
    "    # Get original images and labels\n",
    "    originals, true_labels = get_original_images(config, indices)\n",
    "    \n",
    "    # Prepare a batch of transformed images for the model\n",
    "    batch_images = torch.stack([test_dataset[idx][0] for idx in indices]).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if config.use_amp and device.type == 'cuda':\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(batch_images)\n",
    "                # Handle inception output format\n",
    "                if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "        else:\n",
    "            outputs = model(batch_images)\n",
    "            # Handle inception output format\n",
    "            if model_name == 'inception' and isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "    \n",
    "    # Get prediction probabilities and classes\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    pred_scores, pred_labels = torch.max(probs, dim=1)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    pred_labels = pred_labels.cpu().numpy()\n",
    "    pred_scores = pred_scores.cpu().numpy()\n",
    "    \n",
    "    # Plot results - create a figure with better proportions\n",
    "    fig, axes = plt.subplots(len(config.classes), num_examples, figsize=(num_examples*2.5, len(config.classes)*2))\n",
    "    fig.suptitle(f\"CIFAR-10 Prediction Examples ({model_name})\", fontsize=14, y=0.98)\n",
    "    \n",
    "    # Group samples by true class\n",
    "    class_indices = {i: [] for i in range(len(config.classes))}\n",
    "    for i, label in enumerate(true_labels):\n",
    "        if len(class_indices[label]) < num_examples:\n",
    "            class_indices[label].append(i)\n",
    "    \n",
    "    for class_idx in range(len(config.classes)):\n",
    "        for example_idx in range(num_examples):\n",
    "            ax = axes[class_idx, example_idx]\n",
    "            \n",
    "            # Check if we have enough examples for this class\n",
    "            if example_idx < len(class_indices[class_idx]):\n",
    "                i = class_indices[class_idx][example_idx]\n",
    "                \n",
    "                # Plot image with a border\n",
    "                img = originals[i].permute(1, 2, 0).numpy()\n",
    "                ax.imshow(img)\n",
    "                \n",
    "                # Add prediction info with better formatting\n",
    "                true_label = true_labels[i]\n",
    "                pred_label = pred_labels[i]\n",
    "                color = correct_color if true_label == pred_label else incorrect_color\n",
    "                \n",
    "                # Create a clean title with proper formatting\n",
    "                ax.set_title(f\"True: {config.classes[true_label]}\\nPred: {config.classes[pred_label]}\\nConf: {pred_scores[i]:.3f}\", \n",
    "                            color=color, fontsize=9, pad=3)\n",
    "                \n",
    "                # Add a professional border\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor(color)\n",
    "                    spine.set_linewidth(1.5)\n",
    "            else:\n",
    "                # If not enough examples, hide the empty subplot\n",
    "                ax.set_visible(False)\n",
    "            \n",
    "            # Remove ticks for all subplots (whether they have content or not)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    # Add row labels on the left\n",
    "    for class_idx in range(len(config.classes)):\n",
    "        if axes[class_idx, 0].get_visible():  # Only add label if the first subplot in row is visible\n",
    "            axes[class_idx, 0].set_ylabel(config.classes[class_idx], fontsize=10, \n",
    "                                        rotation=90, labelpad=10, va='center')\n",
    "    \n",
    "    # Add a footer with model information\n",
    "    plt.figtext(0.5, 0.01, \n",
    "               f\"{model_name} model evaluation on CIFAR-10 test set\", \n",
    "               ha=\"center\", fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, bottom=0.05)\n",
    "    plt.savefig(f\"{model_output_dir}/prediction_examples.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Prediction visualizations saved to {model_output_dir}/prediction_examples.png\")\n",
    "\n",
    "\n",
    "####################################\n",
    "# 8. Feature Extraction and Comparison\n",
    "####################################\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Feature extraction helper for comparing intermediate representations\"\"\"\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.model = model\n",
    "        self.features = None\n",
    "        \n",
    "        # Flag to track if hook was registered successfully\n",
    "        self.hook_registered = False\n",
    "        \n",
    "        # Register hook to extract features\n",
    "        for name, module in model.named_modules():\n",
    "            if layer_name in name:  # More flexible matching\n",
    "                module.register_forward_hook(self.hook)\n",
    "                self.hook_registered = True\n",
    "                logger.info(f\"Hook registered for {name}\")\n",
    "                break\n",
    "        \n",
    "        if not self.hook_registered:\n",
    "            logger.warning(f\"Could not find layer {layer_name} in model, using fallback\")\n",
    "            \n",
    "            # Use fallback approach - register hook on the last substantial layer\n",
    "            modules = list(model.named_modules())\n",
    "            \n",
    "            # Try to find a good feature layer heuristically\n",
    "            for name, module in reversed(modules):\n",
    "                if any(x in name.lower() for x in ['avgpool', 'layer4', 'features', 'block', 'encoder']):\n",
    "                    if not any(x in name.lower() for x in ['dropout', 'head', 'fc', 'classification']):\n",
    "                        module.register_forward_hook(self.hook)\n",
    "                        self.hook_registered = True\n",
    "                        logger.info(f\"Fallback hook registered for {name}\")\n",
    "                        break\n",
    "                        \n",
    "    def hook(self, module, input, output):\n",
    "        self.features = output\n",
    "        \n",
    "    def get_features(self, x):\n",
    "        _ = self.model(x)\n",
    "        return self.features\n",
    "\n",
    "def compare_features(models, config, device):\n",
    "    \"\"\"Compare feature representations between models\"\"\"\n",
    "    logger.info(\"Comparing feature representations between models...\")\n",
    "    \n",
    "    if len(models) <= 1:\n",
    "        logger.info(\"Not enough models to compare features.\")\n",
    "        return\n",
    "    \n",
    "    # Define feature extraction layers for each model\n",
    "    feature_layers = {\n",
    "        'vit': 'encoder.ln',\n",
    "        'efficientnet': 'features.8',\n",
    "        'inception': 'Mixed_7c',\n",
    "        'mobilenet': 'features',\n",
    "        'resnet': 'layer4',\n",
    "        'densenet': 'features',\n",
    "        'student': 'features.8'\n",
    "    }\n",
    "    \n",
    "    # Create feature extractors\n",
    "    feature_extractors = {}\n",
    "    for name, model in models.items():\n",
    "        layer_name = feature_layers.get(name)\n",
    "        if layer_name:\n",
    "            feature_extractors[name] = FeatureExtractor(model, layer_name)\n",
    "            \n",
    "    # Skip if no extractors were created\n",
    "    if not feature_extractors:\n",
    "        logger.warning(\"No feature extractors could be created.\")\n",
    "        return\n",
    "            \n",
    "    # Use CIFAR-10 test images for visualization\n",
    "    test_dataset = get_test_dataset(config, 'student')  # Use student transform as default\n",
    "    \n",
    "    # Select random samples\n",
    "    sample_indices = np.random.choice(len(test_dataset), size=100, replace=False)\n",
    "    sample_images = torch.stack([test_dataset[idx][0] for idx in sample_indices])\n",
    "    \n",
    "    # Extract features for each model\n",
    "    features = {}\n",
    "    for name, extractor in feature_extractors.items():\n",
    "        if extractor.hook_registered:\n",
    "            # Process in batches to avoid OOM\n",
    "            batch_size = 10\n",
    "            all_features = []\n",
    "            \n",
    "            for i in range(0, sample_images.shape[0], batch_size):\n",
    "                batch = sample_images[i:i+batch_size].to(device)\n",
    "                with torch.no_grad():\n",
    "                    if config.use_amp and device.type == 'cuda':\n",
    "                        with autocast(device_type='cuda'):\n",
    "                            _ = models[name](batch)\n",
    "                    else:\n",
    "                        _ = models[name](batch)\n",
    "                \n",
    "                # Get features from extractor\n",
    "                if extractor.features is not None:\n",
    "                    # For feature maps, use global average pooling to get a vector\n",
    "                    if len(extractor.features.shape) == 4:  # B x C x H x W\n",
    "                        batch_features = F.adaptive_avg_pool2d(extractor.features, (1, 1))\n",
    "                        batch_features = batch_features.view(batch_features.size(0), -1)\n",
    "                    else:\n",
    "                        batch_features = extractor.features\n",
    "                        \n",
    "                        # If features are still not 2D, flatten them\n",
    "                        if len(batch_features.shape) > 2:\n",
    "                            batch_features = batch_features.view(batch_features.shape[0], -1)\n",
    "                    \n",
    "                    all_features.append(batch_features.cpu())\n",
    "            \n",
    "            # Concatenate all batches\n",
    "            if all_features:\n",
    "                features[name] = torch.cat(all_features, dim=0)\n",
    "                logger.info(f\"Extracted features for {name}: shape {features[name].shape}\")\n",
    "    \n",
    "    # Skip if features couldn't be extracted\n",
    "    if len(features) <= 1:\n",
    "        logger.warning(\"Could not extract features from multiple models for comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Compute pairwise cosine similarity between model features\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Normalize features for each model\n",
    "    normalized_features = {}\n",
    "    for name, feat in features.items():\n",
    "        # Convert to numpy for sklearn\n",
    "        feat_np = feat.numpy()\n",
    "        \n",
    "        # Normalize each feature vector\n",
    "        norms = np.linalg.norm(feat_np, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1  # Avoid division by zero\n",
    "        normalized_features[name] = feat_np / norms\n",
    "    \n",
    "    # Create cosine similarity matrix\n",
    "    model_names = list(normalized_features.keys())\n",
    "    similarity_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "    \n",
    "    for i, name1 in enumerate(model_names):\n",
    "        for j, name2 in enumerate(model_names):\n",
    "            # For each pair of models, compute average pairwise cosine similarity\n",
    "            # between their feature vectors for the same images\n",
    "            if i <= j:  # Compute only upper triangle (matrix is symmetric)\n",
    "                feat1 = normalized_features[name1]\n",
    "                feat2 = normalized_features[name2]\n",
    "                \n",
    "                # Compute cosine similarity for each sample and average\n",
    "                similarities = np.sum(feat1 * feat2, axis=1)\n",
    "                avg_similarity = np.mean(similarities)\n",
    "                \n",
    "                similarity_matrix[i, j] = avg_similarity\n",
    "                similarity_matrix[j, i] = avg_similarity  # Mirror to lower triangle\n",
    "    \n",
    "    # Create similarity dataframe\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarity_matrix, \n",
    "        index=model_names,\n",
    "        columns=model_names\n",
    "    )\n",
    "    \n",
    "    # Plot the similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_df, annot=True, cmap='viridis', vmin=0, vmax=1, \n",
    "                square=True, fmt='.3f', linewidths=0.5)\n",
    "    plt.title('Feature Representation Similarity Between Models', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/feature_similarity_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save similarity matrix as CSV\n",
    "    similarity_df.to_csv(f\"{config.output_dir}/feature_similarity_matrix.csv\")\n",
    "    \n",
    "    logger.info(f\"Feature similarity analysis saved to {config.output_dir}\")\n",
    "    return similarity_df\n",
    "\n",
    "\n",
    "####################################\n",
    "# 9. GradCAM Implementation\n",
    "####################################\n",
    "class GradCAM:\n",
    "    \"\"\"Gradient-weighted Class Activation Mapping\"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        \n",
    "        # Register hooks\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "    \n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        # Forward pass\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Get prediction if target class not specified\n",
    "        if target_class is None:\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_tensor)\n",
    "                target_class = output.argmax(dim=1)\n",
    "        \n",
    "        # Forward pass with gradients\n",
    "        output = self.model(input_tensor)\n",
    "        loss = output[:, target_class].sum()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "        \n",
    "        # Generate CAM\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # Upsample CAM to input size\n",
    "        cam = torch.nn.functional.interpolate(\n",
    "            cam, \n",
    "            size=input_tensor.shape[2:], \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # Normalize CAM\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        return cam\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "def visualize_gradcam(model, test_dataset, config, device, model_name):\n",
    "    \"\"\"Create GradCAM visualizations for each class with improved scientific appearance\"\"\"\n",
    "    logger.info(f\"Generating GradCAM visualizations for {model_name}...\")\n",
    "    \n",
    "    # Create output directory for this model\n",
    "    model_output_dir = os.path.join(config.output_dir, model_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set scientific plotting style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "        'font.size': 10,\n",
    "        'axes.titlesize': 11,\n",
    "        'axes.labelsize': 10\n",
    "    })\n",
    "    \n",
    "    # Find one sample per class\n",
    "    samples_by_class = {c: None for c in range(len(config.classes))}\n",
    "    indices_by_class = {c: None for c in range(len(config.classes))}\n",
    "    \n",
    "    for idx in tqdm(range(len(test_dataset)), desc=f\"Finding class samples for {model_name}\"):\n",
    "        _, label = test_dataset[idx]\n",
    "        if samples_by_class[label] is None:\n",
    "            samples_by_class[label] = test_dataset[idx][0].unsqueeze(0)\n",
    "            indices_by_class[label] = idx\n",
    "        if all(v is not None for v in samples_by_class.values()):\n",
    "            break\n",
    "    \n",
    "    # Find appropriate target layer for GradCAM\n",
    "    target_layer = None\n",
    "    \n",
    "    # Detect model architecture first\n",
    "    model_architecture = None\n",
    "    for name, _ in model.named_modules():\n",
    "        if 'class_token' in name:\n",
    "            model_architecture = 'vit'\n",
    "            break\n",
    "        elif 'features.0.1.weight' in name or 'features' in name and model_name in ['efficientnet', 'student']:\n",
    "            model_architecture = 'efficientnet'\n",
    "            break\n",
    "        elif any(x in name for x in ['Mixed_7c', 'Conv2d_1a_3x3']):\n",
    "            model_architecture = 'inception'\n",
    "            break\n",
    "        elif 'inverted_bottleneck' in name:\n",
    "            model_architecture = 'mobilenet'\n",
    "            break\n",
    "        elif any(x in name for x in ['layer1', 'layer2', 'layer3', 'layer4']):\n",
    "            model_architecture = 'resnet'\n",
    "            break\n",
    "        elif 'denseblock' in name:\n",
    "            model_architecture = 'densenet'\n",
    "            break\n",
    "    \n",
    "    logger.info(f\"Detected architecture for {model_name}: {model_architecture}\")\n",
    "    \n",
    "    # Model-specific target layer selection based on detected architecture\n",
    "    if model_architecture == 'vit':\n",
    "        for name, module in model.named_modules():\n",
    "            if 'encoder.ln' in name or 'blocks.11' in name:\n",
    "                target_layer = module\n",
    "                break\n",
    "    elif model_architecture == 'efficientnet':\n",
    "        for name, module in model.named_modules():\n",
    "            if 'features' in name and isinstance(module, torch.nn.Sequential):\n",
    "                # Get the last feature layer\n",
    "                target_layer = module[-1]\n",
    "                break\n",
    "    elif model_architecture == 'inception':\n",
    "        # Check if the model has inception attribute (wrapper) or is the base model\n",
    "        if hasattr(model, 'is_wrapper') and model.is_wrapper:\n",
    "            target_layer = model.Mixed_7c\n",
    "        elif hasattr(model, 'Mixed_7c'):\n",
    "            target_layer = model.Mixed_7c\n",
    "        else:\n",
    "            # For other cases, find a suitable conv layer\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d) and 'conv' in name.lower():\n",
    "                    target_layer = module\n",
    "                    logger.info(f\"Using fallback layer for inception: {name}\")\n",
    "                    break\n",
    "    elif model_architecture == 'mobilenet':\n",
    "        target_layer = None\n",
    "        for name, module in model.named_modules():\n",
    "            if 'features' in name and isinstance(module, torch.nn.Sequential):\n",
    "                target_layer = module[-1]\n",
    "                break\n",
    "    elif model_architecture == 'resnet':\n",
    "        for name, module in model.named_modules():\n",
    "            if 'layer4' in name:\n",
    "                target_layer = module\n",
    "                break\n",
    "    elif model_architecture == 'densenet':\n",
    "        for name, module in model.named_modules():\n",
    "            if 'denseblock4' in name:\n",
    "                target_layer = module\n",
    "                break\n",
    "    \n",
    "    # Fallback if we still don't have a target layer\n",
    "    if target_layer is None:\n",
    "        logger.warning(f\"Could not find specific target layer for {model_name}, using fallback approach\")\n",
    "        # Find the last convolutional layer as a fallback\n",
    "        last_conv = None\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Conv2d):\n",
    "                last_conv = module\n",
    "        target_layer = last_conv\n",
    "    \n",
    "    if target_layer is None:\n",
    "        logger.error(f\"Could not find any suitable layer for GradCAM in {model_name}\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Using target layer for {model_name}: {target_layer}\")\n",
    "    \n",
    "    # Initialize GradCAM\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Use a scientific colormap\n",
    "    cmap = 'inferno'  # Scientific colormap that works well for heatmaps\n",
    "    \n",
    "    # Create a figure with increased top margin for better title placement\n",
    "    fig = plt.figure(figsize=(15, 14))\n",
    "    \n",
    "    # Create GridSpec with adjusted height ratios to leave room for the title\n",
    "    # Make the last column narrower for the colorbar\n",
    "    gs = fig.add_gridspec(5, 6, height_ratios=[0.5, 1, 1, 1, 1], \n",
    "                         width_ratios=[1, 1, 1, 1, 1, 0.05])  # Narrower colorbar column\n",
    "    \n",
    "    # Add title with improved positioning\n",
    "    fig.suptitle(f\"GradCAM Visualizations for CIFAR-10 Classes ({model_name})\", \n",
    "                fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Create a mapping for grid with proper organization - adjust for the spacing row\n",
    "    class_to_position = {\n",
    "        0: (1, 0),  # airplane - shifted down one row\n",
    "        1: (1, 1),  # automobile\n",
    "        2: (1, 2),  # bird\n",
    "        3: (1, 3),  # cat\n",
    "        4: (1, 4),  # deer\n",
    "        5: (3, 0),  # dog - shifted down one row\n",
    "        6: (3, 1),  # frog\n",
    "        7: (3, 2),  # horse\n",
    "        8: (3, 3),  # ship\n",
    "        9: (3, 4),  # truck\n",
    "    }\n",
    "    \n",
    "    # Variable to store the last heatmap for colorbar reference\n",
    "    last_heatmap = None\n",
    "    \n",
    "    for class_idx in range(len(config.classes)):\n",
    "        logger.info(f\"[{model_name}] Generating GradCAM for class '{config.classes[class_idx]}'\")\n",
    "        \n",
    "        # Get the sample\n",
    "        input_tensor = samples_by_class[class_idx].to(device)\n",
    "        \n",
    "        # Generate CAM\n",
    "        try:\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=config.use_amp):\n",
    "                cam = grad_cam.generate_cam(input_tensor, target_class=class_idx)\n",
    "            cam = cam.cpu().numpy()[0, 0]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating GradCAM for {model_name}, class {class_idx}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # Get original image\n",
    "        orig_imgs, _ = get_original_images(config, [indices_by_class[class_idx]])\n",
    "        orig_img = orig_imgs[0].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Upsample original image to match model input size\n",
    "        input_size = config.get_input_size(model_name)\n",
    "        img_upsampled = transforms.Resize(input_size)(orig_imgs[0])\n",
    "        img_upsampled = img_upsampled.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Get row, col position\n",
    "        row, col = class_to_position[class_idx]\n",
    "        \n",
    "        # Plot original image\n",
    "        ax_orig = fig.add_subplot(gs[row, col])\n",
    "        ax_orig.imshow(img_upsampled)\n",
    "        ax_orig.set_title(f\"{config.classes[class_idx]} (Original)\", fontsize=11)\n",
    "        ax_orig.set_xticks([])\n",
    "        ax_orig.set_yticks([])\n",
    "        \n",
    "        # Plot heatmap overlay\n",
    "        ax_overlay = fig.add_subplot(gs[row+1, col])\n",
    "        ax_overlay.imshow(img_upsampled)\n",
    "        last_heatmap = ax_overlay.imshow(cam, cmap=cmap, alpha=0.6)\n",
    "        ax_overlay.set_title(f\"{config.classes[class_idx]} (GradCAM)\", fontsize=11)\n",
    "        ax_overlay.set_xticks([])\n",
    "        ax_overlay.set_yticks([])\n",
    "    \n",
    "    # Add a colorbar for the heatmap - use a specific position that won't conflict\n",
    "    # Make the colorbar narrower to match the reference image\n",
    "    cax = fig.add_subplot(gs[:, 5])  # Use the last column for colorbar\n",
    "    cbar = fig.colorbar(last_heatmap, cax=cax)\n",
    "    cbar.set_label('Activation Strength', fontsize=10)\n",
    "    \n",
    "    # Add a footer with model information\n",
    "    fig.text(0.5, 0.02, \n",
    "             f\"GradCAM visualizations show regions the {model_name} model focuses on when classifying each category\",\n",
    "             ha=\"center\", fontsize=10, style='italic')\n",
    "    \n",
    "    # Adjust spacing - don't use tight_layout here\n",
    "    plt.subplots_adjust(right=0.95, top=0.92, bottom=0.05, wspace=0.3, hspace=0.4)\n",
    "    \n",
    "    plt.savefig(f\"{model_output_dir}/gradcam_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Clean up\n",
    "    grad_cam.remove_hooks()\n",
    "    \n",
    "    logger.info(f\"GradCAM visualizations saved to {model_output_dir}/gradcam_visualization.png\")\n",
    "\n",
    "\n",
    "####################################\n",
    "# 10. Main Evaluation Function\n",
    "####################################\n",
    "def main():\n",
    "    \"\"\"Main evaluation pipeline\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Mutual Learning Models CIFAR-10 Evaluation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Setup\n",
    "    config, device = setup_environment()\n",
    "    \n",
    "    try:\n",
    "        # 1. Load all models\n",
    "        models = load_models(config, device)\n",
    "        \n",
    "        if not models:\n",
    "            logger.error(\"No models could be loaded. Check model paths.\")\n",
    "            return 1\n",
    "        \n",
    "        # Store all metrics for comparison\n",
    "        all_metrics = []\n",
    "        \n",
    "        # 2. Evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            # Get model-specific test dataset\n",
    "            test_dataset = get_test_dataset(config, model_name)\n",
    "            test_loader = create_data_loader(test_dataset, config)\n",
    "            \n",
    "            # Run inference\n",
    "            targets, predictions, probabilities = run_inference(model, test_loader, config, device, model_name)\n",
    "            \n",
    "            # Generate metrics\n",
    "            metrics = analyze_results(targets, predictions, probabilities, config.classes, config, model_name)\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            # Visualize predictions\n",
    "            visualize_predictions(model, test_dataset, config, device, model_name)\n",
    "            \n",
    "            # Generate GradCAM visualizations\n",
    "            visualize_gradcam(model, test_dataset, config, device, model_name)\n",
    "            \n",
    "            # Clear cache between models\n",
    "            clear_gpu_cache()\n",
    "        \n",
    "        # 3. If more than one model evaluated, compare them\n",
    "        if config.compare_with_teachers and len(models) > 1:\n",
    "            compare_models(all_metrics, config)\n",
    "            compare_features(models, config, device)\n",
    "        \n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"Evaluation completed successfully!\")\n",
    "        logger.info(f\"All results saved to '{config.output_dir}' directory\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        logger.error(f\"[ERROR] An error occurred: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        logger.error(\"\\nTry adjusting the batch_size or num_workers in MutualEvalConfig if experiencing memory issues.\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
