{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70417894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 19:37:02,368 [INFO] - Using device: cuda\n",
      "2025-05-01 19:37:02,384 [INFO] - GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "2025-05-01 19:37:02,384 [INFO] - GPU Memory: 6.00 GB\n",
      "2025-05-01 19:37:02,384 [INFO] - CUDA Version: 12.4\n",
      "2025-05-01 19:37:02,384 [INFO] - cuDNN benchmark mode enabled\n",
      "2025-05-01 19:37:02,409 [INFO] - Configuration: {\n",
      "    \"seed\": 42,\n",
      "    \"model_name\": \"mutual_learning\",\n",
      "    \"dataset\": \"CIFAR-10\",\n",
      "    \"use_amp\": true,\n",
      "    \"memory_efficient_attention\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"batch_size\": 4,\n",
      "    \"gradient_accumulation_steps\": 8,\n",
      "    \"gpu_memory_fraction\": 0.75,\n",
      "    \"input_size\": 32,\n",
      "    \"model_input_size\": 224,\n",
      "    \"num_workers\": 4,\n",
      "    \"val_split\": 0.1,\n",
      "    \"dataset_path\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Dataset\",\n",
      "    \"clear_cache_every_n_batches\": 50,\n",
      "    \"pretrained\": true,\n",
      "    \"num_classes\": 10,\n",
      "    \"models\": [\n",
      "        \"vit\",\n",
      "        \"efficientnet\",\n",
      "        \"inception\",\n",
      "        \"mobilenet\",\n",
      "        \"resnet\",\n",
      "        \"densenet\",\n",
      "        \"student\"\n",
      "    ],\n",
      "    \"model_input_sizes\": {\n",
      "        \"vit\": 224,\n",
      "        \"efficientnet\": 224,\n",
      "        \"inception\": 299,\n",
      "        \"mobilenet\": 224,\n",
      "        \"resnet\": 224,\n",
      "        \"densenet\": 224,\n",
      "        \"student\": 224\n",
      "    },\n",
      "    \"model_batch_sizes\": {\n",
      "        \"vit\": 4,\n",
      "        \"efficientnet\": 4,\n",
      "        \"inception\": 4,\n",
      "        \"mobilenet\": 8,\n",
      "        \"resnet\": 4,\n",
      "        \"densenet\": 4,\n",
      "        \"student\": 4\n",
      "    },\n",
      "    \"model_grad_accum\": {\n",
      "        \"vit\": 96,\n",
      "        \"efficientnet\": 64,\n",
      "        \"inception\": 96,\n",
      "        \"mobilenet\": 32,\n",
      "        \"resnet\": 64,\n",
      "        \"densenet\": 64,\n",
      "        \"student\": 64\n",
      "    },\n",
      "    \"pretrained_model_paths\": {\n",
      "        \"vit\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\ViT\\\\checkpoints\\\\vit_b16_teacher_20250321_053628_best.pth\",\n",
      "        \"efficientnet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\EfficientNetB0\\\\checkpoints\\\\efficientnet_b0_teacher_20250325_132652_best.pth\",\n",
      "        \"inception\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\InceptionV3\\\\checkpoints\\\\inception_v3_teacher_20250321_153825_best.pth\",\n",
      "        \"mobilenet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MobileNetV3\\\\checkpoints\\\\mobilenetv3_20250326_035725_best.pth\",\n",
      "        \"resnet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\ResNet50\\\\checkpoints\\\\resnet50_teacher_20250322_225032_best.pth\",\n",
      "        \"densenet\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\DenseNet121\\\\checkpoints\\\\densenet121_teacher_20250325_160534_best.pth\"\n",
      "    },\n",
      "    \"use_pretrained_models\": false,\n",
      "    \"mixed_precision_dtype\": \"float16\",\n",
      "    \"use_model_specific_batch_size\": true,\n",
      "    \"use_model_specific_transforms\": true,\n",
      "    \"enable_memory_efficient_validation\": true,\n",
      "    \"enable_calibration_early_stopping\": true,\n",
      "    \"inference_batch_mult\": 2.0,\n",
      "    \"initialization_epochs\": 5,\n",
      "    \"mutual_learning_epochs\": 50,\n",
      "    \"soft_target_temp\": 4.0,\n",
      "    \"learn_temperatures\": true,\n",
      "    \"model_temperatures\": {\n",
      "        \"densenet\": 4.0,\n",
      "        \"efficientnet\": 4.0,\n",
      "        \"inception\": 5.0,\n",
      "        \"mobilenet\": 4.0,\n",
      "        \"resnet\": 4.0,\n",
      "        \"vit\": 4.0,\n",
      "        \"student\": 3.0\n",
      "    },\n",
      "    \"lr\": 0.001,\n",
      "    \"weight_decay\": 1e-05,\n",
      "    \"early_stop_patience\": 10,\n",
      "    \"calibration_patience\": 5,\n",
      "    \"max_ece_threshold\": 0.05,\n",
      "    \"model_specific_lr\": {\n",
      "        \"vit\": 0.0003,\n",
      "        \"efficientnet\": 0.001,\n",
      "        \"inception\": 0.0007,\n",
      "        \"mobilenet\": 0.0015,\n",
      "        \"resnet\": 0.001,\n",
      "        \"densenet\": 0.0008,\n",
      "        \"student\": 0.002\n",
      "    },\n",
      "    \"use_warmup\": true,\n",
      "    \"warmup_epochs\": 3,\n",
      "    \"alpha\": 0.6,\n",
      "    \"feature_loss_weight\": 0.1,\n",
      "    \"cal_weight\": 0.2,\n",
      "    \"use_curriculum\": true,\n",
      "    \"curriculum_ramp_epochs\": 30,\n",
      "    \"checkpoint_dir\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MutualLearning\\\\checkpoints\",\n",
      "    \"results_dir\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Results\\\\MutualLearning\",\n",
      "    \"export_dir\": \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\\\\Models\\\\MutualLearning\\\\exports\",\n",
      "    \"per_model_calibration\": true\n",
      "}\n",
      "2025-05-01 19:37:02,435 [INFO] - Random seed set to 42\n",
      "2025-05-01 19:37:02,436 [INFO] - Creating models...\n",
      "2025-05-01 19:37:02,437 [INFO] - Loading ViT-B16 model...\n",
      "2025-05-01 19:37:03,061 [INFO] - Loading EfficientNetB0 model...\n",
      "2025-05-01 19:37:03,170 [INFO] - Loading InceptionV3 model with safe wrapper for small inputs...\n",
      "2025-05-01 19:37:03,346 [INFO] - Adapting InceptionV3 AuxLogits layer...\n",
      "2025-05-01 19:37:03,346 [INFO] - Loading MobileNetV3-Large model...\n",
      "2025-05-01 19:37:03,432 [INFO] - Loading ResNet50 model...\n",
      "2025-05-01 19:37:03,697 [INFO] - Loading DenseNet121 model...\n",
      "2025-05-01 19:37:03,970 [INFO] - Creating student model (EfficientNetB0-based)...\n",
      "2025-05-01 19:37:04,211 [INFO] - Model vit moved to cuda\n",
      "2025-05-01 19:37:04,231 [INFO] - Model efficientnet moved to cuda\n",
      "2025-05-01 19:37:04,269 [INFO] - Model inception moved to cuda\n",
      "2025-05-01 19:37:04,298 [INFO] - Model mobilenet moved to cuda\n",
      "2025-05-01 19:37:04,329 [INFO] - Model resnet moved to cuda\n",
      "2025-05-01 19:37:04,366 [INFO] - Model densenet moved to cuda\n",
      "2025-05-01 19:37:04,373 [INFO] - Model student moved to cuda\n",
      "2025-05-01 19:37:04,388 [INFO] - Model vit: 85.81M parameters (85.81M trainable)\n",
      "2025-05-01 19:37:04,390 [INFO] - Model efficientnet: 4.02M parameters (4.02M trainable)\n",
      "2025-05-01 19:37:04,392 [INFO] - Model inception: 24.37M parameters (24.37M trainable)\n",
      "2025-05-01 19:37:04,395 [INFO] - Model mobilenet: 4.21M parameters (4.21M trainable)\n",
      "2025-05-01 19:37:04,396 [INFO] - Model resnet: 23.53M parameters (23.53M trainable)\n",
      "2025-05-01 19:37:04,398 [INFO] - Model densenet: 6.96M parameters (6.96M trainable)\n",
      "2025-05-01 19:37:04,400 [INFO] - Model student: 4.02M parameters (4.02M trainable)\n",
      "2025-05-01 19:37:04,401 [INFO] - Found latest checkpoint: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\init_checkpoint_epoch_5.pth (Epoch 5, Phase: init)\n",
      "2025-05-01 19:37:04,401 [INFO] - Attempting to load checkpoint: C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\init_checkpoint_epoch_5.pth\n",
      "C:\\Users\\Gading\\AppData\\Local\\Temp\\ipykernel_2920\\2687043568.py:419: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
      "2025-05-01 19:37:06,151 [INFO] - Loaded model state for vit successfully.\n",
      "2025-05-01 19:37:06,173 [INFO] - Loaded model state for efficientnet successfully.\n",
      "2025-05-01 19:37:06,211 [INFO] - Loaded model state for inception successfully.\n",
      "2025-05-01 19:37:06,227 [INFO] - Loaded model state for mobilenet successfully.\n",
      "2025-05-01 19:37:06,261 [INFO] - Loaded model state for resnet successfully.\n",
      "2025-05-01 19:37:06,295 [INFO] - Loaded model state for densenet successfully.\n",
      "2025-05-01 19:37:06,314 [INFO] - Loaded model state for student successfully.\n",
      "2025-05-01 19:37:06,443 [INFO] - Recreated and loaded optimizer state for vit\n",
      "2025-05-01 19:37:06,479 [INFO] - Recreated and loaded optimizer state for efficientnet\n",
      "2025-05-01 19:37:06,542 [INFO] - Recreated and loaded optimizer state for inception\n",
      "2025-05-01 19:37:06,571 [INFO] - Recreated and loaded optimizer state for mobilenet\n",
      "2025-05-01 19:37:06,616 [INFO] - Recreated and loaded optimizer state for resnet\n",
      "2025-05-01 19:37:06,671 [INFO] - Recreated and loaded optimizer state for densenet\n",
      "2025-05-01 19:37:06,698 [INFO] - Recreated and loaded optimizer state for student\n",
      "2025-05-01 19:37:06,701 [WARNING] - No scheduler info found for vit in checkpoint.\n",
      "2025-05-01 19:37:06,702 [WARNING] - No scheduler info found for efficientnet in checkpoint.\n",
      "2025-05-01 19:37:06,703 [WARNING] - No scheduler info found for inception in checkpoint.\n",
      "2025-05-01 19:37:06,704 [WARNING] - No scheduler info found for mobilenet in checkpoint.\n",
      "2025-05-01 19:37:06,705 [WARNING] - No scheduler info found for resnet in checkpoint.\n",
      "2025-05-01 19:37:06,706 [WARNING] - No scheduler info found for densenet in checkpoint.\n",
      "2025-05-01 19:37:06,706 [WARNING] - No scheduler info found for student in checkpoint.\n",
      "2025-05-01 19:37:06,707 [INFO] - Checkpoint loaded successfully from C:\\Users\\Gading\\Downloads\\Research\\Models\\MutualLearning\\checkpoints\\init_checkpoint_epoch_5.pth. Checkpoint epoch: 5, Phase: 'init'.\n",
      "2025-05-01 19:37:06,718 [INFO] - Initialization phase completed (loaded epoch 5). Starting mutual learning from epoch 1.\n",
      "2025-05-01 19:37:06,720 [INFO] - Created new scheduler for vit\n",
      "2025-05-01 19:37:06,720 [INFO] - Created new scheduler for efficientnet\n",
      "2025-05-01 19:37:06,720 [INFO] - Created new scheduler for inception\n",
      "2025-05-01 19:37:06,720 [INFO] - Created new scheduler for mobilenet\n",
      "2025-05-01 19:37:06,723 [INFO] - Created new scheduler for resnet\n",
      "2025-05-01 19:37:06,723 [INFO] - Created new scheduler for densenet\n",
      "2025-05-01 19:37:06,725 [INFO] - Created new scheduler for student\n",
      "2025-05-01 19:37:06,726 [INFO] - Preparing data loaders...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 19:37:15,501 [INFO] - Training samples: 45000 per model\n",
      "2025-05-01 19:37:15,501 [INFO] - Validation samples: 5000 per model\n",
      "2025-05-01 19:37:15,501 [INFO] - Test samples: 10000 per model\n",
      "2025-05-01 19:37:15,501 [INFO] - Skipping initialization phase.\n",
      "2025-05-01 19:37:15,501 [INFO] - Clearing GPU cache before mutual learning phase...\n",
      "2025-05-01 19:37:15,675 [INFO] - GPU Memory: Current=1753.94MB, Peak=1753.94MB, Reserved=1764.00MB\n",
      "2025-05-01 19:37:15,675 [INFO] - Starting mutual learning phase from epoch 1...\n",
      "2025-05-01 19:37:15,675 [INFO] - Starting mutual learning phase...\n",
      "2025-05-01 19:37:15,675 [INFO] - Hook registered for encoder.ln\n",
      "2025-05-01 19:37:15,675 [INFO] - Feature extractor registered for vit at layer encoder.ln\n",
      "2025-05-01 19:37:16,462 [INFO] - Feature shape for vit: torch.Size([1, 197, 768])\n",
      "2025-05-01 19:37:16,462 [INFO] - Hook registered for features.8\n",
      "2025-05-01 19:37:16,462 [INFO] - Feature extractor registered for efficientnet at layer features.8\n",
      "2025-05-01 19:37:16,943 [INFO] - Feature shape for efficientnet: torch.Size([1, 1280, 7, 7])\n",
      "2025-05-01 19:37:16,943 [INFO] - Hook registered for inception.Mixed_7c\n",
      "2025-05-01 19:37:16,943 [INFO] - Feature extractor registered for inception at layer Mixed_7c\n",
      "2025-05-01 19:37:17,454 [INFO] - Feature shape for inception: torch.Size([1, 2048, 5, 5])\n",
      "2025-05-01 19:37:17,455 [INFO] - Hook registered for features\n",
      "2025-05-01 19:37:17,456 [INFO] - Feature extractor registered for mobilenet at layer features\n",
      "2025-05-01 19:37:17,602 [INFO] - Feature shape for mobilenet: torch.Size([1, 960, 7, 7])\n",
      "2025-05-01 19:37:17,602 [INFO] - Hook registered for layer4\n",
      "2025-05-01 19:37:17,602 [INFO] - Feature extractor registered for resnet at layer layer4\n",
      "2025-05-01 19:37:17,791 [INFO] - Feature shape for resnet: torch.Size([1, 2048, 7, 7])\n",
      "2025-05-01 19:37:17,791 [INFO] - Hook registered for features\n",
      "2025-05-01 19:37:17,791 [INFO] - Feature extractor registered for densenet at layer features\n",
      "2025-05-01 19:37:18,146 [INFO] - Feature shape for densenet: torch.Size([1, 1024, 7, 7])\n",
      "2025-05-01 19:37:18,146 [INFO] - Hook registered for features.8\n",
      "2025-05-01 19:37:18,146 [INFO] - Feature extractor registered for student at layer features.8\n",
      "2025-05-01 19:37:18,165 [INFO] - Feature shape for student: torch.Size([1, 1280, 7, 7])\n",
      "2025-05-01 19:37:19,337 [INFO] - HFI Initializing. Target student feature shape (excluding batch): torch.Size([1280, 7, 7])\n",
      "2025-05-01 19:37:19,337 [INFO] -   Teacher vit: Original feature shape torch.Size([197, 768]) (3D)\n",
      "2025-05-01 19:37:19,337 [INFO] -     vit (3D->4D): Using Linear projection 768 -> 1280\n",
      "2025-05-01 19:37:19,337 [INFO] -   Teacher efficientnet: Original feature shape torch.Size([1280, 7, 7]) (4D)\n",
      "2025-05-01 19:37:19,347 [INFO] -     efficientnet (4D->4D): Using Conv2d projection 1280 -> 1280\n",
      "2025-05-01 19:37:19,347 [INFO] -   Teacher inception: Original feature shape torch.Size([2048, 8, 8]) (4D)\n",
      "2025-05-01 19:37:19,361 [INFO] -     inception (4D->4D): Using Conv2d projection 2048 -> 1280\n",
      "2025-05-01 19:37:19,361 [INFO] -   Teacher mobilenet: Original feature shape torch.Size([960, 7, 7]) (4D)\n",
      "2025-05-01 19:37:19,369 [INFO] -     mobilenet (4D->4D): Using Conv2d projection 960 -> 1280\n",
      "2025-05-01 19:37:19,370 [INFO] -   Teacher resnet: Original feature shape torch.Size([2048, 7, 7]) (4D)\n",
      "2025-05-01 19:37:19,379 [INFO] -     resnet (4D->4D): Using Conv2d projection 2048 -> 1280\n",
      "2025-05-01 19:37:19,380 [INFO] -   Teacher densenet: Original feature shape torch.Size([1024, 7, 7]) (4D)\n",
      "2025-05-01 19:37:19,386 [INFO] -     densenet (4D->4D): Using Conv2d projection 1024 -> 1280\n",
      "2025-05-01 19:37:19,387 [INFO] - HFI module initialized with 6 teachers.\n",
      "2025-05-01 19:37:19,491 [INFO] - HFI initialized.\n",
      "2025-05-01 19:37:19,492 [INFO] - Added HFI params to student optimizer.\n",
      "2025-05-01 19:37:20,986 [INFO] - Using LR warmup for 3 epochs for model vit.\n",
      "2025-05-01 19:37:20,986 [INFO] - Using LR warmup for 3 epochs for model efficientnet.\n",
      "2025-05-01 19:37:20,986 [INFO] - Using LR warmup for 3 epochs for model inception.\n",
      "2025-05-01 19:37:20,996 [INFO] - Using LR warmup for 3 epochs for model mobilenet.\n",
      "2025-05-01 19:37:20,996 [INFO] - Using LR warmup for 3 epochs for model resnet.\n",
      "2025-05-01 19:37:21,002 [INFO] - Using LR warmup for 3 epochs for model densenet.\n",
      "2025-05-01 19:37:21,006 [INFO] - Using LR warmup for 3 epochs for model student.\n",
      "2025-05-01 19:37:21,007 [INFO] - Added HFI parameters to the student optimizer.\n",
      "2025-05-01 19:37:21,007 [INFO] - Mutual learning phase using effective batch size: 4\n",
      "2025-05-01 19:37:21,009 [INFO] - --- Starting Mutual Learning Epoch 1/50 ---\n",
      "Mutual Epoch 1:   0%|          | 7/11250 [00:27<4:06:17,  1.31s/it, Acc=85.71%, BatchTime=0.38s, Loss=0.3894]  2025-05-01 19:37:48,653 [INFO] - GPU Memory: Current=1893.48MB, Peak=2498.90MB, Reserved=2674.00MB\n",
      "                                                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3400\u001b[0m\n\u001b[0;32m   3397\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n\u001b[0;32m   3399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 3400\u001b[0m     main()\n\u001b[0;32m   3402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3403\u001b[0m \u001b[38;5;124;03mMutual Learning Approach Explanation:\u001b[39;00m\n\u001b[0;32m   3404\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3441\u001b[0m \n\u001b[0;32m   3442\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 3366\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3364\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting mutual learning phase from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch_mutual\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3365\u001b[0m  \u001b[38;5;66;03m# Pass potentially updated optimizers and schedulers to the mutual phase\u001b[39;00m\n\u001b[1;32m-> 3366\u001b[0m models, history \u001b[38;5;241m=\u001b[39m mutual_learning_phase(models, train_loader, val_loader, config, optimizers, schedulers, start_epoch\u001b[38;5;241m=\u001b[39mstart_epoch_mutual)\n\u001b[0;32m   3367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m history: plot_mutual_learning_history(history, config)\n\u001b[0;32m   3368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistory object not found, skipping plotting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 2571\u001b[0m, in \u001b[0;36mmutual_learning_phase\u001b[1;34m(models, train_loader, val_loader, config, optimizers, schedulers, start_epoch)\u001b[0m\n\u001b[0;32m   2569\u001b[0m     effective_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m grad_accum_steps_mutual\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;66;03m# Perform backward pass with scaler\u001b[39;00m\n\u001b[1;32m-> 2571\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(effective_loss)\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Gradients accumulate here\u001b[39;00m\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# --- 5. Optimizer Step ---\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m global_grad_accum_steps \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mutual Learning Training Script for Six Teacher Models and Student on CIFAR-10\n",
    "- Models: ViT-B16, EfficientNetB0, InceptionV3, MobileNetV3, ResNet50, DenseNet121, Student (EfficientNetB0-scaled)\n",
    "- All models are trained concurrently with knowledge exchange and calibration awareness\n",
    "\n",
    "Part of the research: \n",
    "\"Comparative Analysis of Ensemble Distillation and Mutual Learning: \n",
    "A Unified Framework for Uncertainty-Calibrated Vision Systems\"\n",
    "\n",
    "Target Hardware: RTX 3060 Laptop (6GB VRAM)\n",
    "Optimizations: AMP, gradient accumulation, memory-efficient techniques, GPU cache clearing\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR # Added for warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision.models import (\n",
    "    vit_b_16, ViT_B_16_Weights,\n",
    "    efficientnet_b0, EfficientNet_B0_Weights,\n",
    "    inception_v3, Inception_V3_Weights,\n",
    "    mobilenet_v3_large, MobileNet_V3_Large_Weights,\n",
    "    resnet50, ResNet50_Weights,\n",
    "    densenet121, DenseNet121_Weights\n",
    ")\n",
    "from datetime import datetime\n",
    "import gc  # For explicit garbage collection\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import traceback\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "\n",
    "# Define base paths\n",
    "BASE_PATH = \"C:\\\\Users\\\\Gading\\\\Downloads\\\\Research\"\n",
    "DATASET_PATH = os.path.join(BASE_PATH, \"Dataset\")\n",
    "RESULTS_PATH = os.path.join(BASE_PATH, \"Results\")\n",
    "MODELS_PATH = os.path.join(BASE_PATH, \"Models\")\n",
    "SCRIPTS_PATH = os.path.join(BASE_PATH, \"Scripts\")\n",
    "\n",
    "# Create model-specific paths\n",
    "MODEL_NAME = \"MutualLearning\"\n",
    "MODEL_RESULTS_PATH = os.path.join(RESULTS_PATH, MODEL_NAME)\n",
    "MODEL_CHECKPOINT_PATH = os.path.join(MODELS_PATH, MODEL_NAME, \"checkpoints\")\n",
    "MODEL_EXPORT_PATH = os.path.join(MODELS_PATH, MODEL_NAME, \"exports\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_CHECKPOINT_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_EXPORT_PATH, exist_ok=True)\n",
    "os.makedirs(SCRIPTS_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_RESULTS_PATH, \"logs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_RESULTS_PATH, \"plots\"), exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "log_file = os.path.join(MODEL_RESULTS_PATH, \"logs\", \"mutual_learning.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Set up tensorboard writer\n",
    "writer = SummaryWriter(log_dir=os.path.join(MODEL_RESULTS_PATH, \"logs\"))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    logger.info(\"cuDNN benchmark mode enabled\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Slightly faster with False\n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # General settings\n",
    "        self.seed = 42\n",
    "        self.model_name = \"mutual_learning\"\n",
    "        self.dataset = \"CIFAR-10\"\n",
    "        \n",
    "        # Hardware-specific optimizations - FIXED VALUES for RTX 3060 Laptop (6GB)\n",
    "        self.use_amp = True  # Automatic Mixed Precision\n",
    "        self.memory_efficient_attention = True  # Memory-efficient attention\n",
    "        self.prefetch_factor = 2  # DataLoader prefetch factor\n",
    "        self.pin_memory = True  # Pin memory for faster CPU->GPU transfers\n",
    "        self.persistent_workers = True  # Keep workers alive between epochs\n",
    "        \n",
    "        # RTX 3060 Laptop specific fixes\n",
    "        self.batch_size = 4  # Even more conservative than distillation for multiple models\n",
    "        self.gradient_accumulation_steps = 8  # Accumulate for effective batch of 384\n",
    "        self.gpu_memory_fraction = 0.75  # More conservative memory usage\n",
    "        \n",
    "        # Data settings\n",
    "        self.input_size = 32  # Original CIFAR-10 image size\n",
    "        self.model_input_size = 224  # Required size for pretrained models\n",
    "        self.num_workers = 4  # For data loading\n",
    "        self.val_split = 0.1  # 10% validation split\n",
    "        self.dataset_path = DATASET_PATH\n",
    "        \n",
    "        self.clear_cache_every_n_batches = 50  # Clear cache more frequently (was 100)\n",
    "        \n",
    "        # Model settings\n",
    "        self.pretrained = True  # Use pretrained models\n",
    "        self.num_classes = 10  # CIFAR-10 has 10 classes\n",
    "        \n",
    "        # Models for mutual learning\n",
    "        self.models = ['vit', 'efficientnet', 'inception', 'mobilenet', 'resnet', 'densenet', 'student']\n",
    "        \n",
    "        # Model-specific input sizes (model_name: input_size)\n",
    "        self.model_input_sizes = {\n",
    "            'vit': 224,\n",
    "            'efficientnet': 224,\n",
    "            'inception': 299,  # InceptionV3 requires 299x299 input\n",
    "            'mobilenet': 224,\n",
    "            'resnet': 224,\n",
    "            'densenet': 224,\n",
    "            'student': 224\n",
    "        }\n",
    "        \n",
    "        # Model-specific batch sizes for memory optimization\n",
    "        self.model_batch_sizes = {\n",
    "            'vit': 4,\n",
    "            'efficientnet': 4, # Reduced\n",
    "            'inception': 4,\n",
    "            'mobilenet': 8,    # Reduced\n",
    "            'resnet': 4,       # Reduced\n",
    "            'densenet': 4,     # Reduced\n",
    "            'student': 4       # Reduced - This will likely be the shared loader batch size\n",
    "        }\n",
    "        \n",
    "        # Model-specific gradient accumulation steps\n",
    "        self.model_grad_accum = {\n",
    "            'vit': 96,\n",
    "            'efficientnet': 64,\n",
    "            'inception': 96,\n",
    "            'mobilenet': 32,\n",
    "            'resnet': 64,\n",
    "            'densenet': 64,\n",
    "            'student': 64\n",
    "        }\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        \n",
    "        # Pre-trained teacher model paths (optional starting points)\n",
    "        self.pretrained_model_paths = {\n",
    "            'vit': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\ViT\\checkpoints\\vit_b16_teacher_20250321_053628_best.pth\",\n",
    "            'efficientnet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\EfficientNetB0\\checkpoints\\efficientnet_b0_teacher_20250325_132652_best.pth\",\n",
    "            'inception': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\InceptionV3\\checkpoints\\inception_v3_teacher_20250321_153825_best.pth\",\n",
    "            'mobilenet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\MobileNetV3\\checkpoints\\mobilenetv3_20250326_035725_best.pth\",\n",
    "            'resnet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\ResNet50\\checkpoints\\resnet50_teacher_20250322_225032_best.pth\",\n",
    "            'densenet': r\"C:\\Users\\Gading\\Downloads\\Research\\Models\\DenseNet121\\checkpoints\\densenet121_teacher_20250325_160534_best.pth\"\n",
    "        }\n",
    "        self.use_pretrained_models = False  # Start from ImageNet instead of fine-tuned models\n",
    "        \n",
    "        # Optimization settings\n",
    "        self.mixed_precision_dtype = 'float16'  # Using float16 for faster training\n",
    "        self.use_model_specific_batch_size = True  # Use model-specific batch sizes\n",
    "        self.use_model_specific_transforms = True  # Use model-specific transforms\n",
    "        self.enable_memory_efficient_validation = True  # Use memory-efficient validation\n",
    "        self.enable_calibration_early_stopping = True  # Early stopping based on calibration metrics\n",
    "        self.inference_batch_mult = 2.0  # Larger batch sizes during inference vs training\n",
    "        \n",
    "        # Training phases\n",
    "        self.initialization_epochs = 5  # Epochs for individual initialization\n",
    "        self.mutual_learning_epochs = 50  # Epochs for mutual learning phase\n",
    "        \n",
    "        # Temperature settings\n",
    "        self.soft_target_temp = 4.0  # Temperature for soft targets in KL divergence\n",
    "        self.learn_temperatures = True  # Whether to learn temperatures during training\n",
    "        \n",
    "        # Initial temperatures for each model\n",
    "        self.model_temperatures = {\n",
    "            'densenet': 4.0,\n",
    "            'efficientnet': 4.0,\n",
    "            'inception': 5.0,  # Higher temperature for less confident predictions\n",
    "            'mobilenet': 4.0,\n",
    "            'resnet': 4.0,\n",
    "            'vit': 4.0,\n",
    "            'student': 3.0   # Student starts with lower temperature\n",
    "        }\n",
    "        \n",
    "        # Training settings\n",
    "        self.lr = 1e-3  # Learning rate\n",
    "        self.weight_decay = 1e-5  # Weight decay\n",
    "        self.early_stop_patience = 10  # Early stopping patience\n",
    "        self.calibration_patience = 5  # Early stopping based on calibration\n",
    "        self.max_ece_threshold = 0.05  # Maximum acceptable ECE for early stopping\n",
    "        \n",
    "        # Learning rate settings\n",
    "        self.model_specific_lr = {\n",
    "            'vit': 3e-4,         # ViT needs lower learning rate\n",
    "            'efficientnet': 1e-3,\n",
    "            'inception': 7e-4,   # Slightly lower for inception\n",
    "            'mobilenet': 1.5e-3, # MobileNet can use higher learning rate\n",
    "            'resnet': 1e-3,\n",
    "            'densenet': 8e-4,\n",
    "            'student': 2e-3      # Higher for student to learn faster\n",
    "        }\n",
    "        self.use_warmup = True   # Use learning rate warmup\n",
    "        self.warmup_epochs = 3   # Number of warmup epochs\n",
    "        \n",
    "        # Loss weights\n",
    "        self.alpha = 0.6  # Weight of mutual learning loss vs hard-label loss\n",
    "        self.feature_loss_weight = 0.1  # Feature loss weight\n",
    "        self.cal_weight = 0.2  # Maximum calibration weight\n",
    "        \n",
    "        # Curriculum scheduling settings\n",
    "        self.use_curriculum = True  # Whether to use curriculum scheduling\n",
    "        self.curriculum_ramp_epochs = 30  # Epochs for ramping up calibration weight\n",
    "        \n",
    "        # Output settings\n",
    "        self.checkpoint_dir = MODEL_CHECKPOINT_PATH\n",
    "        self.results_dir = MODEL_RESULTS_PATH\n",
    "        self.export_dir = MODEL_EXPORT_PATH\n",
    "        \n",
    "        # Enhanced calibration settings\n",
    "        self.per_model_calibration = True  # Use per-model calibration loss\n",
    "        \n",
    "        # --- ADDED: Helper to get the specific batch size for the shared mutual loader ---\n",
    "    def get_mutual_learning_batch_size(self):\n",
    "        # Prioritize student's batch size if model-specific is enabled\n",
    "        if self.use_model_specific_batch_size and 'student' in self.model_batch_sizes:\n",
    "            return self.model_batch_sizes['student']\n",
    "        # Otherwise, use the global (now very small) batch size\n",
    "        return self.batch_size\n",
    "    # --- END ADDED ---\n",
    "\n",
    "    # Ensure get_grad_accum_steps uses the correct value for mutual phase\n",
    "    def get_grad_accum_steps(self, model_name, phase='mutual'):\n",
    "         if phase == 'init' and model_name in self.model_grad_accum:\n",
    "             # Use model-specific for init phase if defined\n",
    "             return self.model_grad_accum[model_name]\n",
    "         # For mutual phase or fallback, use the global reduced value\n",
    "         return self.gradient_accumulation_steps\n",
    "        \n",
    "    def __str__(self):\n",
    "        return json.dumps(self.__dict__, indent=4)\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, cls=NumpyEncoder, indent=4) # Use encoder if needed\n",
    "\n",
    "    def get_calibration_weight(self, epoch):\n",
    "        if not self.use_curriculum: return self.cal_weight\n",
    "        if epoch < self.curriculum_ramp_epochs: return self.cal_weight * (epoch + 1) / self.curriculum_ramp_epochs\n",
    "        else: return self.cal_weight\n",
    "\n",
    "    def get_mutual_weight(self, epoch):\n",
    "        if not self.use_curriculum: return self.alpha\n",
    "        if epoch < self.curriculum_ramp_epochs: return self.alpha * (epoch + 1) / self.curriculum_ramp_epochs\n",
    "        else: return self.alpha\n",
    "\n",
    "    def get_batch_size(self, model_name):\n",
    "        # Use the severely reduced batch sizes defined above\n",
    "        if self.use_model_specific_batch_size and model_name in self.model_batch_sizes:\n",
    "            return self.model_batch_sizes[model_name]\n",
    "        return self.batch_size # Fallback to global (also reduced)\n",
    "\n",
    "    def get_input_size(self, model_name):\n",
    "        if model_name in self.model_input_sizes: return self.model_input_sizes[model_name]\n",
    "        return self.model_input_size\n",
    "\n",
    "    def get_grad_accum_steps(self, model_name):\n",
    "        # Use the significantly increased accumulation steps defined above\n",
    "        if model_name in self.model_grad_accum:\n",
    "            return self.model_grad_accum[model_name]\n",
    "        # Fallback to the increased global accumulation step count\n",
    "        return self.gradient_accumulation_steps\n",
    "\n",
    "    def get_learning_rate(self, model_name):\n",
    "        if self.model_specific_lr and model_name in self.model_specific_lr:\n",
    "            return self.model_specific_lr[model_name]\n",
    "        return self.lr\n",
    "\n",
    "    def get_feature_weight(self, epoch):\n",
    "        if not self.use_curriculum: return self.feature_loss_weight\n",
    "        if epoch < self.curriculum_ramp_epochs: return self.feature_loss_weight * (epoch + 1) / self.curriculum_ramp_epochs\n",
    "        else: return self.feature_loss_weight\n",
    "\n",
    "# Helper class for JSON serialization of numpy arrays (if needed for saving config/history)\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        # Add handling for torch tensors if they appear in config/history\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "             return obj.tolist() # Or handle based on tensor type/device\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# Memory utilities\n",
    "def print_gpu_memory_stats():\n",
    "    \"\"\"Print GPU memory usage statistics\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        current_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        max_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        reserved_mem = torch.cuda.memory_reserved() / 1024**2\n",
    "        logger.info(f\"GPU Memory: Current={current_mem:.2f}MB, Peak={max_mem:.2f}MB, Reserved={reserved_mem:.2f}MB\")\n",
    "\n",
    "def clear_gpu_cache(threshold_mb=0.1):\n",
    "    \"\"\"\n",
    "    Clear GPU cache to free up memory and log only if significant memory is freed.\n",
    "\n",
    "    Args:\n",
    "        threshold_mb (float): Minimum MB freed to trigger logging. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Ensure operations are synchronized before checking memory\n",
    "        torch.cuda.synchronize()\n",
    "        before_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        \n",
    "        # Clear cache and collect garbage\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Synchronize again after clearing\n",
    "        torch.cuda.synchronize()\n",
    "        after_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        \n",
    "        freed_mem = before_mem - after_mem\n",
    "        \n",
    "        # Only log if a significant amount of memory was freed\n",
    "        if freed_mem > threshold_mb:\n",
    "            logger.info(f\"GPU cache cleared: {before_mem:.2f}MB â†’ {after_mem:.2f}MB (freed {freed_mem:.2f}MB)\")\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Finds the latest checkpoint file based on epoch number and phase.\"\"\"\n",
    "    init_checkpoints = glob.glob(os.path.join(checkpoint_dir, \"init_checkpoint_epoch_*.pth\"))\n",
    "    mutual_checkpoints = glob.glob(os.path.join(checkpoint_dir, \"mutual_checkpoint_epoch_*.pth\"))\n",
    "\n",
    "    latest_checkpoint = None\n",
    "    latest_epoch = -1\n",
    "    phase = 'none'\n",
    "\n",
    "    # Check initialization checkpoints\n",
    "    for cp in init_checkpoints:\n",
    "        match = re.search(r\"init_checkpoint_epoch_(\\d+)\\.pth\", os.path.basename(cp))\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            # Prioritize init phase only if no mutual phase checkpoints exist\n",
    "            if not mutual_checkpoints and epoch > latest_epoch:\n",
    "                latest_epoch = epoch\n",
    "                latest_checkpoint = cp\n",
    "                phase = 'init'\n",
    "\n",
    "    # Check mutual learning checkpoints (these take precedence if they exist)\n",
    "    for cp in mutual_checkpoints:\n",
    "        match = re.search(r\"mutual_checkpoint_epoch_(\\d+)\\.pth\", os.path.basename(cp))\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            if epoch > latest_epoch:\n",
    "                latest_epoch = epoch\n",
    "                latest_checkpoint = cp\n",
    "                phase = 'mutual' # Mutual phase overrides init phase\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        logger.info(f\"Found latest checkpoint: {latest_checkpoint} (Epoch {latest_epoch}, Phase: {phase})\")\n",
    "    else:\n",
    "        logger.info(\"No checkpoint found.\")\n",
    "\n",
    "    return latest_checkpoint, latest_epoch, phase\n",
    "\n",
    "def load_checkpoint(checkpoint_path, models, config):\n",
    "    \"\"\"\n",
    "    Loads state from checkpoint. Recreates optimizers/schedulers based on saved state.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        models (dict): Dictionary of models (already created).\n",
    "        config (Config): The configuration object.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (start_epoch, phase, optimizers, schedulers)\n",
    "               Returns (0, 'none', {}, {}) on failure.\n",
    "    \"\"\"\n",
    "    optimizers = {}\n",
    "    schedulers = {}\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        logger.error(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        return 0, 'none', optimizers, schedulers\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "\n",
    "        phase = 'none'\n",
    "        filename = os.path.basename(checkpoint_path)\n",
    "        if 'init_checkpoint_epoch_' in filename: phase = 'init'\n",
    "        elif 'mutual_checkpoint_epoch_' in filename: phase = 'mutual'\n",
    "\n",
    "        # Load model states (remains the same)\n",
    "        model_state_dict = checkpoint.get('model_state_dict', {})\n",
    "        for name, model in models.items():\n",
    "            if name in model_state_dict:\n",
    "                try:\n",
    "                    missing_keys, unexpected_keys = model.load_state_dict(model_state_dict[name], strict=False)\n",
    "                    # (Logging for missing/unexpected keys as before) ...\n",
    "                    if not missing_keys and not unexpected_keys:\n",
    "                        logger.info(f\"Loaded model state for {name} successfully.\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Loaded model state for {name} with issues.\")\n",
    "                        if missing_keys: logger.warning(f\"  Missing keys: {missing_keys}\")\n",
    "                        if unexpected_keys: logger.warning(f\"  Unexpected keys: {unexpected_keys}\")\n",
    "                except Exception as e: logger.error(f\"Error loading model state for {name}: {e}\\n{traceback.format_exc()}\")\n",
    "            else: logger.warning(f\"No state dict found for model {name} in checkpoint.\")\n",
    "\n",
    "        # --- MODIFIED: Recreate Optimizers and Load State ---\n",
    "        optimizer_state_dict = checkpoint.get('optimizer_state_dict', {})\n",
    "        for name, model in models.items():\n",
    "            lr = config.get_learning_rate(name) # Get LR from config\n",
    "            # Recreate the optimizer instance\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=config.weight_decay)\n",
    "            state_dict_key = 'hfi' if name == 'hfi' and 'hfi' in optimizer_state_dict else name\n",
    "            if state_dict_key in optimizer_state_dict:\n",
    "                try:\n",
    "                    optimizer.load_state_dict(optimizer_state_dict[state_dict_key])\n",
    "                    logger.info(f\"Recreated and loaded optimizer state for {name}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading optimizer state for {name}: {e}. Optimizer state might be reset.\\n{traceback.format_exc()}\")\n",
    "            else:\n",
    "                logger.warning(f\"No optimizer state dict found for {name} in checkpoint. Optimizer initialized.\")\n",
    "            optimizers[name] = optimizer # Store the potentially loaded optimizer\n",
    "\n",
    "        # --- MODIFIED: Recreate Schedulers and Load State ---\n",
    "        scheduler_info = checkpoint.get('scheduler_info', {})\n",
    "        for name, opt in optimizers.items(): # Iterate through created optimizers\n",
    "            sched_info = scheduler_info.get(name)\n",
    "            scheduler = None # Default to None\n",
    "            if sched_info:\n",
    "                sched_type_name = sched_info.get('type')\n",
    "                sched_state = sched_info.get('state_dict')\n",
    "                try:\n",
    "                    # Recreate scheduler based on type saved in checkpoint\n",
    "                    # Get current LR - needed for some schedulers\n",
    "                    current_lr = opt.param_groups[0]['lr']\n",
    "                    if sched_type_name == 'StepLR':\n",
    "                        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.95) # Match init phase scheduler example\n",
    "                    elif sched_type_name == 'SequentialLR':\n",
    "                         # Recreate the structure used in main/mutual phase\n",
    "                         cosine_t_max = max(1, config.mutual_learning_epochs - config.warmup_epochs)\n",
    "                         base_scheduler = CosineAnnealingLR(opt, T_max=cosine_t_max, eta_min=current_lr * 0.01) # Use current_lr\n",
    "                         if config.use_warmup and config.warmup_epochs > 0:\n",
    "                              warmup_total_iters = max(1, config.warmup_epochs)\n",
    "                              warmup_scheduler = LinearLR(opt, start_factor=1e-3, total_iters=warmup_total_iters)\n",
    "                              milestone_epoch = max(1, config.warmup_epochs)\n",
    "                              scheduler = SequentialLR(opt, schedulers=[warmup_scheduler, base_scheduler], milestones=[milestone_epoch])\n",
    "                         else:\n",
    "                              scheduler = base_scheduler # Fallback if no warmup info? Or assume base Cosine?\n",
    "\n",
    "                    elif sched_type_name == 'CosineAnnealingLR':\n",
    "                        # Recreate just the cosine part if that was saved\n",
    "                        cosine_t_max = max(1, config.mutual_learning_epochs - config.warmup_epochs) # Assuming mutual phase length\n",
    "                        scheduler = CosineAnnealingLR(opt, T_max=cosine_t_max, eta_min=current_lr * 0.01)\n",
    "                    # Add other scheduler types if necessary\n",
    "\n",
    "                    if scheduler and sched_state:\n",
    "                        scheduler.load_state_dict(sched_state)\n",
    "                        logger.info(f\"Recreated and loaded scheduler state for {name} (Type: {sched_type_name})\")\n",
    "                    elif scheduler:\n",
    "                         logger.warning(f\"Recreated scheduler for {name} (Type: {sched_type_name}) but no state dict found.\")\n",
    "                    else:\n",
    "                         logger.warning(f\"Unknown or missing scheduler type '{sched_type_name}' for {name}. Scheduler not recreated.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error recreating/loading scheduler state for {name}: {e}. Scheduler might be reset.\\n{traceback.format_exc()}\")\n",
    "                    scheduler = None # Reset on error\n",
    "            else:\n",
    "                logger.warning(f\"No scheduler info found for {name} in checkpoint.\")\n",
    "\n",
    "            schedulers[name] = scheduler # Store the potentially loaded scheduler\n",
    "\n",
    "        # Load random states (remains the same)\n",
    "        # ... (random state loading code) ...\n",
    "        if 'random_state' in checkpoint: random.setstate(checkpoint['random_state'])\n",
    "        if 'np_random_state' in checkpoint: np.random.set_state(checkpoint['np_random_state'])\n",
    "        if 'torch_random_state' in checkpoint: torch.set_rng_state(checkpoint['torch_random_state'])\n",
    "        if torch.cuda.is_available() and 'cuda_random_state' in checkpoint:\n",
    "            try: torch.cuda.set_rng_state_all(checkpoint['cuda_random_state'])\n",
    "            except Exception as e: logger.warning(f\"Could not load CUDA random state: {e}\")\n",
    "\n",
    "        logger.info(f\"Checkpoint loaded successfully from {checkpoint_path}. Checkpoint epoch: {start_epoch}, Phase: '{phase}'.\")\n",
    "        return start_epoch, phase, optimizers, schedulers # Return recreated objects\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load checkpoint from {checkpoint_path}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return 0, 'none', optimizers, schedulers\n",
    "\n",
    "\n",
    "def save_checkpoint(models, optimizers, schedulers, epoch, config, filename=\"checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the training state to a checkpoint file.\n",
    "    Includes random states for better reproducibility. # <-- Added note\n",
    "    \"\"\"\n",
    "    # ... (keep existing directory creation and path joining) ...\n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(config.checkpoint_dir, filename)\n",
    "\n",
    "    scheduler_states = {}\n",
    "    for name, sched in schedulers.items():\n",
    "        if sched is not None:\n",
    "            scheduler_states[name] = {\n",
    "                'type': type(sched).__name__, # Store class name as string\n",
    "                'state_dict': sched.state_dict()\n",
    "            }\n",
    "    \n",
    "    state = {\n",
    "        'epoch': epoch + 1,\n",
    "        'config': config.__dict__,\n",
    "        'model_state_dict': {name: model.state_dict() for name, model in models.items()},\n",
    "        'optimizer_state_dict': {name: opt.state_dict() for name, opt in optimizers.items()},\n",
    "        'scheduler_info': scheduler_states, # Use the new structure\n",
    "        'random_state': random.getstate(),\n",
    "        'np_random_state': np.random.get_state(),\n",
    "        'torch_random_state': torch.get_rng_state(),\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        state['cuda_random_state'] = torch.cuda.get_rng_state_all()\n",
    "\n",
    "    # Handle separate HFI optimizer/scheduler state saving if they exist\n",
    "    if 'hfi' in optimizers and hasattr(optimizers['hfi'], 'state_dict'):\n",
    "         state['optimizer_state_dict']['hfi'] = optimizers['hfi'].state_dict()\n",
    "    # Save HFI scheduler info if it exists separately\n",
    "    if 'hfi' in schedulers and schedulers['hfi'] is not None:\n",
    "         hfi_sched = schedulers['hfi']\n",
    "         state['scheduler_info']['hfi'] = { # Overwrite or add HFI info\n",
    "             'type': type(hfi_sched).__name__,\n",
    "             'state_dict': hfi_sched.state_dict()\n",
    "         }\n",
    "    \n",
    "    try:\n",
    "        temp_checkpoint_path = checkpoint_path + \".tmp\"\n",
    "        torch.save(state, temp_checkpoint_path)\n",
    "        os.replace(temp_checkpoint_path, checkpoint_path) # Atomic rename\n",
    "        logger.info(f\"Checkpoint saved successfully to {checkpoint_path} (Epoch {epoch})\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint to {checkpoint_path}: {e}\")\n",
    "        if os.path.exists(temp_checkpoint_path):\n",
    "            try: os.remove(temp_checkpoint_path)\n",
    "            except OSError: pass\n",
    "\n",
    "\n",
    "# Calibration Metrics\n",
    "class CalibrationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_ece(probs, targets, n_bins=15):\n",
    "        \"\"\"Compute Expected Calibration Error (ECE)\"\"\"\n",
    "        # Get the confidence (max probability) and predictions\n",
    "        confidences, predictions = torch.max(probs, dim=1)\n",
    "        accuracies = (predictions == targets).float()\n",
    "        \n",
    "        # Sort by confidence\n",
    "        sorted_indices = torch.argsort(confidences)\n",
    "        sorted_confidences = confidences[sorted_indices]\n",
    "        sorted_accuracies = accuracies[sorted_indices]\n",
    "        \n",
    "        # Create bins\n",
    "        bin_size = 1.0 / n_bins\n",
    "        bins = torch.linspace(0, 1.0, n_bins+1)\n",
    "        ece = 0.0\n",
    "        \n",
    "        for i in range(n_bins):\n",
    "            # Determine bin boundaries\n",
    "            bin_start = bins[i]\n",
    "            bin_end = bins[i+1]\n",
    "            \n",
    "            # Find samples in bin\n",
    "            in_bin = (sorted_confidences >= bin_start) & (sorted_confidences < bin_end)\n",
    "            bin_count = in_bin.sum()\n",
    "            \n",
    "            if bin_count > 0:\n",
    "                bin_conf = sorted_confidences[in_bin].mean()\n",
    "                bin_acc = sorted_accuracies[in_bin].mean()\n",
    "                # Add weighted absolute difference to ECE\n",
    "                ece += (bin_count / len(confidences)) * torch.abs(bin_acc - bin_conf)\n",
    "        \n",
    "        return ece\n",
    "    \n",
    "    @staticmethod\n",
    "    def calibration_loss(logits, targets):\n",
    "        \"\"\"Compute a loss term that encourages better calibration\"\"\"\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(probs, dim=1)\n",
    "        accuracies = (predictions == targets).float()\n",
    "        \n",
    "        # MSE between confidence and accuracy\n",
    "        return torch.mean((confidences - accuracies) ** 2)\n",
    "\n",
    "# Custom wrapper for InceptionV3 to handle auxiliary outputs safely\n",
    "class InceptionV3Wrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for InceptionV3 that safely handles auxiliary outputs for small inputs.\n",
    "    This prevents the \"Kernel size can't be greater than actual input size\" error.\n",
    "    \"\"\"\n",
    "    def __init__(self, inception_model):\n",
    "        super(InceptionV3Wrapper, self).__init__()\n",
    "        self.inception = inception_model\n",
    "        # Directly access the internal model components we need\n",
    "        self.Conv2d_1a_3x3 = inception_model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = inception_model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = inception_model.Conv2d_2b_3x3\n",
    "        self.maxpool1 = inception_model.maxpool1\n",
    "        self.Conv2d_3b_1x1 = inception_model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = inception_model.Conv2d_4a_3x3\n",
    "        self.maxpool2 = inception_model.maxpool2\n",
    "        self.Mixed_5b = inception_model.Mixed_5b\n",
    "        self.Mixed_5c = inception_model.Mixed_5c\n",
    "        self.Mixed_5d = inception_model.Mixed_5d\n",
    "        self.Mixed_6a = inception_model.Mixed_6a\n",
    "        self.Mixed_6b = inception_model.Mixed_6b\n",
    "        self.Mixed_6c = inception_model.Mixed_6c\n",
    "        self.Mixed_6d = inception_model.Mixed_6d\n",
    "        self.Mixed_6e = inception_model.Mixed_6e\n",
    "        self.Mixed_7a = inception_model.Mixed_7a\n",
    "        self.Mixed_7b = inception_model.Mixed_7b\n",
    "        self.Mixed_7c = inception_model.Mixed_7c\n",
    "        self.avgpool = inception_model.avgpool\n",
    "        self.dropout = inception_model.dropout\n",
    "        self.fc = inception_model.fc\n",
    "        \n",
    "        # Important: mark that this is a wrapper\n",
    "        self.is_wrapper = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get the batch size for reshaping later\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Basic stem\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.Mixed_5b(x)\n",
    "        x = self.Mixed_5c(x)\n",
    "        x = self.Mixed_5d(x)\n",
    "        x = self.Mixed_6a(x)\n",
    "        x = self.Mixed_6b(x)\n",
    "        x = self.Mixed_6c(x)\n",
    "        x = self.Mixed_6d(x)\n",
    "        x = self.Mixed_6e(x)\n",
    "        \n",
    "        # No auxiliary classifier usage - skip those layers that cause issues\n",
    "        \n",
    "        x = self.Mixed_7a(x)\n",
    "        x = self.Mixed_7b(x)\n",
    "        x = self.Mixed_7c(x)\n",
    "        \n",
    "        # Final pooling and prediction\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        # Set both the wrapper and the internal model to the correct mode\n",
    "        result = super(InceptionV3Wrapper, self).train(mode)\n",
    "        self.inception.train(mode)\n",
    "        return result\n",
    "    \n",
    "    def eval(self):\n",
    "        # Set both the wrapper and the internal model to eval mode\n",
    "        result = super(InceptionV3Wrapper, self).eval()\n",
    "        self.inception.eval()\n",
    "        return result\n",
    "    \n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        # Return the state dict of the internal model\n",
    "        return self.inception.state_dict(*args, **kwargs)\n",
    "    \n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        # Load the state dict into the internal model\n",
    "        return self.inception.load_state_dict(state_dict, strict)\n",
    "\n",
    "# Add this improved model state loading utility after the InceptionV3Wrapper class\n",
    "class ModelStateLoader:\n",
    "    \"\"\"\n",
    "    Utility class for robust model state loading with architecture compatibility checks\n",
    "    and state dictionary adaptation capabilities.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_model_type(model):\n",
    "        \"\"\"Determine the type of model architecture\"\"\"\n",
    "        if hasattr(model, 'is_wrapper') and model.is_wrapper:\n",
    "            return \"inception\"\n",
    "        \n",
    "        model_str = str(model.__class__).lower()\n",
    "        \n",
    "        if \"vit\" in model_str:\n",
    "            return \"vit\"\n",
    "        elif \"efficientnet\" in model_str:\n",
    "            return \"efficientnet\"\n",
    "        elif \"mobilenet\" in model_str:\n",
    "            return \"mobilenet\"\n",
    "        elif \"resnet\" in model_str:\n",
    "            return \"resnet\"\n",
    "        elif \"densenet\" in model_str:\n",
    "            return \"densenet\"\n",
    "        else:\n",
    "            # Default\n",
    "            return \"unknown\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_state_dict_type(state_dict):\n",
    "        \"\"\"Analyze state dict structure to determine model architecture type\"\"\"\n",
    "        # Get a sample of keys to check architecture patterns\n",
    "        keys = list(state_dict.keys())[:20]  # Use more keys for better detection\n",
    "        \n",
    "        # Check for architecture patterns\n",
    "        if any(\"encoder\" in k or \"class_token\" in k or \"head\" in k and \"blocks\" in str(keys) for k in keys):\n",
    "            return \"vit\"\n",
    "        elif any(\"features\" in k and \"block\" in k and \"0.0.weight\" not in k for k in keys):\n",
    "            return \"efficientnet\"\n",
    "        elif any(\"Mixed\" in k or \"Conv2d_1a_3x3\" in k for k in keys):  \n",
    "            return \"inception\"\n",
    "        elif any(\"features\" in k and \"1.0.block\" in k for k in keys):\n",
    "            return \"mobilenet\"\n",
    "        elif any(\"layer1\" in k or \"layer2\" in k for k in keys):\n",
    "            return \"resnet\"\n",
    "        elif any(\"features.denseblock\" in k or \"features.norm5\" in k for k in keys):\n",
    "            return \"densenet\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def adapt_state_dict(state_dict, model):\n",
    "        \"\"\"\n",
    "        Attempt to adapt a state dictionary to be compatible with the target model\n",
    "        by mapping keys and handling size mismatches appropriately.\n",
    "        \"\"\"\n",
    "        model_keys = set(model.state_dict().keys())\n",
    "        state_dict_keys = set(state_dict.keys())\n",
    "        \n",
    "        # If all keys match, no adaptation needed\n",
    "        if model_keys == state_dict_keys:\n",
    "            return state_dict\n",
    "        \n",
    "        # Create a new state dict with only compatible keys/values\n",
    "        adapted_dict = {}\n",
    "        \n",
    "        # First, add all direct matches\n",
    "        for key in model_keys & state_dict_keys:\n",
    "            if model.state_dict()[key].shape == state_dict[key].shape:\n",
    "                adapted_dict[key] = state_dict[key]\n",
    "        \n",
    "        # Try to find approximate matches for remaining keys\n",
    "        remaining_model_keys = model_keys - set(adapted_dict.keys())\n",
    "        remaining_state_keys = state_dict_keys - set(adapted_dict.keys())\n",
    "        \n",
    "        # Create a mapping table for common key pattern changes\n",
    "        key_patterns = [\n",
    "            # MobileNetV3 mapping (old â†’ new)\n",
    "            (r'features\\.(\\d+)\\.0\\.block', r'features.\\1.block'),  # Handle nesting differences\n",
    "            (r'features\\.(\\d+)\\.(\\d+)\\.block', r'features.\\1.block'),  # Flatter hierarchy\n",
    "            \n",
    "            # EfficientNet mapping\n",
    "            (r'blocks\\.(\\d+)\\.(\\d+)', r'features.\\1.\\2'),  # Different block naming\n",
    "            \n",
    "            # ViT mapping\n",
    "            (r'encoder\\.layers\\.(\\d+)', r'blocks.\\1'),  # Different encoder naming\n",
    "            (r'mlp\\.fc(\\d+)', r'mlp.linear\\1'),  # MLP layer naming differences\n",
    "            \n",
    "            # ResNet mapping\n",
    "            (r'conv(\\d+)', r'layer\\1'),  # Conv layer naming\n",
    "            \n",
    "            # General patterns\n",
    "            (r'\\.bn\\.', r'.1.'),  # BatchNorm position differences\n",
    "            (r'\\.conv\\.', r'.0.'),  # Conv position differences\n",
    "        ]\n",
    "        \n",
    "        # Try to match remaining keys using patterns\n",
    "        import re\n",
    "        matched_keys = set()\n",
    "        \n",
    "        for model_key in remaining_model_keys:\n",
    "            model_shape = model.state_dict()[model_key].shape\n",
    "            \n",
    "            # Try direct substring matching first (more reliable)\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "            \n",
    "            for state_key in remaining_state_keys:\n",
    "                state_shape = state_dict[state_key].shape\n",
    "                \n",
    "                # Skip if shapes don't match\n",
    "                if state_shape != model_shape:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate match score (how many segments match)\n",
    "                model_parts = model_key.split('.')\n",
    "                state_parts = state_key.split('.')\n",
    "                \n",
    "                # Calculate similarity score\n",
    "                common_parts = sum(1 for a, b in zip(model_parts, state_parts) if a == b)\n",
    "                similarity = common_parts / max(len(model_parts), len(state_parts))\n",
    "                \n",
    "                if similarity > best_score:\n",
    "                    best_score = similarity\n",
    "                    best_match = state_key\n",
    "            \n",
    "            # If we found a good match, use it\n",
    "            if best_match and best_score > 0.6:  # 60% similarity threshold\n",
    "                adapted_dict[model_key] = state_dict[best_match]\n",
    "                matched_keys.add(best_match)\n",
    "            \n",
    "            # Otherwise try pattern-based matching\n",
    "            else:\n",
    "                for old_pattern, new_pattern in key_patterns:\n",
    "                    # Try transforming model key to state key pattern\n",
    "                    transformed_key = re.sub(new_pattern, old_pattern, model_key)\n",
    "                    if transformed_key in remaining_state_keys and \\\n",
    "                       state_dict[transformed_key].shape == model_shape:\n",
    "                        adapted_dict[model_key] = state_dict[transformed_key]\n",
    "                        matched_keys.add(transformed_key)\n",
    "                        break\n",
    "        \n",
    "        # Check what percentage of keys we managed to adapt\n",
    "        coverage = len(adapted_dict) / len(model_keys) * 100\n",
    "        \n",
    "        return adapted_dict, coverage\n",
    "    \n",
    "    @staticmethod\n",
    "    def verify_compatibility(model, state_dict):\n",
    "        \"\"\"\n",
    "        Verify if a state dictionary is compatible with a model.\n",
    "        Returns a compatibility score (0-100%) and details about compatibility issues.\n",
    "        \"\"\"\n",
    "        model_type = ModelStateLoader.get_model_type(model)\n",
    "        state_dict_type = ModelStateLoader.get_state_dict_type(state_dict)\n",
    "        \n",
    "        # Basic type check\n",
    "        if model_type != \"unknown\" and state_dict_type != \"unknown\" and model_type != state_dict_type:\n",
    "            return 0, f\"Model type mismatch: model is {model_type}, state dict is {state_dict_type}\"\n",
    "        \n",
    "        # Check key coverage and shape compatibility\n",
    "        model_state = model.state_dict()\n",
    "        model_keys = set(model_state.keys())\n",
    "        dict_keys = set(state_dict.keys())\n",
    "        \n",
    "        # Calculate key overlap\n",
    "        common_keys = model_keys.intersection(dict_keys)\n",
    "        key_coverage = len(common_keys) / len(model_keys) * 100\n",
    "        \n",
    "        # Check shape compatibility for common keys\n",
    "        shape_mismatches = []\n",
    "        shape_matches = 0\n",
    "        \n",
    "        for key in common_keys:\n",
    "            if model_state[key].shape == state_dict[key].shape:\n",
    "                shape_matches += 1\n",
    "            else:\n",
    "                shape_mismatches.append((key, model_state[key].shape, state_dict[key].shape))\n",
    "        \n",
    "        shape_compatibility = shape_matches / max(1, len(common_keys)) * 100\n",
    "        \n",
    "        # Overall compatibility score\n",
    "        compatibility_score = key_coverage * shape_compatibility / 100\n",
    "        \n",
    "        details = {\n",
    "            'model_type': model_type,\n",
    "            'state_dict_type': state_dict_type,\n",
    "            'key_coverage': key_coverage,\n",
    "            'shape_compatibility': shape_compatibility,\n",
    "            'shape_mismatches': shape_mismatches\n",
    "        }\n",
    "        \n",
    "        return compatibility_score, details\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_state_dict(model, state_dict, strict=False):\n",
    "        \"\"\"\n",
    "        Load a state dictionary into a model with adaptive compatibility handling.\n",
    "        \n",
    "        Args:\n",
    "            model: Target model to load state into\n",
    "            state_dict: Source state dictionary\n",
    "            strict: If True, raise error on key mismatch\n",
    "            \n",
    "        Returns:\n",
    "            success: Boolean indicating if loading was successful\n",
    "            coverage: Percentage of model parameters loaded\n",
    "            details: Additional information about the loading process\n",
    "        \"\"\"\n",
    "        # Extract model state dict from checkpoint format if needed\n",
    "        if 'model_state_dict' in state_dict:\n",
    "            state_dict = state_dict['model_state_dict']\n",
    "        elif 'state_dict' in state_dict:\n",
    "            state_dict = state_dict['state_dict']\n",
    "        \n",
    "        # Check compatibility\n",
    "        compatibility, details = ModelStateLoader.verify_compatibility(model, state_dict)\n",
    "        \n",
    "        # High compatibility - try direct loading\n",
    "        if compatibility > 90:\n",
    "            try:\n",
    "                model.load_state_dict(state_dict, strict=strict)\n",
    "                return True, compatibility, details\n",
    "            except Exception as e:\n",
    "                if strict:\n",
    "                    raise e\n",
    "        \n",
    "        # Lower compatibility - try adaptive loading\n",
    "        adapted_dict, coverage = ModelStateLoader.adapt_state_dict(state_dict, model)\n",
    "        \n",
    "        # If we have sufficient coverage, use the adapted dict\n",
    "        if coverage >= 70 or not strict:\n",
    "            try:\n",
    "                # Use original model.load_state_dict for parameters we could adapt\n",
    "                model.load_state_dict(adapted_dict, strict=False)\n",
    "                return True, coverage, {\n",
    "                    **details,\n",
    "                    'adapted': True,\n",
    "                    'coverage': coverage\n",
    "                }\n",
    "            except Exception as e:\n",
    "                if strict:\n",
    "                    raise e\n",
    "                return False, coverage, {\n",
    "                    **details,\n",
    "                    'error': str(e),\n",
    "                    'adapted': True,\n",
    "                    'coverage': coverage\n",
    "                }\n",
    "        \n",
    "        return False, coverage, {\n",
    "            **details,\n",
    "            'adapted': True, \n",
    "            'coverage': coverage,\n",
    "            'error': 'Insufficient parameter coverage'\n",
    "        }\n",
    "\n",
    "# Feature extraction\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature extraction helper class that registers forward hooks to capture\n",
    "    intermediate layer outputs from neural networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.model = model\n",
    "        self.features = None\n",
    "        \n",
    "        # Flag to track if hook was registered successfully\n",
    "        self.hook_registered = False\n",
    "        \n",
    "        # Register hook to extract features\n",
    "        for name, module in model.named_modules():\n",
    "            if layer_name in name:  # More flexible matching\n",
    "                module.register_forward_hook(self.hook)\n",
    "                self.hook_registered = True\n",
    "                logger.info(f\"Hook registered for {name}\")\n",
    "                break\n",
    "        \n",
    "        if not self.hook_registered:\n",
    "            logger.warning(f\"Could not find layer {layer_name} in model, listing available layers:\")\n",
    "            for name, _ in model.named_modules():\n",
    "                logger.warning(f\"  - {name}\")\n",
    "                \n",
    "    def hook(self, module, input, output):\n",
    "        self.features = output\n",
    "        \n",
    "    def get_features(self, x):\n",
    "        _ = self.model(x)\n",
    "        return self.features\n",
    "\n",
    "# Add this utility function before mutual_learning_phase\n",
    "def get_feature_shapes(feature_extractors, config):\n",
    "    \"\"\"\n",
    "    Utility to get teacher_feature_shapes and student_feature_shape for HFI.\n",
    "    Returns:\n",
    "        teacher_feature_shapes: dict of {teacher_name: feature_shape}\n",
    "        student_feature_shape: shape tuple\n",
    "    \"\"\"\n",
    "    teacher_feature_shapes = {}\n",
    "    student_feature_shape = None\n",
    "    for name, extractor in feature_extractors.items():\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, config.get_input_size(name), config.get_input_size(name)).to(device)\n",
    "            _ = extractor.model(dummy_input)\n",
    "            if extractor.features is not None:\n",
    "                if name == 'student':\n",
    "                    student_feature_shape = extractor.features.shape\n",
    "                else:\n",
    "                    teacher_feature_shapes[name] = extractor.features.shape\n",
    "    return teacher_feature_shapes, student_feature_shape\n",
    "\n",
    "# Feature Alignment Loss\n",
    "class FeatureAlignmentLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function that aligns feature representations between models,\n",
    "    handling different feature dimensions through adaptive transformations.\n",
    "    \n",
    "    This helps in transferring knowledge at the feature level, which can be\n",
    "    particularly useful in mutual learning and knowledge distillation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureAlignmentLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.projections = {}  # Cache projections for efficiency\n",
    "    \n",
    "    def forward(self, student_features, teacher_features):\n",
    "        \"\"\"\n",
    "        Align student features with teacher features by handling dimension mismatches\n",
    "        \n",
    "        Args:\n",
    "            student_features: Features from the student/source model\n",
    "            teacher_features: Features from the teacher/target model\n",
    "            \n",
    "        Returns:\n",
    "            MSE loss between aligned features\n",
    "        \"\"\"\n",
    "        # Log shape information for debugging\n",
    "        student_shape = student_features.shape\n",
    "        teacher_shape = teacher_features.shape\n",
    "        \n",
    "        # Detach teacher features to prevent gradients flowing back to teachers\n",
    "        teacher_features = teacher_features.detach()\n",
    "\n",
    "        # Get batch size which should always be the first dimension\n",
    "        batch_size = student_features.size(0)\n",
    "        \n",
    "        try:\n",
    "            # Check if dimensions are compatible or need transformation\n",
    "            if student_shape != teacher_shape:\n",
    "                # Handle different dimensionality cases\n",
    "                student_dim = len(student_shape)\n",
    "                teacher_dim = len(teacher_shape)\n",
    "                \n",
    "                logger.debug(f\"Aligning features: student shape {student_shape} ({student_dim}D) to teacher shape {teacher_shape} ({teacher_dim}D)\")\n",
    "                \n",
    "                # Case 1: 4D to 4D (CNN to CNN, different spatial/channel dims)\n",
    "                if student_dim == 4 and teacher_dim == 4:\n",
    "                    # Handle CNN feature maps (B, C, H, W)\n",
    "                    student_channels = student_shape[1]\n",
    "                    teacher_channels = teacher_shape[1]\n",
    "                    teacher_h, teacher_w = teacher_shape[2], teacher_shape[3]\n",
    "                    \n",
    "                    # Spatial dimension adjustment\n",
    "                    spatial_pool = nn.AdaptiveAvgPool2d((teacher_h, teacher_w))\n",
    "                    student_features = spatial_pool(student_features)\n",
    "                    \n",
    "                    # Channel dimension adjustment\n",
    "                    if student_channels != teacher_channels:\n",
    "                        key = f\"4d_{student_channels}_{teacher_channels}\"\n",
    "                        if key not in self.projections:\n",
    "                            self.projections[key] = nn.Conv2d(\n",
    "                                student_channels, teacher_channels, kernel_size=1, bias=False\n",
    "                            ).to(student_features.device)\n",
    "                        student_features = self.projections[key](student_features)\n",
    "                \n",
    "                # Case 2: 3D to 3D (sequence to sequence, different lengths/dims)\n",
    "                elif student_dim == 3 and teacher_dim == 3:\n",
    "                    # Handle sequence-like features (B, L, D) or (B, D, L)\n",
    "                    # Determine if dimensions are (B, L, D) or (B, D, L) based on common patterns\n",
    "                    if student_shape[2] > student_shape[1]:  # Likely (B, L, D)\n",
    "                        student_len, student_channels = student_shape[1], student_shape[2]\n",
    "                        teacher_len, teacher_channels = teacher_shape[1], teacher_shape[2]\n",
    "                        is_transposed = False\n",
    "                    else:  # Likely (B, D, L)\n",
    "                        student_channels, student_len = student_shape[1], student_shape[2]\n",
    "                        teacher_channels, teacher_len = teacher_shape[1], teacher_shape[2]\n",
    "                        is_transposed = True\n",
    "                        student_features = student_features.transpose(1, 2)\n",
    "                        teacher_features = teacher_features.transpose(1, 2)\n",
    "                    \n",
    "                    # Sequence length adjustment\n",
    "                    if student_len != teacher_len:\n",
    "                        if is_transposed:\n",
    "                            # Already in (B, L, D) format after transpose\n",
    "                            pass\n",
    "                        else:\n",
    "                            # Convert to (B, D, L) for pooling\n",
    "                            student_features = student_features.transpose(1, 2)\n",
    "                            \n",
    "                        # Use adaptive pooling along sequence dimension\n",
    "                        student_features = F.adaptive_avg_pool1d(student_features, teacher_len)\n",
    "                        \n",
    "                        if is_transposed:\n",
    "                            # Keep in (B, L, D) format\n",
    "                            pass\n",
    "                        else:\n",
    "                            # Convert back to (B, L, D)\n",
    "                            student_features = student_features.transpose(1, 2)\n",
    "                    \n",
    "                    # Feature dimension adjustment\n",
    "                    if student_channels != teacher_channels:\n",
    "                        key = f\"3d_{student_channels}_{teacher_channels}\"\n",
    "                        if key not in self.projections:\n",
    "                            self.projections[key] = nn.Linear(\n",
    "                                student_channels, teacher_channels, bias=False\n",
    "                            ).to(student_features.device)\n",
    "                        \n",
    "                        # Apply linear projection\n",
    "                        orig_shape = student_features.shape\n",
    "                        student_features = student_features.view(-1, student_channels)\n",
    "                        student_features = self.projections[key](student_features)\n",
    "                        student_features = student_features.view(batch_size, -1, teacher_channels)\n",
    "                    \n",
    "                    # Restore original tensor format if needed\n",
    "                    if is_transposed:\n",
    "                        student_features = student_features.transpose(1, 2)\n",
    "                        teacher_features = teacher_features.transpose(1, 2)\n",
    "                \n",
    "                # Case 3: 2D to 2D (vector to vector, different dims)\n",
    "                elif student_dim == 2 and teacher_dim == 2:\n",
    "                    # Handle vector features (B, D)\n",
    "                    student_channels = student_shape[1]\n",
    "                    teacher_channels = teacher_shape[1]\n",
    "                    \n",
    "                    # Feature dimension adjustment\n",
    "                    if student_channels != teacher_channels:\n",
    "                        key = f\"2d_{student_channels}_{teacher_channels}\"\n",
    "                        if key not in self.projections:\n",
    "                            self.projections[key] = nn.Linear(\n",
    "                                student_channels, teacher_channels, bias=False\n",
    "                            ).to(student_features.device)\n",
    "                        student_features = self.projections[key](student_features)\n",
    "                \n",
    "                # Case 4: 4D to 3D (CNN to Transformer)\n",
    "                elif student_dim == 4 and teacher_dim == 3:\n",
    "                    # Convert CNN features (B, C, H, W) to sequence-like (B, L, D)\n",
    "                    student_channels = student_shape[1]\n",
    "                    \n",
    "                    # Check if teacher shape is (B, L, D) or (B, D, L)\n",
    "                    if teacher_shape[1] <= teacher_shape[2]:  # Probably (B, L, D)\n",
    "                        teacher_len, teacher_channels = teacher_shape[1], teacher_shape[2]\n",
    "                        is_teacher_transposed = False\n",
    "                    else:  # Probably (B, D, L)\n",
    "                        teacher_channels, teacher_len = teacher_shape[1], teacher_shape[2]\n",
    "                        is_teacher_transposed = True\n",
    "                        teacher_features = teacher_features.transpose(1, 2)\n",
    "                    \n",
    "                    # Reshape: [B, C, H, W] â†’ [B, C, H*W] â†’ [B, H*W, C]\n",
    "                    student_features = student_features.flatten(2)  # [B, C, H*W]\n",
    "                    student_features = student_features.transpose(1, 2)  # [B, H*W, C]\n",
    "                    \n",
    "                    # Adjust sequence length to match teacher\n",
    "                    if student_features.size(1) != teacher_len:\n",
    "                        # Use adaptive pooling to get the right sequence length\n",
    "                        student_features = student_features.transpose(1, 2)  # [B, C, H*W]\n",
    "                        student_features = F.adaptive_avg_pool1d(student_features, teacher_len)\n",
    "                        student_features = student_features.transpose(1, 2)  # [B, L, C]\n",
    "                    \n",
    "                    # Adjust feature dimension\n",
    "                    if student_channels != teacher_channels:\n",
    "                        key = f\"4d3d_{student_channels}_{teacher_channels}\"\n",
    "                        if key not in self.projections:\n",
    "                            self.projections[key] = nn.Linear(\n",
    "                                student_channels, teacher_channels, bias=False\n",
    "                            ).to(student_features.device)\n",
    "                        \n",
    "                        # Apply projection\n",
    "                        orig_shape = student_features.shape\n",
    "                        student_features = student_features.reshape(-1, student_channels)\n",
    "                        student_features = self.projections[key](student_features)\n",
    "                        student_features = student_features.reshape(batch_size, -1, teacher_channels)\n",
    "                    \n",
    "                    # Restore teacher format if needed\n",
    "                    if is_teacher_transposed:\n",
    "                        student_features = student_features.transpose(1, 2)\n",
    "                        teacher_features = teacher_features.transpose(1, 2)\n",
    "                \n",
    "                # Case 5: 3D to 4D (Transformer to CNN)\n",
    "                elif student_dim == 3 and teacher_dim == 4:\n",
    "                    # Convert sequence features (B, L, D) or (B, D, L) to CNN-like (B, C, H, W)\n",
    "                    teacher_channels = teacher_shape[1]\n",
    "                    teacher_h, teacher_w = teacher_shape[2], teacher_shape[3]\n",
    "                    \n",
    "                    # Check if student is (B, L, D) or (B, D, L)\n",
    "                    if student_shape[1] <= student_shape[2]:  # Probably (B, L, D)\n",
    "                        student_len, student_channels = student_shape[1], student_shape[2]\n",
    "                        student_features = student_features.transpose(1, 2)  # Convert to (B, D, L)\n",
    "                    else:  # Probably (B, D, L)\n",
    "                        student_channels, student_len = student_shape[1], student_shape[2]\n",
    "                    \n",
    "                    # Try to reshape to create a square-like spatial representation\n",
    "                    target_h = int(np.sqrt(student_len))\n",
    "                    if target_h * target_h == student_len:\n",
    "                        # Perfect square - reshape directly\n",
    "                        target_w = target_h\n",
    "                    else:\n",
    "                        # Not a perfect square, use adaptive pooling\n",
    "                        student_features = F.adaptive_avg_pool1d(student_features, teacher_h * teacher_w)\n",
    "                        target_h, target_w = teacher_h, teacher_w\n",
    "                    \n",
    "                    # Reshape to 4D\n",
    "                    try:\n",
    "                        student_features = student_features.reshape(batch_size, student_channels, target_h, target_w)\n",
    "                    except RuntimeError:\n",
    "                        # If reshape fails, use adaptive pooling as fallback\n",
    "                        logger.warning(f\"Reshape failed, using adaptive pooling: {student_features.shape} -> [{batch_size}, {student_channels}, {teacher_h}, {teacher_w}]\")\n",
    "                        if len(student_features.shape) == 3:\n",
    "                            # If still in (B, D, L) format\n",
    "                            student_features = F.adaptive_avg_pool1d(student_features, teacher_h * teacher_w)\n",
    "                            student_features = student_features.reshape(batch_size, student_channels, teacher_h, teacher_w)\n",
    "                    \n",
    "                    # Channel dimension adjustment\n",
    "                    if student_channels != teacher_channels:\n",
    "                        key = f\"3d4d_{student_channels}_{teacher_channels}\"\n",
    "                        if key not in self.projections:\n",
    "                            self.projections[key] = nn.Conv2d(\n",
    "                                student_channels, teacher_channels, kernel_size=1, bias=False\n",
    "                            ).to(student_features.device)\n",
    "                        student_features = self.projections[key](student_features)\n",
    "                \n",
    "                # Fallback: transform to common format by flattening\n",
    "                else:\n",
    "                    logger.warning(f\"Using fallback alignment for dimensions: student {student_dim}D -> teacher {teacher_dim}D\")\n",
    "                    \n",
    "                    # Flatten both tensors to 2D (batch_size, -1)\n",
    "                    student_flat = student_features.reshape(batch_size, -1)\n",
    "                    teacher_flat = teacher_features.reshape(batch_size, -1)\n",
    "                    \n",
    "                    student_dim = student_flat.size(1)\n",
    "                    teacher_dim = teacher_flat.size(1)\n",
    "                    \n",
    "                    # Project to match dimensions if needed\n",
    "                    if student_dim != teacher_dim:\n",
    "                        key = f\"flat_{student_dim}_{teacher_dim}\"\n",
    "                        if key not in self.projections:\n",
    "                            self.projections[key] = nn.Linear(\n",
    "                                student_dim, teacher_dim, bias=False\n",
    "                            ).to(student_features.device)\n",
    "                        student_features = self.projections[key](student_flat)\n",
    "                    else:\n",
    "                        student_features = student_flat\n",
    "                    \n",
    "                    # Try to reshape back to teacher shape if possible\n",
    "                    try:\n",
    "                        student_features = student_features.reshape(teacher_shape)\n",
    "                    except RuntimeError:\n",
    "                        # If reshape fails, keep as flattened\n",
    "                        teacher_features = teacher_flat\n",
    "            \n",
    "            # Verify the shapes match after transformation\n",
    "            if student_features.shape != teacher_features.shape:\n",
    "                logger.warning(f\"Failed to match shapes: student {student_features.shape} vs teacher {teacher_features.shape}\")\n",
    "                \n",
    "                # Final fallback: flatten both tensors\n",
    "                student_features = student_features.reshape(batch_size, -1)\n",
    "                teacher_features = teacher_features.reshape(batch_size, -1)\n",
    "                \n",
    "                # If dimensions still don't match, use global average\n",
    "                if student_features.size(1) != teacher_features.size(1):\n",
    "                    logger.warning(\"Using global average for feature alignment as shapes cannot be matched\")\n",
    "                    student_features = torch.mean(student_features, dim=1, keepdim=True)\n",
    "                    teacher_features = torch.mean(teacher_features, dim=1, keepdim=True)\n",
    "            \n",
    "            # Apply MSE loss on aligned features\n",
    "            return self.mse_loss(student_features, teacher_features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch any unexpected errors to prevent training from crashing\n",
    "            logger.error(f\"Error in feature alignment: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            \n",
    "            # Return zero loss to continue training despite the error\n",
    "            return torch.tensor(0.0, device=student_features.device, requires_grad=True)\n",
    "\n",
    "# Heterogeneous Feature Integration (HFI) - Updated with shape adaptation code\n",
    "class HeterogeneousFeatureIntegrator(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Heterogeneous Feature Integration (HFI) mechanism.\n",
    "    This module fuses features from multiple teacher models using learnable projections and attention weights.\n",
    "    Handles shape mismatches between teachers and student.\n",
    "    \"\"\"\n",
    "    def __init__(self, teacher_feature_shapes, student_feature_shape):\n",
    "        super(HeterogeneousFeatureIntegrator, self).__init__()\n",
    "        self.teacher_names = list(teacher_feature_shapes.keys())\n",
    "        self.K = len(self.teacher_names)\n",
    "        self.student_feature_shape = student_feature_shape # Store target shape (excluding batch)\n",
    "\n",
    "        logger.info(f\"HFI Initializing. Target student feature shape (excluding batch): {self.student_feature_shape}\")\n",
    "\n",
    "        # Step 2: Learnable projections (Ï†_j)\n",
    "        self.projections = nn.ModuleDict()\n",
    "\n",
    "        # Determine student dimensionality\n",
    "        self.student_dim = len(self.student_feature_shape) + 1 # Add 1 for batch dim\n",
    "\n",
    "        for teacher_name, feature_shape in teacher_feature_shapes.items():\n",
    "            teacher_dim = len(feature_shape) + 1\n",
    "            logger.info(f\"  Teacher {teacher_name}: Original feature shape {feature_shape} ({teacher_dim}D)\")\n",
    "\n",
    "            # --- Projection Logic ---\n",
    "            # Case 1: Teacher is 4D (CNN), Student is 4D (CNN)\n",
    "            if teacher_dim == 4 and self.student_dim == 4:\n",
    "                # Project channels using 1x1 Conv\n",
    "                self.projections[teacher_name] = nn.Conv2d(\n",
    "                    feature_shape[0], # Input channels C\n",
    "                    self.student_feature_shape[0], # Target channels C'\n",
    "                    kernel_size=1, bias=False\n",
    "                )\n",
    "                logger.info(f\"    {teacher_name} (4D->4D): Using Conv2d projection {feature_shape[0]} -> {self.student_feature_shape[0]}\")\n",
    "\n",
    "            # Case 2: Teacher is 3D (ViT), Student is 4D (CNN)\n",
    "            elif teacher_dim == 3 and self.student_dim == 4:\n",
    "                # Project embedding dim D to target channels C' using Linear\n",
    "                # feature_shape is (L, D) for ViT\n",
    "                self.projections[teacher_name] = nn.Linear(\n",
    "                    feature_shape[1], # Input embedding dim D\n",
    "                    self.student_feature_shape[0], # Target channels C'\n",
    "                    bias=False\n",
    "                )\n",
    "                logger.info(f\"    {teacher_name} (3D->4D): Using Linear projection {feature_shape[1]} -> {self.student_feature_shape[0]}\")\n",
    "\n",
    "            # Case 3: Teacher is 4D (CNN), Student is 3D (ViT) - Less common but possible\n",
    "            elif teacher_dim == 4 and self.student_dim == 3:\n",
    "                 # Project channels C to target embedding dim D' using Conv1x1 then flatten/pool\n",
    "                 # feature_shape is (C, H, W)\n",
    "                 self.projections[teacher_name] = nn.Conv2d(\n",
    "                     feature_shape[0], # Input channels C\n",
    "                     self.student_feature_shape[1], # Target embedding D'\n",
    "                     kernel_size=1, bias=False\n",
    "                 )\n",
    "                 logger.info(f\"    {teacher_name} (4D->3D): Using Conv2d projection {feature_shape[0]} -> {self.student_feature_shape[1]}\")\n",
    "\n",
    "            # Case 4: Teacher is 3D (ViT), Student is 3D (ViT)\n",
    "            elif teacher_dim == 3 and self.student_dim == 3:\n",
    "                 # Project embedding dim D to target embedding dim D' using Linear\n",
    "                 self.projections[teacher_name] = nn.Linear(\n",
    "                     feature_shape[1], # Input embedding D\n",
    "                     self.student_feature_shape[1], # Target embedding D'\n",
    "                     bias=False\n",
    "                 )\n",
    "                 logger.info(f\"    {teacher_name} (3D->3D): Using Linear projection {feature_shape[1]} -> {self.student_feature_shape[1]}\")\n",
    "\n",
    "            # Add more cases or a fallback if needed\n",
    "            else:\n",
    "                logger.warning(f\"    Unsupported feature shape combination for {teacher_name}: {teacher_dim}D -> {self.student_dim}D. Skipping projection.\")\n",
    "                # Optionally add a fallback projection (e.g., flatten and linear)\n",
    "                # self.projections[teacher_name] = nn.Identity() # Or some fallback\n",
    "\n",
    "        # Step 3: Learnable attention weights (W)\n",
    "        self.attention_weights = nn.Parameter(torch.zeros(self.K))\n",
    "\n",
    "        logger.info(f\"HFI module initialized with {self.K} teachers.\")\n",
    "\n",
    "    def forward(self, teacher_features):\n",
    "        \"\"\"\n",
    "        Fuse features from multiple teachers using learned projections and attention.\n",
    "\n",
    "        Args:\n",
    "            teacher_features: Dict with teacher_name -> feature_tensor (BATCH included)\n",
    "\n",
    "        Returns:\n",
    "            Fused feature tensor with same shape as student features (BATCH included)\n",
    "        \"\"\"\n",
    "        # Ensure teacher_features is not empty\n",
    "        if not teacher_features:\n",
    "             logger.warning(\"HFI received empty teacher_features dict.\")\n",
    "             return None\n",
    "\n",
    "        # Get device and batch size from the first valid feature tensor\n",
    "        first_valid_feat = next((f for f in teacher_features.values() if f is not None), None)\n",
    "        if first_valid_feat is None:\n",
    "            logger.warning(\"HFI received dict with all None features.\")\n",
    "            return None\n",
    "        device = self.attention_weights.device\n",
    "        batch_size = first_valid_feat.size(0)\n",
    "\n",
    "        # Calculate attention weights (Î±) using softmax\n",
    "        alpha = F.softmax(self.attention_weights, dim=0)\n",
    "        alpha_dict = {name: alpha[i].item() for i, name in enumerate(self.teacher_names)} # For logging\n",
    "\n",
    "        fused_features = None\n",
    "        target_shape_with_batch = (batch_size,) + self.student_feature_shape # Full target shape\n",
    "\n",
    "        for i, teacher_name in enumerate(self.teacher_names):\n",
    "            # Ensure the teacher feature exists, is not None, and has a projection layer\n",
    "            if teacher_name not in teacher_features or teacher_features[teacher_name] is None:\n",
    "                # logger.debug(f\"Skipping {teacher_name}: Features are None.\") # Optional debug log\n",
    "                continue\n",
    "            if teacher_name not in self.projections:\n",
    "                logger.warning(f\"Skipping {teacher_name}: No projection layer found.\")\n",
    "                continue\n",
    "\n",
    "            feat = teacher_features[teacher_name].detach().float() # Use float32 for stability\n",
    "            teacher_shape_with_batch = feat.shape\n",
    "            teacher_dim = len(teacher_shape_with_batch)\n",
    "\n",
    "            # --- Apply Projection ---\n",
    "            projected_feat = self.projections[teacher_name](feat)\n",
    "\n",
    "            # --- Adapt Shape to Match Student ---\n",
    "            adapted_feat = None\n",
    "            # Case 1: Teacher 4D, Student 4D\n",
    "            if teacher_dim == 4 and self.student_dim == 4:\n",
    "                # Adapt spatial dimensions if needed\n",
    "                if projected_feat.shape[2:] != self.student_feature_shape[1:]: # Compare H, W\n",
    "                    adapted_feat = F.adaptive_avg_pool2d(\n",
    "                        projected_feat, output_size=self.student_feature_shape[1:] # Target H, W\n",
    "                    )\n",
    "                else:\n",
    "                    adapted_feat = projected_feat # Shapes already match\n",
    "\n",
    "            # Case 2: Teacher 3D (ViT: B, L, D), Student 4D (CNN: B, C', H', W')\n",
    "            elif teacher_dim == 3 and self.student_dim == 4:\n",
    "                # Projected feat shape is (B, L, C') after linear projection\n",
    "                target_channels, target_h, target_w = self.student_feature_shape # C', H', W'\n",
    "                current_len = projected_feat.size(1) # L\n",
    "                current_channels = projected_feat.size(2) # C' (should match target_channels)\n",
    "\n",
    "                if current_channels != target_channels:\n",
    "                     # This shouldn't happen if projection is correct, but log warning\n",
    "                     logger.warning(f\"HFI shape mismatch after 3D->4D projection for {teacher_name}: Got C={current_channels}, expected C={target_channels}\")\n",
    "                     continue # Skip if projection failed\n",
    "\n",
    "                # Reshape/Pool sequence L into spatial H'*W'\n",
    "                # Transpose to (B, C', L) for pooling\n",
    "                feat_to_pool = projected_feat.transpose(1, 2)\n",
    "                # Pool sequence dimension L to target spatial size H'*W'\n",
    "                pooled_feat = F.adaptive_avg_pool1d(feat_to_pool, target_h * target_w)\n",
    "                # Reshape to target spatial dimensions (B, C', H', W')\n",
    "                try:\n",
    "                    adapted_feat = pooled_feat.view(batch_size, target_channels, target_h, target_w)\n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"HFI reshape failed for {teacher_name} (3D->4D): {e}. Shape was {pooled_feat.shape}\")\n",
    "                    continue # Skip if reshape fails\n",
    "\n",
    "            # Case 3: Teacher 4D (CNN: B, C, H, W), Student 3D (ViT: B, L', D')\n",
    "            elif teacher_dim == 4 and self.student_dim == 3:\n",
    "                 # Projected feat shape is (B, D', H, W) after Conv projection\n",
    "                 target_len, target_channels = self.student_feature_shape # L', D'\n",
    "                 current_channels = projected_feat.size(1) # D' (should match target_channels)\n",
    "\n",
    "                 if current_channels != target_channels:\n",
    "                      logger.warning(f\"HFI shape mismatch after 4D->3D projection for {teacher_name}: Got D={current_channels}, expected D={target_channels}\")\n",
    "                      continue\n",
    "\n",
    "                 # Flatten spatial dims H, W into sequence L = H*W\n",
    "                 feat_flat = projected_feat.flatten(2) # (B, D', L)\n",
    "                 # Pool sequence dimension L to target L'\n",
    "                 if feat_flat.size(2) != target_len:\n",
    "                     pooled_feat = F.adaptive_avg_pool1d(feat_flat, target_len) # Pool along L dim\n",
    "                 else:\n",
    "                     pooled_feat = feat_flat\n",
    "                 # Transpose to target format (B, L', D')\n",
    "                 adapted_feat = pooled_feat.transpose(1, 2)\n",
    "\n",
    "            # Case 4: Teacher 3D, Student 3D\n",
    "            elif teacher_dim == 3 and self.student_dim == 3:\n",
    "                 # Projected feat shape is (B, L, D') after linear projection\n",
    "                 target_len, target_channels = self.student_feature_shape # L', D'\n",
    "                 current_len = projected_feat.size(1) # L\n",
    "                 current_channels = projected_feat.size(2) # D' (should match target_channels)\n",
    "\n",
    "                 if current_channels != target_channels:\n",
    "                      logger.warning(f\"HFI shape mismatch after 3D->3D projection for {teacher_name}: Got D={current_channels}, expected D={target_channels}\")\n",
    "                      continue\n",
    "\n",
    "                 # Pool sequence dimension L to target L' if needed\n",
    "                 if current_len != target_len:\n",
    "                     feat_to_pool = projected_feat.transpose(1, 2) # (B, D', L)\n",
    "                     pooled_feat = F.adaptive_avg_pool1d(feat_to_pool, target_len)\n",
    "                     adapted_feat = pooled_feat.transpose(1, 2) # (B, L', D')\n",
    "                 else:\n",
    "                     adapted_feat = projected_feat # Shapes already match\n",
    "\n",
    "            else:\n",
    "                logger.warning(f\"Skipping {teacher_name}: Unhandled shape adaptation from {teacher_dim}D to {self.student_dim}D.\")\n",
    "                continue\n",
    "\n",
    "            # --- Feature Fusion ---\n",
    "            if adapted_feat is not None:\n",
    "                # Ensure adapted_feat matches the full target shape\n",
    "                if adapted_feat.shape != target_shape_with_batch:\n",
    "                     logger.error(f\"HFI Adaptation Error for {teacher_name}: Final shape {adapted_feat.shape} != Target {target_shape_with_batch}\")\n",
    "                     continue # Skip if adaptation failed\n",
    "\n",
    "                weighted = alpha[i] * adapted_feat\n",
    "\n",
    "                if fused_features is None:\n",
    "                    fused_features = weighted\n",
    "                else:\n",
    "                    # --- Check shapes BEFORE adding ---\n",
    "                    if fused_features.shape != weighted.shape:\n",
    "                         logger.error(f\"HFI Fusion Error: Shape mismatch before adding {teacher_name}. \"\n",
    "                                      f\"Fused: {fused_features.shape}, Weighted: {weighted.shape}\")\n",
    "                         # Option: Skip adding this problematic tensor\n",
    "                         continue\n",
    "                         # Option: Try to reshape/re-adapt 'weighted' again (less ideal)\n",
    "                         # Option: Raise error\n",
    "                    fused_features = fused_features + weighted\n",
    "            else:\n",
    "                 logger.warning(f\"Feature adaptation failed for {teacher_name}, not included in fusion.\")\n",
    "\n",
    "\n",
    "        # Handle case where no teachers contributed valid features\n",
    "        if fused_features is None:\n",
    "             logger.error(\"HFI resulted in None fused_features. Returning zeros.\")\n",
    "             # Return a zero tensor matching the student shape instead of None\n",
    "             return torch.zeros(target_shape_with_batch, device=device, dtype=torch.float32)\n",
    "\n",
    "        return fused_features\n",
    "\n",
    "# Mutual Learning Loss Functions\n",
    "class MutualLearningLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MutualLearningLoss, self).__init__()\n",
    "        self.config = config\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.temperatures = {name: config.model_temperatures.get(name, config.soft_target_temp) \n",
    "                            for name in config.models}\n",
    "        self.learnable_temps = None\n",
    "        \n",
    "        # Create learnable temperature parameters if enabled\n",
    "        if config.learn_temperatures:\n",
    "            self.learnable_temps = nn.ParameterDict({\n",
    "                name: nn.Parameter(torch.tensor(temp).to(device))\n",
    "                for name, temp in self.temperatures.items()\n",
    "            })\n",
    "    \n",
    "    def get_temperature(self, model_name):\n",
    "        \"\"\"Get temperature for a specific model\"\"\"\n",
    "        if self.learnable_temps is not None:\n",
    "            # Use softplus for positive temperatures, allow <1.0\n",
    "            return torch.nn.functional.softplus(self.learnable_temps[model_name]) + 0.1\n",
    "        else:\n",
    "            return self.temperatures[model_name]\n",
    "    \n",
    "    def forward(self, logits, targets, peer_logits, model_name, alpha, cal_weight, feature_loss, feature_weight):\n",
    "        \"\"\"\n",
    "        Calculate the combined loss for mutual learning\n",
    "        \n",
    "        Args:\n",
    "            logits: The output logits of the current model\n",
    "            targets: The ground truth labels\n",
    "            peer_logits: Dictionary of logits from peer models {model_name: logits}\n",
    "            model_name: Name of the current model\n",
    "            alpha: Weight for the mutual learning component\n",
    "            cal_weight: Weight for the calibration component\n",
    "            feature_loss: Feature alignment loss\n",
    "            feature_weight: Weight for the feature alignment component\n",
    "            \n",
    "        Returns:\n",
    "            total_loss, ce_loss, mutual_loss, cal_loss, feature_loss\n",
    "        \"\"\"\n",
    "        # Cross-entropy loss\n",
    "        ce_loss = self.ce_loss(logits, targets)\n",
    "        \n",
    "        # Mutual learning loss (KL divergence with peer models)\n",
    "        mutual_losses = []\n",
    "        for peer_name, peer_output in peer_logits.items():\n",
    "            if peer_name == model_name:\n",
    "                continue  # Skip self\n",
    "            \n",
    "            # Get temperatures for both models\n",
    "            temp_self = self.get_temperature(model_name)\n",
    "            temp_peer = self.get_temperature(peer_name)\n",
    "            \n",
    "            # Calculate KL divergence in both directions\n",
    "            # Current model â†’ peer\n",
    "            self_soft = F.log_softmax(logits / temp_self, dim=1)\n",
    "            peer_soft = F.softmax(peer_output.detach() / temp_peer, dim=1)  # detach to prevent backprop to peer\n",
    "            kl_loss_to_peer = F.kl_div(self_soft, peer_soft, reduction='batchmean') * (temp_self ** 2)\n",
    "            \n",
    "            mutual_losses.append(kl_loss_to_peer)\n",
    "        \n",
    "        # Average mutual losses if there are peers\n",
    "        if mutual_losses:\n",
    "            mutual_loss = sum(mutual_losses) / len(mutual_losses)\n",
    "        else:\n",
    "            mutual_loss = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        # Calibration loss\n",
    "        cal_loss = CalibrationMetrics.calibration_loss(logits, targets)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * mutual_loss + cal_weight * cal_loss + feature_weight * feature_loss\n",
    "        \n",
    "        return total_loss, ce_loss, mutual_loss, cal_loss, feature_loss\n",
    "\n",
    "# Data Preparation\n",
    "def get_cifar10_loaders(config):\n",
    "    \"\"\"Prepare CIFAR-10 dataset and dataloaders with model-specific transforms/batch sizes if enabled\"\"\"\n",
    "    # ImageNet normalization\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    # Build per-model transforms if enabled\n",
    "    if config.use_model_specific_transforms:\n",
    "        train_transforms = {}\n",
    "        test_transforms = {}\n",
    "        for model_name in config.models:\n",
    "            input_size = config.get_input_size(model_name)\n",
    "            # Resize first, then RandomCrop, then normalization\n",
    "            train_transforms[model_name] = transforms.Compose([\n",
    "                transforms.Resize(input_size + 8, antialias=True),\n",
    "                transforms.RandomCrop(input_size, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "            test_transforms[model_name] = transforms.Compose([\n",
    "                transforms.Resize(input_size, antialias=True),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "    else:\n",
    "        # Use global transform\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(config.model_input_size + 8, antialias=True),\n",
    "            transforms.RandomCrop(config.model_input_size, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(config.model_input_size, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    cifar10_path = os.path.join(config.dataset_path, \"CIFAR-10\")\n",
    "    \n",
    "    # Download dataset once to avoid duplicate messages\n",
    "    datasets.CIFAR10(root=cifar10_path, train=True, download=True)\n",
    "    datasets.CIFAR10(root=cifar10_path, train=False, download=True)\n",
    "    \n",
    "    # Build per-model datasets/loaders if enabled\n",
    "    if config.use_model_specific_transforms or config.use_model_specific_batch_size:\n",
    "        train_loader_per_model = {}\n",
    "        val_loader_per_model = {}\n",
    "        test_loader_per_model = {}\n",
    "        for model_name in config.models:\n",
    "            # Datasets - use download=False since we've already downloaded above\n",
    "            full_train_dataset = datasets.CIFAR10(\n",
    "                root=cifar10_path, train=True, download=False, transform=train_transforms[model_name] if config.use_model_specific_transforms else train_transform\n",
    "            )\n",
    "            test_dataset = datasets.CIFAR10(\n",
    "                root=cifar10_path, train=False, download=False, transform=test_transforms[model_name] if config.use_model_specific_transforms else test_transform\n",
    "            )\n",
    "            val_size = int(len(full_train_dataset) * config.val_split)\n",
    "            train_size = len(full_train_dataset) - val_size\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "                full_train_dataset, [train_size, val_size],\n",
    "                generator=torch.Generator().manual_seed(config.seed)\n",
    "            )\n",
    "            val_dataset_with_transform = torch.utils.data.Subset(\n",
    "                datasets.CIFAR10(\n",
    "                    root=cifar10_path, train=True, download=False, transform=test_transforms[model_name] if config.use_model_specific_transforms else test_transform\n",
    "                ),\n",
    "                val_dataset.indices\n",
    "            )\n",
    "            batch_size = config.get_batch_size(model_name)\n",
    "            train_loader_per_model[model_name] = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=config.num_workers,\n",
    "                pin_memory=config.pin_memory,\n",
    "                persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "                prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "            )\n",
    "            val_loader_per_model[model_name] = DataLoader(\n",
    "                val_dataset_with_transform,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=config.num_workers,\n",
    "                pin_memory=config.pin_memory,\n",
    "                persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "                prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "            )\n",
    "            test_loader_per_model[model_name] = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=config.num_workers,\n",
    "                pin_memory=config.pin_memory,\n",
    "                persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "                prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "            )\n",
    "        logger.info(f\"Training samples: {len(train_dataset)} per model\")\n",
    "        logger.info(f\"Validation samples: {len(val_dataset)} per model\")\n",
    "        logger.info(f\"Test samples: {len(test_dataset)} per model\")\n",
    "        return train_loader_per_model, val_loader_per_model, test_loader_per_model\n",
    "    else:\n",
    "        # Use single loader with download=False since we've already downloaded\n",
    "        full_train_dataset = datasets.CIFAR10(\n",
    "            root=cifar10_path, train=True, download=False, transform=train_transform\n",
    "        )\n",
    "        test_dataset = datasets.CIFAR10(\n",
    "            root=cifar10_path, train=False, download=False, transform=test_transform\n",
    "        )\n",
    "        val_size = int(len(full_train_dataset) * config.val_split)\n",
    "        train_size = len(full_train_dataset) - val_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            full_train_dataset, [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(config.seed)\n",
    "        )\n",
    "        val_dataset_with_transform = torch.utils.data.Subset(\n",
    "            datasets.CIFAR10(\n",
    "                root=cifar10_path, train=True, download=False, transform=test_transform\n",
    "            ),\n",
    "            val_dataset.indices\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=config.pin_memory,\n",
    "            persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "            prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset_with_transform,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=config.pin_memory,\n",
    "            persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "            prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=config.pin_memory,\n",
    "            persistent_workers=config.persistent_workers if config.num_workers > 0 else False,\n",
    "            prefetch_factor=config.prefetch_factor if config.num_workers > 0 else None\n",
    "        )\n",
    "        logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "        logger.info(f\"Validation samples: {len(val_dataset)}\")\n",
    "        logger.info(f\"Test samples: {len(test_dataset)}\")\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "# Model Creation Functions - Modified for mutual learning\n",
    "def create_models(config):\n",
    "    \"\"\"Create or load all models for mutual learning (refactored for consistency with individual model scripts)\"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # ViT-B16\n",
    "    logger.info(\"Loading ViT-B16 model...\")\n",
    "    if config.pretrained:\n",
    "        vit_model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        vit_model = vit_b_16()\n",
    "    in_features = vit_model.heads.head.in_features\n",
    "    vit_model.heads.head = nn.Linear(in_features, config.num_classes)\n",
    "    vit_model.custom_lr = config.model_specific_lr.get('vit', config.lr)\n",
    "    models['vit'] = vit_model\n",
    "\n",
    "    # EfficientNetB0\n",
    "    logger.info(\"Loading EfficientNetB0 model...\")\n",
    "    if config.pretrained:\n",
    "        effnet = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        effnet = efficientnet_b0()\n",
    "    in_features = effnet.classifier[1].in_features\n",
    "    effnet.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features, config.num_classes)\n",
    "    )\n",
    "    effnet.custom_lr = config.model_specific_lr.get('efficientnet', config.lr)\n",
    "    models['efficientnet'] = effnet\n",
    "\n",
    "    # InceptionV3 (wrapped)\n",
    "    logger.info(\"Loading InceptionV3 model with safe wrapper for small inputs...\")\n",
    "    if config.pretrained:\n",
    "        weights = Inception_V3_Weights.IMAGENET1K_V1\n",
    "        inception = inception_v3(weights=weights, aux_logits=True) # Keep aux_logits=True for consistency if pretrained has them\n",
    "    else:\n",
    "        inception = inception_v3(aux_logits=True) # Keep aux_logits=True\n",
    "\n",
    "    in_features = inception.fc.in_features\n",
    "    inception.fc = nn.Linear(in_features, config.num_classes)\n",
    "    # Adapt AuxLogits as well, mirroring InceptionV3.py teacher script\n",
    "    if hasattr(inception, 'AuxLogits') and inception.AuxLogits is not None:\n",
    "        logger.info(\"Adapting InceptionV3 AuxLogits layer...\")\n",
    "        aux_in_features = inception.AuxLogits.fc.in_features\n",
    "        inception.AuxLogits.fc = nn.Linear(aux_in_features, config.num_classes)\n",
    "    else:\n",
    "         logger.info(\"InceptionV3 AuxLogits not found or not enabled, skipping adaptation.\")\n",
    "\n",
    "    # Wrap the adapted model\n",
    "    models['inception'] = InceptionV3Wrapper(inception)\n",
    "    # Assign custom LR to the wrapper instance\n",
    "    models['inception'].custom_lr = config.model_specific_lr.get('inception', config.lr)\n",
    "\n",
    "    # MobileNetV3-Large\n",
    "    logger.info(\"Loading MobileNetV3-Large model...\")\n",
    "    if config.pretrained:\n",
    "        mobilenet = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        mobilenet = mobilenet_v3_large()\n",
    "    in_features = mobilenet.classifier[-1].in_features\n",
    "    mobilenet.classifier[-1] = nn.Linear(in_features, config.num_classes)\n",
    "    mobilenet.custom_lr = config.model_specific_lr.get('mobilenet', config.lr)\n",
    "    models['mobilenet'] = mobilenet\n",
    "\n",
    "    # ResNet50\n",
    "    logger.info(\"Loading ResNet50 model...\")\n",
    "    if config.pretrained:\n",
    "        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        resnet = resnet50()\n",
    "    in_features = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(in_features, config.num_classes)\n",
    "    resnet.custom_lr = config.model_specific_lr.get('resnet', config.lr)\n",
    "    models['resnet'] = resnet\n",
    "\n",
    "    # DenseNet121\n",
    "    logger.info(\"Loading DenseNet121 model...\")\n",
    "    if config.pretrained:\n",
    "        densenet = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        densenet = densenet121()\n",
    "    in_features = densenet.classifier.in_features\n",
    "    densenet.classifier = nn.Linear(in_features, config.num_classes)\n",
    "    densenet.custom_lr = config.model_specific_lr.get('densenet', config.lr)\n",
    "    models['densenet'] = densenet\n",
    "\n",
    "    # Student (EfficientNetB0-based)\n",
    "    logger.info(\"Creating student model (EfficientNetB0-based)...\")\n",
    "    if config.pretrained:\n",
    "        student = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        student = efficientnet_b0()\n",
    "    in_features = student.classifier[1].in_features\n",
    "    student.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features, config.num_classes)\n",
    "    )\n",
    "    student.custom_lr = config.model_specific_lr.get('student', config.lr)\n",
    "    models['student'] = student\n",
    "\n",
    "    # Optionally load pre-trained weights\n",
    "    if config.use_pretrained_models:\n",
    "        for name, model_path in config.pretrained_model_paths.items():\n",
    "            if name in models and os.path.exists(model_path):\n",
    "                logger.info(f\"Loading pre-trained weights for {name} from {model_path}\")\n",
    "                try:\n",
    "                    checkpoint = torch.load(model_path, map_location=device)\n",
    "                    if 'model_state_dict' in checkpoint:\n",
    "                        models[name].load_state_dict(checkpoint['model_state_dict'])\n",
    "                    elif 'state_dict' in checkpoint:\n",
    "                        models[name].load_state_dict(checkpoint['state_dict'])\n",
    "                    else:\n",
    "                        models[name].load_state_dict(checkpoint)\n",
    "                    logger.info(f\"Successfully loaded pre-trained weights for {name}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading weights for {name}: {str(e)}\")\n",
    "\n",
    "    # Move all models to device\n",
    "    for name, model in models.items():\n",
    "        models[name] = model.to(device)\n",
    "        logger.info(f\"Model {name} moved to {device}\")\n",
    "\n",
    "    # Log model parameters\n",
    "    for name, model in models.items():\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Model {name}: {total_params/1e6:.2f}M parameters ({trainable_params/1e6:.2f}M trainable)\")\n",
    "\n",
    "    return models\n",
    "\n",
    "# Feature Extraction for Feature Alignment Loss\n",
    "def setup_feature_extractors(models, config):\n",
    "    \"\"\"Setup feature extractors for all models\"\"\"\n",
    "    feature_extractors = {}\n",
    "    \n",
    "    # Define feature extraction layers for each model\n",
    "    feature_layers = {\n",
    "        'vit': 'encoder.ln',  # For torchvision ViT\n",
    "        'efficientnet': 'features.8',  # For torchvision EfficientNet\n",
    "        'inception': 'Mixed_7c',\n",
    "        'mobilenet': 'features',\n",
    "        'resnet': 'layer4',\n",
    "        'densenet': 'features',\n",
    "        'student': 'features.8'  # Same as efficientnet\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        layer_name = feature_layers.get(name)\n",
    "        if layer_name:\n",
    "            feature_extractors[name] = FeatureExtractor(model, layer_name)\n",
    "            if feature_extractors[name].hook_registered:\n",
    "                logger.info(f\"Feature extractor registered for {name} at layer {layer_name}\")\n",
    "                # Log feature shape for debugging\n",
    "                with torch.no_grad():\n",
    "                    dummy_input = torch.randn(1, 3, config.model_input_size, config.model_input_size).to(device)\n",
    "                    _ = model(dummy_input)\n",
    "                    if feature_extractors[name].features is not None:\n",
    "                        feature_shape = feature_extractors[name].features.shape\n",
    "                        logger.info(f\"Feature shape for {name}: {feature_shape}\")\n",
    "            else:\n",
    "                logger.warning(f\"Feature extractor failed for {name} at layer {layer_name}\")\n",
    "    \n",
    "    return feature_extractors\n",
    "\n",
    "def initialization_phase(models, train_loader, val_loader, config, optimizers, schedulers, start_epoch=0):\n",
    "    \"\"\" Phase 1: Initialize each model separately. Accepts optimizers and schedulers. \"\"\"\n",
    "    logger.info(\"Starting initialization phase...\")\n",
    "    best_val_acc = {name: 0.0 for name in models.keys()}\n",
    "    best_states = {name: None for name in models.keys()}\n",
    "    # Optimizers and schedulers are now passed in, no need to recreate them here.\n",
    "    scalers = {name: GradScaler(enabled=config.use_amp) for name in models.keys()}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        lr = config.get_learning_rate(name)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=config.weight_decay)\n",
    "        # Simple scheduler for initialization phase, e.g., StepLR or None\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95) # Example: decay LR slightly each epoch\n",
    "        optimizers[name] = optimizer\n",
    "        schedulers[name] = scheduler\n",
    "        logger.info(f\"Optimizer and Scheduler set for {name} with LR={lr}\")\n",
    "\n",
    "\n",
    "    # Create AMP GradScaler for each model\n",
    "    scalers = {name: GradScaler() if config.use_amp else None for name in models.keys()}\n",
    "\n",
    "    # Standard cross entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop for initialization phase\n",
    "    for epoch in range(start_epoch, config.initialization_epochs):\n",
    "        logger.info(f\"Initialization Epoch {epoch + 1}/{config.initialization_epochs}\")\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Store losses/accuracies for this epoch\n",
    "        epoch_train_loss = {name: 0.0 for name in models.keys()}\n",
    "        epoch_train_correct = {name: 0 for name in models.keys()}\n",
    "        epoch_train_total = {name: 0 for name in models.keys()}\n",
    "\n",
    "        use_per_model_loader = isinstance(train_loader, dict)\n",
    "\n",
    "        # Iterate through each model for training\n",
    "        for name, model in models.items():\n",
    "            model.train()\n",
    "            optimizer = optimizers[name]\n",
    "            scheduler = schedulers[name] # Get the scheduler for this model\n",
    "            scaler = scalers[name]\n",
    "\n",
    "            # Select the correct dataloader\n",
    "            current_train_loader = train_loader[name] if use_per_model_loader else train_loader\n",
    "            if not current_train_loader:\n",
    "                 logger.warning(f\"No train loader found for {name}, skipping training for this model.\")\n",
    "                 continue\n",
    "            grad_accum_steps = config.get_grad_accum_steps(name) # Get model-specific grad accum\n",
    "\n",
    "            optimizer.zero_grad() # Zero grad at the start of the model's epoch pass\n",
    "\n",
    "            # Use tqdm for the dataloader of the current model\n",
    "            pbar = tqdm(enumerate(current_train_loader), total=len(current_train_loader), desc=f\"Train {name} E{epoch+1}\")\n",
    "            for batch_idx, batch_data in pbar:\n",
    "                # Ensure batch_data is correctly unpacked\n",
    "                if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "                    inputs, targets = batch_data\n",
    "                    if not isinstance(inputs, torch.Tensor) or not isinstance(targets, torch.Tensor):\n",
    "                         logger.warning(f\"Skipping batch {batch_idx} for {name}: inputs or targets are not tensors (types: {type(inputs)}, {type(targets)})\")\n",
    "                         continue\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping batch {batch_idx} for {name}: Unexpected data format (type: {type(batch_data)})\")\n",
    "                    continue\n",
    "\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                with autocast(device_type='cuda', enabled=config.use_amp, dtype=torch.float16 if config.mixed_precision_dtype == 'float16' else torch.bfloat16): # Add device_type\n",
    "                    outputs = model(inputs)\n",
    "                    # Handle potential tuple output from InceptionV3 if not wrapped correctly (though wrapper should prevent this)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0] # Assume primary output is first\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss = loss / grad_accum_steps # Scale loss for gradient accumulation\n",
    "\n",
    "                if scaler:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                # Optimizer step after accumulation\n",
    "                if (batch_idx + 1) % grad_accum_steps == 0 or (batch_idx + 1) == len(current_train_loader):\n",
    "                    if scaler:\n",
    "                        # Unscale before clipping\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        # Optional: Gradient Clipping\n",
    "                        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        # Optional: Gradient Clipping\n",
    "                        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                    optimizer.zero_grad() # Zero grad after step\n",
    "\n",
    "                # Update metrics (use loss without scaling for logging)\n",
    "                # Recompute loss outside autocast/grad scaling for stable logging value\n",
    "                with torch.no_grad():\n",
    "                     log_loss = criterion(outputs, targets).item()\n",
    "                epoch_train_loss[name] += log_loss\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                epoch_train_total[name] += targets.size(0)\n",
    "                epoch_train_correct[name] += (predicted == targets).sum().item()\n",
    "\n",
    "                # Update tqdm\n",
    "                current_loss_avg = epoch_train_loss[name] / (batch_idx + 1) if (batch_idx + 1) > 0 else 0\n",
    "                current_acc_avg = 100. * epoch_train_correct[name] / epoch_train_total[name] if epoch_train_total[name] > 0 else 0\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f\"{current_loss_avg:.4f}\",\n",
    "                    'Acc': f\"{current_acc_avg:.2f}%\"\n",
    "                })\n",
    "\n",
    "                # Optional: Clear cache periodically\n",
    "                if (batch_idx + 1) % config.clear_cache_every_n_batches == 0:\n",
    "                    clear_gpu_cache()\n",
    "\n",
    "            # End of batches for one model\n",
    "            if scheduler: # Step scheduler per model after its batches are done\n",
    "                 scheduler.step()\n",
    "\n",
    "        # --- Validation ---\n",
    "        epoch_val_loss = {name: 0.0 for name in models.keys()}\n",
    "        epoch_val_correct = {name: 0 for name in models.keys()}\n",
    "        epoch_val_total = {name: 0 for name in models.keys()}\n",
    "\n",
    "        # Store original num_workers setting\n",
    "        original_num_workers = config.num_workers\n",
    "        original_persistent_workers = config.persistent_workers\n",
    "\n",
    "        try: # Use try/finally to ensure settings are restored\n",
    "            # --- ADDED FIX: Temporarily disable workers for validation ---\n",
    "            config.num_workers = 0\n",
    "            config.persistent_workers = False # Must be False if num_workers is 0\n",
    "            logger.info(f\"Temporarily setting num_workers=0 for validation to reduce RAM usage.\")\n",
    "            # --- END OF FIX ---\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model.eval()\n",
    "\n",
    "                # --- MODIFIED DATALOADER CREATION ---\n",
    "                # Recreate val_loader instance with num_workers=0\n",
    "                # Assuming val_loader is a dict of datasets or similar structure was passed\n",
    "                # Need the actual dataset object used for validation\n",
    "                # Let's assume 'val_loader' passed to the function holds dataset info or is the loader dict\n",
    "                current_val_dataset = None\n",
    "                if use_per_model_loader and isinstance(val_loader, dict) and name in val_loader:\n",
    "                     # If val_loader is a dict of loaders, get the dataset from it\n",
    "                     if hasattr(val_loader[name], 'dataset'):\n",
    "                         current_val_dataset = val_loader[name].dataset\n",
    "                     else:\n",
    "                         logger.warning(f\"Could not extract dataset from val_loader dict for {name}. Skipping validation.\")\n",
    "                         continue\n",
    "                elif not use_per_model_loader and hasattr(val_loader, 'dataset'):\n",
    "                     # If val_loader is a single loader, get the dataset\n",
    "                     current_val_dataset = val_loader.dataset\n",
    "                else:\n",
    "                    # Fallback or error if we can't get the dataset\n",
    "                    logger.error(f\"Cannot determine validation dataset for {name}. Skipping validation.\")\n",
    "                    continue\n",
    "\n",
    "                # Create a new DataLoader instance for validation with num_workers=0\n",
    "                temp_val_loader = DataLoader(\n",
    "                    current_val_dataset,\n",
    "                    batch_size=config.get_batch_size(name), # Use model-specific batch size\n",
    "                    shuffle=False,\n",
    "                    num_workers=config.num_workers, # This is now 0\n",
    "                    pin_memory=config.pin_memory, # pin_memory is okay with num_workers=0\n",
    "                    persistent_workers=config.persistent_workers # This is now False\n",
    "                    # prefetch_factor is ignored when num_workers=0\n",
    "                )\n",
    "                # --- END OF MODIFIED DATALOADER CREATION ---\n",
    "\n",
    "\n",
    "                if not temp_val_loader: # Should not happen if dataset was found\n",
    "                     logger.warning(f\"Temporary validation loader creation failed for {name}.\")\n",
    "                     continue\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pbar_val = tqdm(enumerate(temp_val_loader), total=len(temp_val_loader), desc=f\"Val {name} E{epoch+1}\")\n",
    "                    for batch_idx, batch_data in pbar_val:\n",
    "                        # (Rest of the inner validation loop remains the same...)\n",
    "                        # Ensure batch_data is correctly unpacked\n",
    "                        if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "                            inputs, targets = batch_data\n",
    "                            if not isinstance(inputs, torch.Tensor) or not isinstance(targets, torch.Tensor):\n",
    "                                 logger.warning(f\"Skipping validation batch {batch_idx} for {name}: inputs or targets are not tensors (types: {type(inputs)}, {type(targets)})\")\n",
    "                                 continue\n",
    "                        else:\n",
    "                            logger.warning(f\"Skipping validation batch {batch_idx} for {name}: Unexpected data format (type: {type(batch_data)})\")\n",
    "                            continue\n",
    "\n",
    "                        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                        with autocast(device_type='cuda', enabled=config.use_amp, dtype=torch.float16 if config.mixed_precision_dtype == 'float16' else torch.bfloat16):\n",
    "                            outputs = model(inputs)\n",
    "                            if isinstance(outputs, tuple):\n",
    "                                outputs = outputs[0]\n",
    "                            loss = criterion(outputs, targets) # Use the main criterion\n",
    "\n",
    "                        epoch_val_loss[name] += loss.item()\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        epoch_val_total[name] += targets.size(0)\n",
    "                        epoch_val_correct[name] += (predicted == targets).sum().item()\n",
    "\n",
    "                        current_loss_avg = epoch_val_loss[name] / epoch_val_total[name] if epoch_val_total[name] > 0 else 0\n",
    "                        current_acc_avg = 100. * epoch_val_correct[name] / epoch_val_total[name] if epoch_val_total[name] > 0 else 0\n",
    "                        pbar_val.set_postfix({\n",
    "                            'Loss': f\"{current_loss_avg:.4f}\",\n",
    "                            'Acc': f\"{current_acc_avg:.2f}%\"\n",
    "                        })\n",
    "\n",
    "                # Clear GPU cache after validating each model (from previous fix)\n",
    "                clear_gpu_cache()\n",
    "\n",
    "        finally:\n",
    "             # --- ADDED FIX: Restore original worker settings ---\n",
    "             config.num_workers = original_num_workers\n",
    "             config.persistent_workers = original_persistent_workers\n",
    "             logger.info(f\"Restored num_workers to {config.num_workers}.\")\n",
    "             # --- END OF FIX ---\n",
    "\n",
    "        # Log results after validating all models\n",
    "        for name in models.keys():\n",
    "            # Check if loader exists and has data before calculating averages\n",
    "            # Use epoch_val_total[name] which counts processed samples\n",
    "            val_loader_len = len(val_loader[name] if use_per_model_loader else val_loader) if (val_loader[name] if use_per_model_loader else val_loader) else 0\n",
    "            processed_samples = epoch_val_total[name]\n",
    "\n",
    "            # Use processed_samples for averaging to avoid division by zero if loader was empty/skipped\n",
    "            train_loss_avg = epoch_train_loss[name] / len(train_loader[name] if use_per_model_loader else train_loader) if len(train_loader[name] if use_per_model_loader else train_loader) > 0 else 0\n",
    "            train_acc_avg = 100. * epoch_train_correct[name] / epoch_train_total[name] if epoch_train_total[name] > 0 else 0\n",
    "            val_loss_avg = epoch_val_loss[name] / processed_samples if processed_samples > 0 else 0 # Average loss over processed samples\n",
    "            val_acc_avg = 100. * epoch_val_correct[name] / processed_samples if processed_samples > 0 else 0 # Average accuracy over processed samples\n",
    "\n",
    "            logger.info(f\"  {name}: Train Loss={train_loss_avg:.4f}, Train Acc={train_acc_avg:.2f}%, Val Loss={val_loss_avg:.4f}, Val Acc={val_acc_avg:.2f}%\")\n",
    "\n",
    "            # Update best model state only if validation was performed and samples were processed\n",
    "            if processed_samples > 0 and val_acc_avg > best_val_acc[name]:\n",
    "                best_val_acc[name] = val_acc_avg\n",
    "                # Use deepcopy to store the state to avoid issues with shared references\n",
    "                best_states[name] = copy.deepcopy(model.state_dict())\n",
    "                logger.info(f\"  New best validation accuracy for {name}: {val_acc_avg:.2f}%\")\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        logger.info(f\"Initialization epoch {epoch + 1} completed in {epoch_duration:.2f}s\")\n",
    "\n",
    "        save_checkpoint(models, optimizers, schedulers, epoch, config, # Pass epoch (0-based index of completed epoch)\n",
    "                        filename=f\"init_checkpoint_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "    # Restore best states\n",
    "    for name, model in models.items():\n",
    "        if best_states[name] is not None:\n",
    "            try:\n",
    "                # --- MODIFIED: Use standard load_state_dict ---\n",
    "                # Load state dict with strict=False to ignore minor mismatches if any\n",
    "                missing_keys, unexpected_keys = model.load_state_dict(best_states[name], strict=False)\n",
    "                if missing_keys:\n",
    "                    logger.warning(f\"Missing keys when restoring best state for {name}: {missing_keys}\")\n",
    "                if unexpected_keys:\n",
    "                    logger.warning(f\"Unexpected keys when restoring best state for {name}: {unexpected_keys}\")\n",
    "                # --- END MODIFICATION ---\n",
    "                logger.info(f\"Restored best state for {name} from initialization (Val Acc: {best_val_acc[name]:.2f}%)\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to restore best state for {name} after initialization: {e}\")\n",
    "                logger.error(traceback.format_exc()) # Log traceback for debugging\n",
    "        else:\n",
    "            logger.warning(f\"No best state found for {name} during initialization.\")\n",
    "\n",
    "    logger.info(\"Initialization phase completed\")\n",
    "    return models # Return models with potentially updated states\n",
    "\n",
    "\n",
    "# Mutual Learning Phase\n",
    "def mutual_learning_phase(models, train_loader, val_loader, config, optimizers, schedulers, start_epoch=0):\n",
    "    \"\"\" Phase 2: Train all models mutually. Accepts optimizers and schedulers. \"\"\"\n",
    "    logger.info(\"Starting mutual learning phase...\")\n",
    "\n",
    "    # --- Setup ---\n",
    "    use_per_model_loader = isinstance(train_loader, dict)\n",
    "    shared_train_loader = train_loader.get('student') if use_per_model_loader else train_loader # Simplified example\n",
    "    shared_val_loader = val_loader.get('student') if isinstance(val_loader, dict) else val_loader # Simplified example\n",
    "    if not shared_train_loader: raise ValueError(\"No shared train loader found\")\n",
    "    if not shared_val_loader: raise ValueError(\"No shared val loader found\")\n",
    "\n",
    "    feature_extractors = setup_feature_extractors(models, config)\n",
    "    teacher_names = [name for name in config.models if name != 'student']\n",
    "    ref_input_size = config.get_input_size('student')\n",
    "    dummy_input = torch.randn(2, 3, ref_input_size, ref_input_size).to(device)\n",
    "    teacher_feature_shapes = {}\n",
    "    student_feature_shape = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "         for name, model in models.items():\n",
    "            model.eval()\n",
    "            try:\n",
    "                model_input_size = config.get_input_size(name)\n",
    "                current_dummy_input = F.interpolate(dummy_input, size=(model_input_size, model_input_size), mode='bilinear', align_corners=False)\n",
    "                if name in feature_extractors and feature_extractors[name].hook_registered:\n",
    "                     _ = model(current_dummy_input); features = feature_extractors[name].features\n",
    "                     if features is not None:\n",
    "                         shape = features.shape[1:]\n",
    "                         if name == 'student': student_feature_shape = shape\n",
    "                         else: teacher_feature_shapes[name] = shape\n",
    "            except Exception as e: logger.error(f\"Error during dummy pass for {name}: {e}\")\n",
    "            model.train()\n",
    "\n",
    "\n",
    "    # Initialize HFI only if teacher and student shapes are available\n",
    "    hfi = None\n",
    "    if config.feature_loss_weight > 0 and teacher_feature_shapes and student_feature_shape:\n",
    "        try:\n",
    "            hfi = HeterogeneousFeatureIntegrator(teacher_feature_shapes, student_feature_shape).to(device)\n",
    "            logger.info(\"HFI initialized.\")\n",
    "            # Add HFI params to student optimizer if needed (or create separate)\n",
    "            if 'student' in optimizers:\n",
    "                hfi_lr = config.get_learning_rate('student')\n",
    "                optimizers['student'].add_param_group({'params': hfi.parameters(), 'lr': hfi_lr})\n",
    "                logger.info(\"Added HFI params to student optimizer.\")\n",
    "                # If HFI uses student's optimizer, its scheduler is handled by the student's scheduler.\n",
    "            else: # Create separate optimizer/scheduler if student optimizer doesn't exist\n",
    "                logger.warning(\"Student optimizer not found for HFI params.\")\n",
    "                # Potentially create separate HFI optimizer/scheduler here and add to dicts\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize HFI: {e}. Disabling feature loss.\")\n",
    "            hfi = None; config.feature_loss_weight = 0\n",
    "    else:\n",
    "        logger.info(\"HFI not initialized/required.\")\n",
    "        config.feature_loss_weight = 0\n",
    "\n",
    "\n",
    "    # Setup mutual learning loss\n",
    "    ml_loss = MutualLearningLoss(config).to(device)\n",
    "    best_val_metrics = {name: {'acc': 0.0, 'ece': float('inf')} for name in models.keys()}\n",
    "    # Move state dict to CPU before deep copying to save GPU memory\n",
    "    # Get state dict (on GPU), then move tensors to CPU before deep copying\n",
    "    best_states = {}\n",
    "    for name, model in models.items():\n",
    "        # 1. Get state dict (likely on GPU)\n",
    "        state_dict_gpu = model.state_dict()\n",
    "        # 2. Create a new state dict, moving each tensor to CPU\n",
    "        state_dict_cpu = {k: v.cpu() for k, v in state_dict_gpu.items()}\n",
    "        # 3. Deep copy the CPU state dict\n",
    "        best_states[name] = copy.deepcopy(state_dict_cpu)\n",
    "        # Optional: Clear the intermediate dicts if memory is extremely tight, though CPU RAM is usually plentiful\n",
    "        del state_dict_gpu\n",
    "        del state_dict_cpu\n",
    "        gc.collect() # Add garbage collection if needed\n",
    "    best_epoch = {name: 0 for name in models.keys()}\n",
    "    early_stop_counter = {name: 0 for name in models.keys()}\n",
    "    cal_stop_counter = {name: 0 for name in models.keys()}\n",
    "\n",
    "\n",
    "    scalers = {name: GradScaler(enabled=config.use_amp) for name in optimizers.keys()} # Create scalers based on passed optimizers\n",
    "\n",
    "    for name, model in models.items():\n",
    "        lr = config.get_learning_rate(name)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=config.weight_decay)\n",
    "        optimizers[name] = optimizer\n",
    "\n",
    "        # --- LR Scheduler Setup with Warmup ---\n",
    "        # Ensure T_max is at least 1 for CosineAnnealingLR\n",
    "        cosine_t_max = max(1, config.mutual_learning_epochs - config.warmup_epochs)\n",
    "        base_scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cosine_t_max, # T_max for cosine part\n",
    "            eta_min=lr * 0.01 # Minimum learning rate\n",
    "        )\n",
    "        if config.use_warmup and config.warmup_epochs > 0:\n",
    "            # Ensure total_iters is at least 1 for LinearLR\n",
    "            warmup_total_iters = max(1, config.warmup_epochs)\n",
    "            warmup_scheduler = LinearLR(\n",
    "                optimizer,\n",
    "                start_factor=1e-3, # Start LR multiplier (Consider adjusting this, e.g., 0.1)\n",
    "                total_iters=warmup_total_iters # Number of warmup epochs\n",
    "            )\n",
    "            # Chain warmup and cosine decay\n",
    "            # Ensure milestones list is valid even if warmup_epochs is 0 or 1\n",
    "            milestone_epoch = max(1, config.warmup_epochs)\n",
    "            scheduler = SequentialLR(\n",
    "                optimizer,\n",
    "                schedulers=[warmup_scheduler, base_scheduler],\n",
    "                milestones=[milestone_epoch] # Switch schedulers after warmup_epochs\n",
    "            )\n",
    "            logger.info(f\"Using LR warmup for {config.warmup_epochs} epochs for model {name}.\")\n",
    "        else:\n",
    "            scheduler = base_scheduler # No warmup\n",
    "            logger.info(f\"No LR warmup used for model {name}.\")\n",
    "\n",
    "        schedulers[name] = scheduler\n",
    "\n",
    "\n",
    "    # Add HFI parameters to an optimizer and create its scheduler\n",
    "    if hfi:\n",
    "        hfi_lr = config.get_learning_rate('student') # Default to student LR for HFI\n",
    "        hfi_optimizer_target = None\n",
    "\n",
    "        if 'student' in optimizers:\n",
    "            # Add HFI parameters to the student's optimizer\n",
    "            optimizers['student'].add_param_group({'params': hfi.parameters(), 'lr': hfi_lr})\n",
    "            hfi_optimizer_target = optimizers['student'] # HFI uses student's optimizer\n",
    "            logger.info(\"Added HFI parameters to the student optimizer.\")\n",
    "        else:\n",
    "            # Create a separate optimizer if student doesn't exist\n",
    "            hfi_optimizer = torch.optim.AdamW(hfi.parameters(), lr=hfi_lr, weight_decay=config.weight_decay)\n",
    "            optimizers['hfi'] = hfi_optimizer # Store separately\n",
    "            hfi_optimizer_target = hfi_optimizer # HFI uses its own optimizer\n",
    "            logger.warning(\"HFI parameters added to a separate optimizer 'hfi'.\")\n",
    "\n",
    "        # Create scheduler for HFI parameters, whether separate or part of student's optimizer\n",
    "        if hfi_optimizer_target:\n",
    "            # Use the same warmup/cosine logic for HFI scheduler\n",
    "            cosine_t_max_hfi = max(1, config.mutual_learning_epochs - config.warmup_epochs)\n",
    "            base_scheduler_hfi = CosineAnnealingLR(\n",
    "                hfi_optimizer_target, # Use the target optimizer\n",
    "                T_max=cosine_t_max_hfi,\n",
    "                eta_min=hfi_lr * 0.01\n",
    "            )\n",
    "            if config.use_warmup and config.warmup_epochs > 0:\n",
    "                warmup_total_iters_hfi = max(1, config.warmup_epochs)\n",
    "                warmup_scheduler_hfi = LinearLR(\n",
    "                    hfi_optimizer_target,\n",
    "                    start_factor=1e-3, # Consider adjusting\n",
    "                    total_iters=warmup_total_iters_hfi\n",
    "                )\n",
    "                milestone_epoch_hfi = max(1, config.warmup_epochs)\n",
    "                # If HFI has its own optimizer, store scheduler under 'hfi' key\n",
    "                # If HFI params are in student's optimizer, this scheduler will run alongside student's main scheduler,\n",
    "                # which is generally okay for SequentialLR, but be mindful if using other schedulers.\n",
    "                # For simplicity here, we create it but might need adjustment depending on exact behavior desired.\n",
    "                # A common approach is to just let the main optimizer's scheduler handle all param groups.\n",
    "                # However, explicitly creating it allows separate tracking/potential modification.\n",
    "                # Let's store it separately if a separate HFI optimizer exists.\n",
    "                scheduler_key = 'hfi' if 'hfi' in optimizers else 'student_hfi_part' # Use a distinct key if needed\n",
    "                if scheduler_key == 'hfi': # Only store if separate optimizer\n",
    "                     schedulers[scheduler_key] = SequentialLR(\n",
    "                        hfi_optimizer_target,\n",
    "                        schedulers=[warmup_scheduler_hfi, base_scheduler_hfi],\n",
    "                        milestones=[milestone_epoch_hfi]\n",
    "                     )\n",
    "                     logger.info(f\"Created separate LR scheduler for HFI optimizer with {config.warmup_epochs} warmup epochs.\")\n",
    "                # If attached to student optimizer, the student's main scheduler already covers these params.\n",
    "                # No need to add another scheduler object for the same optimizer instance.\n",
    "\n",
    "            elif 'hfi' in optimizers: # No warmup, separate HFI optimizer\n",
    "                 schedulers['hfi'] = base_scheduler_hfi\n",
    "                 logger.info(\"Created separate LR scheduler for HFI optimizer (no warmup).\")\n",
    "\n",
    "\n",
    "    # Create AMP GradScaler for each model\n",
    "    scalers = {name: GradScaler(enabled=config.use_amp) for name in models.keys()}\n",
    "    if hfi and 'hfi' in optimizers: # Add scaler for separate HFI optimizer if exists\n",
    "        scalers['hfi'] = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "\n",
    "    # Feature alignment loss\n",
    "    feature_loss_fn = FeatureAlignmentLoss().to(device) # Ensure loss module is on device\n",
    "\n",
    "    # Set up validation criterion\n",
    "    val_criterion = nn.CrossEntropyLoss()\n",
    "    history = {name: {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'ce_loss': [], 'mutual_loss': [], 'cal_loss': [], 'feature_loss': [], 'val_ece': []} for name in models.keys()}\n",
    "    history.update({'epochs': [], 'cal_weights': [], 'mutual_weights': [], 'feature_weights': [], 'temperatures': [], 'hfi_attention': []})\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Config already saved in main\n",
    "    try:\n",
    "        # Get batch size from the loader instance if possible\n",
    "        mutual_batch_size = shared_train_loader.batch_size\n",
    "        if mutual_batch_size is None: # Might happen if batch_sampler is used\n",
    "             mutual_batch_size = config.get_mutual_learning_batch_size() # Fallback to config value\n",
    "             logger.warning(f\"Could not determine batch size from shared loader, using config value: {mutual_batch_size}\")\n",
    "        else:\n",
    "             logger.info(f\"Mutual learning phase using effective batch size: {mutual_batch_size}\")\n",
    "    except Exception as e:\n",
    "         mutual_batch_size = config.get_mutual_learning_batch_size() # Fallback\n",
    "         logger.error(f\"Error getting batch size from loader: {e}. Using config value: {mutual_batch_size}\")\n",
    "\n",
    "    # --- Training loop for mutual learning ---\n",
    "    for epoch in range(start_epoch, config.mutual_learning_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        logger.info(f\"--- Starting Mutual Learning Epoch {epoch + 1}/{config.mutual_learning_epochs} ---\")\n",
    "        # Set all models to train mode\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "        if hfi: hfi.train() # Set HFI to train mode if it exists\n",
    "\n",
    "        # Accumulators for epoch metrics\n",
    "        epoch_metrics = {name: {'loss': 0.0, 'acc': 0.0, 'ce': 0.0, 'mutual': 0.0, 'cal': 0.0, 'feat': 0.0, 'count': 0} for name in models.keys()}\n",
    "\n",
    "        # Use the determined shared loader\n",
    "        progress_bar = tqdm(shared_train_loader, desc=f\"Mutual Epoch {epoch + 1}\", leave=False, dynamic_ncols=True)\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(progress_bar):\n",
    "            # --- Zero Gradients at the START of the batch ---\n",
    "            for optimizer in optimizers.values():\n",
    "                 optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            batch_start_time = time.time()\n",
    "            # Handle potential variations in batch format\n",
    "            if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "                inputs, targets = batch_data\n",
    "            else:\n",
    "                logger.error(f\"Unexpected batch format at index {batch_idx}: {type(batch_data)}. Skipping batch.\")\n",
    "                continue # Skip batch if format is wrong\n",
    "\n",
    "            inputs, targets = inputs.to(device, non_blocking=config.pin_memory), targets.to(device, non_blocking=config.pin_memory)\n",
    "\n",
    "            # --- 1. Forward pass for ALL models ---\n",
    "            all_logits = {}\n",
    "            all_features = {} # Store features if needed for HFI/alignment\n",
    "\n",
    "            # Use torch.set_grad_enabled(True) to ensure grads are computed within autocast\n",
    "            with torch.set_grad_enabled(True):\n",
    "                for name, model in models.items():\n",
    "                    # Determine appropriate input size/transform if needed\n",
    "                    # Simple approach: resize input to match model's expected size\n",
    "                    model_input_size = config.get_input_size(name)\n",
    "                    if inputs.shape[-2:] != (model_input_size, model_input_size):\n",
    "                         current_input = F.interpolate(inputs, size=(model_input_size, model_input_size), mode='bilinear', align_corners=False)\n",
    "                    else:\n",
    "                         current_input = inputs\n",
    "\n",
    "                    with autocast(device_type='cuda', dtype=torch.float16 if config.mixed_precision_dtype == 'float16' else torch.bfloat16, enabled=config.use_amp):\n",
    "                        outputs = model(current_input)\n",
    "                        # Handle potential tuple output (e.g., Inception aux logits)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            outputs = outputs[0] # Assume primary output is first\n",
    "\n",
    "                    all_logits[name] = outputs\n",
    "\n",
    "                    # Extract features if needed (hook should capture during forward)\n",
    "                    if name in feature_extractors and feature_extractors[name].hook_registered:\n",
    "                        # Features are captured by the hook during model(current_input)\n",
    "                        # Convert features to float32 for stability in loss calculations if needed\n",
    "                        captured_features = feature_extractors[name].features\n",
    "                        if captured_features is not None:\n",
    "                             all_features[name] = captured_features.float() # Store as float32\n",
    "                        else:\n",
    "                             # Log if features weren't captured, maybe hook issue or layer name mismatch\n",
    "                             if batch_idx == 0: # Log only once per epoch start\n",
    "                                 logger.warning(f\"Feature extractor for {name} did not capture features in batch {batch_idx}.\")\n",
    "\n",
    "\n",
    "            # --- 2. Compute HFI features (if enabled) ---\n",
    "            fused_hfi_features = None\n",
    "            hfi_attention_weights = None # To store attention weights for logging\n",
    "            if hfi and 'student' in models and all_features:\n",
    "                # Prepare detached teacher features for HFI\n",
    "                teacher_features_detached = {\n",
    "                    t_name: feat # Features are already detached in HFI's forward method\n",
    "                    for t_name, feat in all_features.items()\n",
    "                    if t_name != 'student' and feat is not None\n",
    "                }\n",
    "                if teacher_features_detached: # Only run HFI if there are teacher features\n",
    "                     with autocast(device_type='cuda', dtype=torch.float16 if config.mixed_precision_dtype == 'float16' else torch.bfloat16, enabled=config.use_amp):\n",
    "                        fused_hfi_features = hfi(teacher_features_detached)\n",
    "                        # Get attention weights if HFI exposes them (assuming softmax applied internally)\n",
    "                        if hasattr(hfi, 'attention_weights'):\n",
    "                             hfi_attention_weights = F.softmax(hfi.attention_weights, dim=0).detach().cpu().numpy()\n",
    "\n",
    "                else:\n",
    "                    if batch_idx == 0: logger.warning(\"No valid teacher features found for HFI in this batch.\")\n",
    "\n",
    "\n",
    "            # --- 3. Compute losses for ALL models ---\n",
    "            losses_to_backward = [] # Store individual losses before backward\n",
    "            batch_metrics = {name: {} for name in models.keys()}\n",
    "\n",
    "            for name, model in models.items():\n",
    "                logits = all_logits[name]\n",
    "                # Detach peer logits for KL loss calculation\n",
    "                peer_logits = {p_name: p_logit.detach().clone()\n",
    "                               for p_name, p_logit in all_logits.items() if p_name != name}\n",
    "\n",
    "                # Calculate feature alignment loss (only for student, using HFI output)\n",
    "                current_feature_loss = torch.tensor(0.0, device=device)\n",
    "                # Ensure student features and HFI features are available and valid\n",
    "                student_features = all_features.get('student')\n",
    "                if config.feature_loss_weight > 0 and name == 'student' and fused_hfi_features is not None and student_features is not None:\n",
    "                     with autocast(device_type='cuda', dtype=torch.float16 if config.mixed_precision_dtype == 'float16' else torch.bfloat16, enabled=config.use_amp):\n",
    "                        # feature_loss_fn expects detached teacher features (HFI output is based on detached features)\n",
    "                        current_feature_loss = feature_loss_fn(student_features, fused_hfi_features)\n",
    "\n",
    "                # Get dynamic weights from config\n",
    "                alpha = config.get_mutual_weight(epoch)\n",
    "                cal_weight = config.get_calibration_weight(epoch)\n",
    "                # Apply feature weight only to the student model\n",
    "                feature_weight = config.get_feature_weight(epoch) if name == 'student' else 0.0\n",
    "\n",
    "                # Calculate total loss using MutualLearningLoss module\n",
    "                with autocast(device_type='cuda', dtype=torch.float16 if config.mixed_precision_dtype == 'float16' else torch.bfloat16, enabled=config.use_amp):\n",
    "                     # Ensure logits and targets are compatible with loss function (e.g., float32)\n",
    "                     total_loss, ce_loss_val, mutual_loss_val, cal_loss_val, feat_loss_val = ml_loss(\n",
    "                        logits=logits.float(), # Cast logits to float32 for stability\n",
    "                        targets=targets,\n",
    "                        peer_logits=peer_logits, # Pass detached peer logits\n",
    "                        model_name=name,\n",
    "                        alpha=alpha,\n",
    "                        cal_weight=cal_weight,\n",
    "                        feature_loss=current_feature_loss.float(), # Cast feature loss\n",
    "                        feature_weight=feature_weight\n",
    "                    )\n",
    "\n",
    "                # Store loss for backward pass (append to list)\n",
    "                losses_to_backward.append({'name': name, 'loss': total_loss})\n",
    "\n",
    "                # --- Log metrics for this model's batch ---\n",
    "                with torch.no_grad(): # Metrics calculation should not track gradients\n",
    "                    acc = (logits.argmax(dim=1) == targets).float().mean().item() * 100\n",
    "                batch_metrics[name] = {\n",
    "                    'loss': total_loss.item(), 'acc': acc, 'ce': ce_loss_val.item(),\n",
    "                    'mutual': mutual_loss_val.item(), 'cal': cal_loss_val.item(),\n",
    "                    'feat': feat_loss_val.item() # Use the returned feature loss value\n",
    "                }\n",
    "                # Accumulate epoch metrics (weighted by batch size)\n",
    "                current_batch_size = inputs.size(0)\n",
    "                epoch_metrics[name]['loss'] += total_loss.item() * current_batch_size\n",
    "                epoch_metrics[name]['acc'] += acc * current_batch_size\n",
    "                epoch_metrics[name]['ce'] += ce_loss_val.item() * current_batch_size\n",
    "                epoch_metrics[name]['mutual'] += mutual_loss_val.item() * current_batch_size\n",
    "                epoch_metrics[name]['cal'] += cal_loss_val.item() * current_batch_size\n",
    "                epoch_metrics[name]['feat'] += feat_loss_val.item() * current_batch_size\n",
    "                epoch_metrics[name]['count'] += current_batch_size\n",
    "\n",
    "\n",
    "            # --- 4. Backward pass for ALL models ---\n",
    "            # Gradient accumulation: Scale and backward for each loss\n",
    "            # Gradients are accumulated implicitly since zero_grad is called once per epoch/step\n",
    "            for item in losses_to_backward:\n",
    "                name = item['name']\n",
    "                loss = item['loss']\n",
    "                scaler = scalers[name]\n",
    "                # Apply gradient accumulation scaling factor\n",
    "                grad_accum_steps_mutual = config.gradient_accumulation_steps\n",
    "                effective_loss = loss / grad_accum_steps_mutual\n",
    "                # Perform backward pass with scaler\n",
    "                scaler.scale(effective_loss).backward() # Gradients accumulate here\n",
    "\n",
    "\n",
    "            # --- 5. Optimizer Step ---\n",
    "            global_grad_accum_steps = config.gradient_accumulation_steps\n",
    "            if (batch_idx + 1) % global_grad_accum_steps == 0:\n",
    "                for name, model in models.items(): # Use models dict keys\n",
    "                    if name in optimizers: # Check if optimizer exists for this model\n",
    "                        scaler = scalers[name]\n",
    "                        optimizer = optimizers[name]\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                # Step HFI optimizer if separate\n",
    "                if 'hfi' in optimizers:\n",
    "                    scaler = scalers['hfi']; optimizer = optimizers['hfi']\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    # torch.nn.utils.clip_grad_norm_(hfi.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer); scaler.update()\n",
    "                \n",
    "                for optimizer in optimizers.values():\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Log memory usage periodically after optimizer step\n",
    "                if (batch_idx // global_grad_accum_steps) % 50 == 0: # Log every 50 optimizer steps\n",
    "                     print_gpu_memory_stats()\n",
    "\n",
    "\n",
    "            # --- Housekeeping ---\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            # Update progress bar (show average loss/acc across models for simplicity)\n",
    "            avg_batch_loss = sum(m['loss'] for m in batch_metrics.values()) / len(models) if models else 0\n",
    "            avg_batch_acc = sum(m['acc'] for m in batch_metrics.values()) / len(models) if models else 0\n",
    "            progress_bar.set_postfix(Loss=f\"{avg_batch_loss:.4f}\", Acc=f\"{avg_batch_acc:.2f}%\", BatchTime=f\"{batch_time:.2f}s\")\n",
    "\n",
    "            # Clear cache periodically based on config\n",
    "            if (batch_idx + 1) % config.clear_cache_every_n_batches == 0:\n",
    "                clear_gpu_cache()\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        # Calculate average metrics for the epoch\n",
    "        for name in models.keys():\n",
    "            count = epoch_metrics[name]['count']\n",
    "            if count > 0:\n",
    "                for metric in ['loss', 'acc', 'ce', 'mutual', 'cal', 'feat']:\n",
    "                    epoch_metrics[name][metric] /= count\n",
    "\n",
    "        # Log epoch summary (average across models)\n",
    "        avg_epoch_loss = sum(m['loss'] for m in epoch_metrics.values()) / len(models) if models else 0\n",
    "        avg_epoch_acc = sum(m['acc'] for m in epoch_metrics.values()) / len(models) if models else 0\n",
    "        logger.info(f\"--- Mutual Epoch {epoch + 1} Summary (Time: {epoch_time:.2f}s): Avg Loss={avg_epoch_loss:.4f}, Avg Acc={avg_epoch_acc:.2f}% ---\")\n",
    "\n",
    "        # Log individual model epoch metrics\n",
    "        for name in models.keys():\n",
    "             metrics = epoch_metrics[name]\n",
    "             logger.info(f\"  {name}: Loss={metrics['loss']:.4f}, Acc={metrics['acc']:.2f}%, CE={metrics['ce']:.4f}, Mut={metrics['mutual']:.4f}, Cal={metrics['cal']:.4f}, Feat={metrics['feat']:.4f}\")\n",
    "\n",
    "\n",
    "        # --- Validation Step ---\n",
    "        logger.info(f\"--- Starting Validation for Epoch {epoch + 1} ---\")\n",
    "        val_metrics = {name: {'loss': 0.0, 'acc': 0.0, 'ece': 0.0, 'count': 0} for name in models.keys()}\n",
    "        all_val_probs = {name: [] for name in models.keys()}\n",
    "        # --- MODIFIED: Store targets globally for the shared validation set ---\n",
    "        all_val_targets_list = []\n",
    "        targets_collected = False # Flag to collect targets only once per epoch\n",
    "        # --- END MODIFICATION ---\n",
    "\n",
    "        with torch.no_grad(): # No gradients needed for validation\n",
    "            # --- Collect all targets first (only needs to be done once) ---\n",
    "            logger.info(\"Collecting validation targets...\")\n",
    "            for batch_data in shared_val_loader: # Iterate once to get all targets\n",
    "                if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "                    _, targets = batch_data\n",
    "                    all_val_targets_list.append(targets.cpu())\n",
    "                else: continue # Skip malformed batch\n",
    "            # Concatenate all targets collected across batches\n",
    "            if all_val_targets_list:\n",
    "                all_targets_tensor = torch.cat(all_val_targets_list)\n",
    "                targets_collected = True\n",
    "                logger.info(f\"Collected {len(all_targets_tensor)} validation targets.\")\n",
    "            else:\n",
    "                logger.error(\"Failed to collect any validation targets!\")\n",
    "                # Handle error: maybe skip ECE calculation or return\n",
    "                all_targets_tensor = None # Set to None if collection failed\n",
    "            # --- End target collection ---\n",
    "\n",
    "            # Now validate each model\n",
    "            for name, model in models.items():\n",
    "                model.eval() # Set model to evaluation mode\n",
    "                val_progress = tqdm(shared_val_loader, desc=f\"Validate {name} E{epoch+1}\", leave=False, dynamic_ncols=True)\n",
    "                # Reset per-model probability list for this epoch\n",
    "                all_val_probs[name] = []\n",
    "\n",
    "                for batch_data in val_progress: # Iterate through loader again for model outputs\n",
    "                    if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "                        inputs, targets = batch_data # Targets are used for loss/acc here\n",
    "                    else: continue # Skip malformed batch\n",
    "\n",
    "                    inputs, targets = inputs.to(device, non_blocking=config.pin_memory), targets.to(device, non_blocking=config.pin_memory)\n",
    "\n",
    "                    # Resize input if necessary\n",
    "                    model_input_size = config.get_input_size(name)\n",
    "                    if inputs.shape[-2:] != (model_input_size, model_input_size):\n",
    "                        current_input = F.interpolate(inputs, size=(model_input_size, model_input_size), mode='bilinear', align_corners=False)\n",
    "                    else:\n",
    "                        current_input = inputs\n",
    "\n",
    "                    outputs = model(current_input)\n",
    "                    if isinstance(outputs, tuple): outputs = outputs[0]\n",
    "\n",
    "                    loss = val_criterion(outputs, targets)\n",
    "                    acc = (outputs.argmax(dim=1) == targets).float().mean().item() * 100\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "                    val_metrics[name]['loss'] += loss.item() * inputs.size(0)\n",
    "                    val_metrics[name]['acc'] += acc * inputs.size(0)\n",
    "                    val_metrics[name]['count'] += inputs.size(0)\n",
    "\n",
    "                    all_val_probs[name].append(probs.cpu()) # Collect probabilities for this model\n",
    "\n",
    "                # Calculate average validation metrics and ECE for the model\n",
    "                count = val_metrics[name]['count']\n",
    "                if count > 0:\n",
    "                    val_metrics[name]['loss'] /= count\n",
    "                    val_metrics[name]['acc'] /= count\n",
    "                    # --- MODIFIED: Calculate ECE using the globally collected targets ---\n",
    "                    if targets_collected and all_targets_tensor is not None: # Check if targets were collected successfully\n",
    "                        model_probs = torch.cat(all_val_probs[name])\n",
    "                        # Ensure number of predictions matches number of targets\n",
    "                        if len(model_probs) == len(all_targets_tensor):\n",
    "                            val_metrics[name]['ece'] = CalibrationMetrics.compute_ece(model_probs, all_targets_tensor).item()\n",
    "                        else:\n",
    "                            logger.error(f\"Mismatch between probabilities ({len(model_probs)}) and targets ({len(all_targets_tensor)}) for model {name}. Skipping ECE calculation.\")\n",
    "                            val_metrics[name]['ece'] = float('inf') # Indicate ECE calculation failed\n",
    "                    else:\n",
    "                        logger.warning(f\"Validation targets not available for model {name}. Skipping ECE calculation.\")\n",
    "                        val_metrics[name]['ece'] = float('inf') # Indicate ECE calculation failed\n",
    "                    # --- END MODIFICATION ---\n",
    "                else:\n",
    "                    val_metrics[name]['ece'] = float('inf') # Assign inf if no validation samples processed\n",
    "\n",
    "                logger.info(f\"  Validation {name}: Loss={val_metrics[name]['loss']:.4f}, Acc={val_metrics[name]['acc']:.2f}%, ECE={val_metrics[name]['ece']:.4f}\")\n",
    "\n",
    "        # --- Scheduler Step ---\n",
    "        for name in optimizers.keys(): # Step schedulers associated with existing optimizers\n",
    "            if name in schedulers and schedulers[name] is not None:\n",
    "                 schedulers[name].step()\n",
    "                 # (Logging LR) ...\n",
    "                 current_lr = optimizers[name].param_groups[0]['lr']\n",
    "                 writer.add_scalar(f'LearningRate/{name}', current_lr, epoch + 1)\n",
    "                 logger.debug(f\"LR for {name} updated to {current_lr:.6f}\")\n",
    "\n",
    "        # Step HFI scheduler if it exists and is separate\n",
    "        if 'hfi' in schedulers:\n",
    "             schedulers['hfi'].step()\n",
    "             # Log HFI LR (assuming it's the first param group in the hfi optimizer)\n",
    "             if 'hfi' in optimizers:\n",
    "                 current_hfi_lr = optimizers['hfi'].param_groups[0]['lr']\n",
    "                 writer.add_scalar('LearningRate/hfi', current_hfi_lr, epoch + 1)\n",
    "                 logger.debug(f\"LR for HFI updated to {current_hfi_lr:.6f}\")\n",
    "\n",
    "\n",
    "        # --- History Logging & Checkpointing ---\n",
    "        history['epochs'].append(epoch + 1)\n",
    "        history['cal_weights'].append(config.get_calibration_weight(epoch))\n",
    "        history['mutual_weights'].append(config.get_mutual_weight(epoch))\n",
    "        history['feature_weights'].append(config.get_feature_weight(epoch)) # Log scheduled feature weight\n",
    "        if hfi_attention_weights is not None:\n",
    "             history['hfi_attention'].append(hfi_attention_weights.tolist()) # Log HFI attention\n",
    "\n",
    "        # Log temperatures if learnable\n",
    "        if ml_loss.learnable_temps is not None:\n",
    "             temps = {name: temp.item() for name, temp in ml_loss.learnable_temps.items()}\n",
    "             history['temperatures'].append(temps)\n",
    "             for name, temp_val in temps.items():\n",
    "                 writer.add_scalar(f'Temperature/{name}', temp_val, epoch + 1)\n",
    "\n",
    "        # Log metrics to history and TensorBoard\n",
    "        for name in models.keys():\n",
    "            # Training metrics\n",
    "            history[name]['train_loss'].append(epoch_metrics[name]['loss'])\n",
    "            history[name]['train_acc'].append(epoch_metrics[name]['acc'])\n",
    "            history[name]['ce_loss'].append(epoch_metrics[name]['ce'])\n",
    "            history[name]['mutual_loss'].append(epoch_metrics[name]['mutual'])\n",
    "            history[name]['cal_loss'].append(epoch_metrics[name]['cal'])\n",
    "            history[name]['feature_loss'].append(epoch_metrics[name]['feat'])\n",
    "            # Validation metrics\n",
    "            history[name]['val_loss'].append(val_metrics[name]['loss'])\n",
    "            history[name]['val_acc'].append(val_metrics[name]['acc'])\n",
    "            history[name]['val_ece'].append(val_metrics[name]['ece'])\n",
    "\n",
    "            # TensorBoard logging\n",
    "            writer.add_scalars(f'Loss/{name}', {'train': epoch_metrics[name]['loss'], 'val': val_metrics[name]['loss']}, epoch + 1)\n",
    "            writer.add_scalars(f'Accuracy/{name}', {'train': epoch_metrics[name]['acc'], 'val': val_metrics[name]['acc']}, epoch + 1)\n",
    "            writer.add_scalar(f'ECE/{name}', val_metrics[name]['ece'], epoch + 1)\n",
    "            writer.add_scalars(f'LossComponents/{name}', {\n",
    "                'CE': epoch_metrics[name]['ce'],\n",
    "                'Mutual': epoch_metrics[name]['mutual'],\n",
    "                'Calibration': epoch_metrics[name]['cal'],\n",
    "                'Feature': epoch_metrics[name]['feat']\n",
    "            }, epoch + 1)\n",
    "\n",
    "        # --- Checkpointing and Early Stopping ---\n",
    "        all_stopped = True # Assume all models should stop unless proven otherwise\n",
    "        for name, model in models.items():\n",
    "            current_val_acc = val_metrics[name]['acc']\n",
    "            current_val_ece = val_metrics[name]['ece']\n",
    "            improved = False\n",
    "\n",
    "            # Checkpoint based on validation accuracy\n",
    "            if current_val_acc > best_val_metrics[name]['acc']:\n",
    "                best_val_metrics[name]['acc'] = current_val_acc\n",
    "                best_states[name] = copy.deepcopy(model.state_dict()) # Save best state\n",
    "                best_epoch[name] = epoch + 1\n",
    "                early_stop_counter[name] = 0 # Reset counter on improvement\n",
    "                improved = True\n",
    "                # Save best model checkpoint immediately\n",
    "                best_model_path = os.path.join(config.checkpoint_dir, f\"{name}_mutual_best_acc.pth\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                logger.info(f\"New best accuracy for {name}: {current_val_acc:.2f}%. Checkpoint saved to {best_model_path}\")\n",
    "            else:\n",
    "                early_stop_counter[name] += 1\n",
    "\n",
    "            # Checkpoint based on validation ECE (lower is better)\n",
    "            if current_val_ece < best_val_metrics[name]['ece']:\n",
    "                best_val_metrics[name]['ece'] = current_val_ece\n",
    "                # Optionally save best ECE state separately or update best_states based on primary metric (acc)\n",
    "                cal_stop_counter[name] = 0 # Reset calibration counter\n",
    "                improved = True # Mark improvement even if only ECE improved\n",
    "                logger.info(f\"New best ECE for {name}: {current_val_ece:.4f}.\")\n",
    "                # Save best ECE model checkpoint\n",
    "                best_ece_model_path = os.path.join(config.checkpoint_dir, f\"{name}_mutual_best_ece.pth\")\n",
    "                torch.save(model.state_dict(), best_ece_model_path)\n",
    "\n",
    "            elif config.enable_calibration_early_stopping:\n",
    "                 # Increment calibration counter only if ECE did not improve AND cal stopping is enabled\n",
    "                 cal_stop_counter[name] += 1\n",
    "\n",
    "\n",
    "            # Check early stopping conditions for this model\n",
    "            stop_model = False\n",
    "            if early_stop_counter[name] >= config.early_stop_patience:\n",
    "                 logger.info(f\"Model {name} triggered early stopping based on accuracy patience ({config.early_stop_patience} epochs).\")\n",
    "                 stop_model = True\n",
    "            if config.enable_calibration_early_stopping and cal_stop_counter[name] >= config.calibration_patience:\n",
    "                 logger.info(f\"Model {name} triggered early stopping based on calibration patience ({config.calibration_patience} epochs).\")\n",
    "                 stop_model = True\n",
    "\n",
    "            if not stop_model:\n",
    "                 all_stopped = False # If any model hasn't stopped, continue training\n",
    "\n",
    "\n",
    "                # --- ADDED CHECKPOINT CALL ---\n",
    "        # Save checkpoint at the end of every mutual learning epoch\n",
    "        save_checkpoint(models, optimizers, schedulers, epoch, config, # Pass epoch (0-based index of completed epoch)\n",
    "                         filename=f\"mutual_checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "        # Break training loop if all models met early stopping criteria\n",
    "        if all_stopped:\n",
    "            logger.info(f\"All models met early stopping criteria at epoch {epoch + 1}. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Clear cache at the end of the epoch\n",
    "        clear_gpu_cache()\n",
    "        print_gpu_memory_stats() # Log memory after epoch cleanup\n",
    "\n",
    "\n",
    "    # --- End of Training Loop ---\n",
    "\n",
    "    # Restore best states based on validation accuracy\n",
    "    logger.info(\"Restoring best model states based on validation accuracy...\")\n",
    "    for name, model in models.items():\n",
    "        if best_states[name]:\n",
    "            try:\n",
    "                # Ensure state dict keys match (handle potential wrapper issues)\n",
    "                if hasattr(model, 'is_wrapper') and model.is_wrapper:\n",
    "                     model.load_state_dict(best_states[name]) # Wrapper handles loading into internal model\n",
    "                else:\n",
    "                     model.load_state_dict(best_states[name])\n",
    "                logger.info(f\"Restored best state for {name} from epoch {best_epoch[name]} (Acc: {best_val_metrics[name]['acc']:.2f}%)\")\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to load best state for {name}: {e}. Keeping final state.\")\n",
    "        else:\n",
    "             logger.warning(f\"No best state found for {name}. Keeping final state.\")\n",
    "\n",
    "\n",
    "    # Save final models (which are now the best models)\n",
    "    logger.info(\"Saving final (best) model states...\")\n",
    "    for name, model in models.items():\n",
    "         final_model_path = os.path.join(config.checkpoint_dir, f\"{name}_mutual_final_best.pth\")\n",
    "         torch.save(model.state_dict(), final_model_path)\n",
    "         logger.info(f\"Final best model for {name} saved to {final_model_path}\")\n",
    "\n",
    "\n",
    "    # Save full training history\n",
    "    history_path = os.path.join(config.results_dir, f\"mutual_learning_{timestamp}_history.json\")\n",
    "    try:\n",
    "        # Use custom encoder for numpy arrays if present in history (e.g., HFI attention)\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history, f, cls=NumpyEncoder, indent=4)\n",
    "        logger.info(f\"Training history saved to {history_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save training history: {e}\")\n",
    "\n",
    "\n",
    "    logger.info(\"Mutual learning phase completed\")\n",
    "    writer.close() # Close TensorBoard writer\n",
    "\n",
    "    # Return models (now loaded with best states) and history\n",
    "    return models, history\n",
    "\n",
    "# Helper class for JSON serialization of numpy arrays\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def plot_mutual_learning_history(history, config):\n",
    "    \"\"\"Plot training history with multiple metrics and models\"\"\"\n",
    "    plt.figure(figsize=(24, 20))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(4, 3, 1)\n",
    "    for name in config.models:\n",
    "        plt.plot(history[name]['val_loss'], label=f\"{name}\")\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(4, 3, 2)\n",
    "    for name in config.models:\n",
    "        plt.plot(history[name]['val_acc'], label=f\"{name}\")\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot ECE\n",
    "    plt.subplot(4, 3, 3)\n",
    "    for name in config.models:\n",
    "        plt.plot(history[name]['val_ece'], label=f\"{name}\")\n",
    "    plt.title('Expected Calibration Error (ECE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ECE')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot mutual loss\n",
    "    plt.subplot(4, 3, 4)\n",
    "    for name in config.models:\n",
    "        plt.plot(history[name]['mutual_loss'], label=f\"{name}\")\n",
    "    plt.title('Mutual Learning Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot calibration loss\n",
    "    plt.subplot(4, 3, 5)\n",
    "    for name in config.models:\n",
    "        plt.plot(history[name]['cal_loss'], label=f\"{name}\")\n",
    "    plt.title('Calibration Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot CE loss\n",
    "    plt.subplot(4, 3, 6)\n",
    "    for name in config.models:\n",
    "        plt.plot(history[name]['ce_loss'], label=f\"{name}\")\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot feature loss\n",
    "    plt.subplot(4, 3, 7)\n",
    "    for name in config.models:\n",
    "        plt.plot(history[name]['feature_loss'], label=f\"{name}\")\n",
    "    plt.title('Feature Alignment Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss weights\n",
    "    plt.subplot(4, 3, 8)\n",
    "    plt.plot(history['mutual_weights'], label=\"Mutual Weight\")\n",
    "    plt.plot(history['cal_weights'], label=\"Calibration Weight\")\n",
    "    plt.title('Loss Component Weights')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot temperatures\n",
    "    plt.subplot(4, 3, 9)\n",
    "    if 'temperatures' in history and history['temperatures']:\n",
    "        for name in config.models:\n",
    "            temps = [epoch_temps.get(name, 4.0) for epoch_temps in history['temperatures']]\n",
    "            plt.plot(temps, label=name)\n",
    "        plt.title('Model Temperatures')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Temperature')\n",
    "        plt.legend()\n",
    "    \n",
    "    # Plot HFI attention weights\n",
    "    plt.subplot(4, 3, 10)\n",
    "    if 'hfi_attention' in history and len(history['hfi_attention']) > 0:\n",
    "        teacher_names = list(history['hfi_attention'][0].keys())\n",
    "        for tname in teacher_names:\n",
    "            weights = [epoch_hfi.get(tname, 0.0) for epoch_hfi in history['hfi_attention']]\n",
    "            plt.plot(weights, label=tname)\n",
    "        plt.title('HFI Attention Weights')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Weight')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Calibration-Aware Mutual Learning History', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(config.results_dir, 'plots', 'mutual_learning_history.png'), dpi=300)\n",
    "    logger.info(f\"Training history plot saved to {os.path.join(config.results_dir, 'plots', 'mutual_learning_history.png')}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_calibration_curve(model, test_loader, config):\n",
    "    \"\"\"Plot calibration reliability diagram\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    confidences = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # Compute confidences and accuracies\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Computing calibration data\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda', enabled=config.use_amp):\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            confidence, predictions = torch.max(probabilities, dim=1)\n",
    "            \n",
    "            accuracy = (predictions == targets).float()\n",
    "            \n",
    "            confidences.append(confidence.cpu())\n",
    "            accuracies.append(accuracy.cpu())\n",
    "    \n",
    "    # Concatenate lists\n",
    "    confidences = torch.cat(confidences)\n",
    "    accuracies = torch.cat(accuracies)\n",
    "    \n",
    "    # Calculate ECE\n",
    "    n_bins = 10\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    bin_confidences = []\n",
    "    bin_accuracies = []\n",
    "    bin_sizes = []\n",
    "    \n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        bin_size = in_bin.sum().item()\n",
    "        \n",
    "        if bin_size > 0:\n",
    "            bin_confidence = confidences[in_bin].mean().item()\n",
    "            bin_accuracy = accuracies[in_bin].mean().item()\n",
    "        else:\n",
    "            bin_confidence = (bin_lower + bin_upper) / 2  # Use bin center if empty\n",
    "            bin_accuracy = 0\n",
    "            \n",
    "        bin_confidences.append(bin_confidence)\n",
    "        bin_accuracies.append(bin_accuracy)\n",
    "        bin_sizes.append(bin_size)\n",
    "    \n",
    "    bin_sizes = np.array(bin_sizes) / sum(bin_sizes)  # Normalize sizes\n",
    "    \n",
    "    # Plot reliability diagram\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Plot bins\n",
    "    plt.bar(bin_lowers, bin_accuracies, width=1/n_bins, align='edge', alpha=0.5, label='Accuracy in bin')\n",
    "    for i, (conf, acc) in enumerate(zip(bin_confidences, bin_accuracies)):\n",
    "        plt.plot([conf, conf], [0, acc], 'r--', alpha=0.3)\n",
    "    \n",
    "    # Add histogram of confidence distribution\n",
    "    twin_ax = plt.twinx()\n",
    "    twin_ax.bar(bin_lowers, bin_sizes, width=1/n_bins, align='edge', alpha=0.3, color='g', label='Samples')\n",
    "    twin_ax.set_ylabel('Proportion of Samples')\n",
    "    \n",
    "    # Calculate ECE\n",
    "    ece = sum(bin_sizes[i] * abs(bin_accuracies[i] - bin_confidences[i]) for i in range(len(bin_sizes)))\n",
    "    \n",
    "    plt.title(f'Calibration Reliability Diagram (ECE = {ece:.4f})')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(config.results_dir, 'plots', 'calibration_curve.png'), dpi=300)\n",
    "    logger.info(f\"Calibration curve saved to {os.path.join(config.results_dir, 'plots', 'calibration_curve.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    return ece\n",
    "\n",
    "def plot_teacher_calibration_curves(teachers, test_loader, student, config):\n",
    "    \"\"\"Plot calibration reliability diagrams for all teachers and the student\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Setup for plotting multiple reliability diagrams\n",
    "    n_bins = 10\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1].numpy()\n",
    "    bin_uppers = bin_boundaries[1:].numpy()\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Color mapping\n",
    "    colors = {'student': 'blue', 'densenet': 'green', 'efficientnet': 'red', \n",
    "              'inception': 'purple', 'mobilenet': 'orange', 'resnet': 'brown', 'vit': 'pink'}\n",
    "    \n",
    "    # Process each model\n",
    "    all_models = {'student': student}\n",
    "    all_models.update(teachers)\n",
    "    \n",
    "    # Track ECE values\n",
    "    ece_values = {}\n",
    "    \n",
    "    for name, model in all_models.items():\n",
    "        model.eval()\n",
    "        \n",
    "        confidences = []\n",
    "        accuracies = []\n",
    "        \n",
    "        # Compute confidences and accuracies\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(test_loader, desc=f\"Computing calibration for {name}\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                with autocast(device_type='cuda', enabled=config.use_amp):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # Handle inception output format\n",
    "                    if name == 'inception' and isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                \n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                confidence, predictions = torch.max(probabilities, dim=1)\n",
    "                \n",
    "                accuracy = (predictions == targets).float()\n",
    "                \n",
    "                confidences.append(confidence.cpu())\n",
    "                accuracies.append(accuracy.cpu())\n",
    "        \n",
    "        # Concatenate lists\n",
    "        confidences = torch.cat(confidences)\n",
    "        accuracies = torch.cat(accuracies)\n",
    "        \n",
    "        # Calculate bin statistics\n",
    "        bin_confidences = []\n",
    "        bin_accuracies = []\n",
    "        bin_sizes = []\n",
    "        \n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "            bin_size = in_bin.sum().item()\n",
    "            \n",
    "            if bin_size > 0:\n",
    "                bin_confidence = confidences[in_bin].mean().item()\n",
    "                bin_accuracy = accuracies[in_bin].mean().item()\n",
    "            else:\n",
    "                bin_confidence = (bin_lower + bin_upper) / 2\n",
    "                bin_accuracy = 0\n",
    "                \n",
    "            bin_confidences.append(bin_confidence)\n",
    "            bin_accuracies.append(bin_accuracy)\n",
    "            bin_sizes.append(bin_size)\n",
    "        \n",
    "        # Calculate ECE\n",
    "        bin_sizes_norm = np.array(bin_sizes) / sum(bin_sizes)\n",
    "        ece = sum(bin_sizes_norm[i] * abs(bin_accuracies[i] - bin_confidences[i]) for i in range(len(bin_sizes)))\n",
    "        ece_values[name] = ece\n",
    "        \n",
    "        # Plot reliability curve\n",
    "        color = colors.get(name, 'gray')\n",
    "        line_style = '-' if name == 'student' else '--'\n",
    "        line_width = 2 if name == 'student' else 1\n",
    "        \n",
    "        # Plot accuracy points\n",
    "        plt.plot(bin_confidences, bin_accuracies, 'o-', color=color, linestyle=line_style, \n",
    "                 linewidth=line_width, label=f\"{name} (ECE={ece:.4f})\")\n",
    "    \n",
    "    plt.title('Calibration Reliability Diagrams')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(config.results_dir, 'plots', 'teacher_calibration_curves.png'), dpi=300)\n",
    "    logger.info(f\"Teacher calibration curves saved to {os.path.join(config.results_dir, 'plots', 'teacher_calibration_curves.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    return ece_values\n",
    "\n",
    "def evaluate_student(student, test_loader, config):\n",
    "    \"\"\"Evaluate the student model on the test set\"\"\"\n",
    "    logger.info(\"Evaluating student model...\")\n",
    "    \n",
    "    student.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc='Testing'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda', enabled=config.use_amp):\n",
    "                outputs = student(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            all_probs.append(probs.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "            all_preds.append(predicted.cpu())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    all_probs_tensor = torch.cat(all_probs, dim=0)\n",
    "    all_targets_tensor = torch.cat(all_targets, dim=0)\n",
    "    all_preds_tensor = torch.cat(all_preds, dim=0)\n",
    "    \n",
    "    # Convert to numpy for sklearn metrics\n",
    "    all_targets_np = all_targets_tensor.numpy()\n",
    "    all_preds_np = all_preds_tensor.numpy()\n",
    "    \n",
    "    f1 = f1_score(all_targets_np, all_preds_np, average='macro')\n",
    "    precision = precision_score(all_targets_np, all_preds_np, average='macro')\n",
    "    recall = recall_score(all_targets_np, all_preds_np, average='macro')\n",
    "    ece = CalibrationMetrics.compute_ece(all_probs_tensor, all_targets_tensor).item()\n",
    "    \n",
    "    logger.info(f\"Test Results:\")\n",
    "    logger.info(f\"Loss: {test_loss:.4f}\")\n",
    "    logger.info(f\"Accuracy: {test_acc:.2f}%\")\n",
    "    logger.info(f\"F1 Score: {f1:.4f}\")\n",
    "    logger.info(f\"Precision: {precision:.4f}\")\n",
    "    logger.info(f\"Recall: {recall:.4f}\")\n",
    "    logger.info(f\"ECE: {ece:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics = {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_acc,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'ece': ece\n",
    "    }\n",
    "    \n",
    "    metrics_path = os.path.join(config.results_dir, 'student_test_metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# In main function, around line 3089\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        config = Config()\n",
    "        logger.info(f\"Configuration: {config}\")\n",
    "        set_seed(config.seed)\n",
    "\n",
    "        # --- Checkpoint Loading Logic ---\n",
    "        start_epoch_init = 0\n",
    "        start_epoch_mutual = 0\n",
    "        init_phase_completed = False\n",
    "        logger.info(\"Creating models...\")\n",
    "        models = create_models(config)\n",
    "\n",
    "        optimizers = {}\n",
    "        schedulers = {}\n",
    "\n",
    "        latest_checkpoint_path, latest_epoch_num, phase = find_latest_checkpoint(config.checkpoint_dir)\n",
    "        init_phase_completed = False # Flag to track if init phase is done\n",
    "\n",
    "        if latest_checkpoint_path:\n",
    "            logger.info(f\"Attempting to load checkpoint: {latest_checkpoint_path}\")\n",
    "            \n",
    "            # Load state into existing models, optimizers, schedulers\n",
    "            # loaded_epoch_num is the epoch *saved* in the checkpoint (i.e., the one that just finished)\n",
    "            loaded_epoch_num, loaded_phase, optimizers, schedulers = load_checkpoint(\n",
    "                latest_checkpoint_path, models, config # Pass config for recreating schedulers\n",
    "            )\n",
    "\n",
    "            if loaded_phase == 'init':\n",
    "                # Check if the loaded epoch is the last one of the init phase\n",
    "                if loaded_epoch_num >= config.initialization_epochs:\n",
    "                    init_phase_completed = True\n",
    "                    start_epoch_init = config.initialization_epochs # Ensure init loop doesn't run\n",
    "                    start_epoch_mutual = 0 # Start mutual from beginning\n",
    "                    logger.info(f\"Initialization phase completed (loaded epoch {loaded_epoch_num}). Starting mutual learning from epoch 1.\")\n",
    "                else:\n",
    "                    # Resuming within init phase\n",
    "                    start_epoch_init = loaded_epoch_num # Start next init epoch\n",
    "                    start_epoch_mutual = 0\n",
    "                    init_phase_completed = False\n",
    "                    logger.info(f\"Resuming initialization phase from epoch {start_epoch_init + 1}\")\n",
    "\n",
    "            elif loaded_phase == 'mutual':\n",
    "                # Resuming within mutual phase\n",
    "                init_phase_completed = True # Init must be done if we are in mutual\n",
    "                start_epoch_init = config.initialization_epochs\n",
    "                start_epoch_mutual = loaded_epoch_num # Start next mutual epoch\n",
    "                logger.info(f\"Resuming mutual learning phase from epoch {start_epoch_mutual + 1}\")\n",
    "\n",
    "            else:\n",
    "                logger.warning(\"Could not determine resume phase from checkpoint. Starting from scratch.\")\n",
    "                start_epoch_init = 0\n",
    "                start_epoch_mutual = 0\n",
    "                init_phase_completed = False\n",
    "        else:\n",
    "            logger.info(\"No checkpoint found. Starting training from scratch.\")\n",
    "            start_epoch_init = 0\n",
    "            start_epoch_mutual = 0\n",
    "            init_phase_completed = False\n",
    "        # --- End Checkpoint Loading Logic ---\n",
    "\n",
    "        for name, model in models.items():\n",
    "            if name not in optimizers: # If optimizer wasn't loaded/recreated\n",
    "                lr = config.get_learning_rate(name)\n",
    "                optimizers[name] = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=config.weight_decay)\n",
    "                logger.info(f\"Created new optimizer for {name}\")\n",
    "            if name not in schedulers or schedulers[name] is None: # If scheduler wasn't loaded/recreated\n",
    "                 opt = optimizers[name]\n",
    "                 lr = config.get_learning_rate(name) # Use config LR for initial setup\n",
    "                 cosine_t_max = max(1, config.mutual_learning_epochs - config.warmup_epochs)\n",
    "                 base_scheduler = CosineAnnealingLR(opt, T_max=cosine_t_max, eta_min=lr * 0.01)\n",
    "                 if config.use_warmup and config.warmup_epochs > 0:\n",
    "                      warmup_total_iters = max(1, config.warmup_epochs)\n",
    "                      warmup_scheduler = LinearLR(opt, start_factor=1e-3, total_iters=warmup_total_iters)\n",
    "                      milestone_epoch = max(1, config.warmup_epochs)\n",
    "                      schedulers[name] = SequentialLR(opt, schedulers=[warmup_scheduler, base_scheduler], milestones=[milestone_epoch])\n",
    "                 else:\n",
    "                      schedulers[name] = base_scheduler\n",
    "                 logger.info(f\"Created new scheduler for {name}\")\n",
    "\n",
    "        # Get data loaders (after potential random state loading)\n",
    "        logger.info(\"Preparing data loaders...\")\n",
    "        train_loader, val_loader, test_loader = get_cifar10_loaders(config)\n",
    "\n",
    "\n",
    "        # --- Run Training Phases ---\n",
    "        # Pass the potentially loaded optimizers/schedulers to the phases\n",
    "        if not init_phase_completed:\n",
    "            logger.info(f\"Starting initialization phase from epoch {start_epoch_init + 1}...\")\n",
    "            # Pass optimizers and schedulers to the init phase\n",
    "            models = initialization_phase(models, train_loader, val_loader, config, optimizers, schedulers, start_epoch=start_epoch_init)\n",
    "            init_phase_completed = True\n",
    "        else:\n",
    "            logger.info(\"Skipping initialization phase.\")\n",
    "\n",
    "        logger.info(\"Clearing GPU cache before mutual learning phase...\")\n",
    "        clear_gpu_cache(); print_gpu_memory_stats()\n",
    "\n",
    "        # Phase 2: Mutual Learning\n",
    "        if start_epoch_mutual < config.mutual_learning_epochs:\n",
    "            logger.info(f\"Starting mutual learning phase from epoch {start_epoch_mutual + 1}...\")\n",
    "             # Pass potentially updated optimizers and schedulers to the mutual phase\n",
    "            models, history = mutual_learning_phase(models, train_loader, val_loader, config, optimizers, schedulers, start_epoch=start_epoch_mutual)\n",
    "            if 'history' in locals() and history: plot_mutual_learning_history(history, config)\n",
    "            else: logger.warning(\"History object not found, skipping plotting.\")\n",
    "        else:\n",
    "            logger.info(\"Mutual learning phase already completed. Skipping.\")\n",
    "        \n",
    "        logger.info(\"Starting final evaluation and export...\")\n",
    "        test_loader_student = None\n",
    "        if isinstance(test_loader, dict): test_loader_student = test_loader.get('student') # Simplified\n",
    "        else: test_loader_student = test_loader\n",
    "        if not test_loader_student and isinstance(test_loader, dict) and test_loader:\n",
    "            first_key = next(iter(test_loader)); test_loader_student = test_loader[first_key]\n",
    "            logger.warning(f\"Student test loader not found, using loader for '{first_key}'.\")\n",
    "\n",
    "        if test_loader_student:\n",
    "            logger.info(\"Plotting calibration curves...\")\n",
    "            for name in models: models[name].to(device)\n",
    "            plot_teacher_calibration_curves({n: m for n, m in models.items() if n != 'student'}, test_loader_student, models['student'], config) # Pass teachers dict\n",
    "            logger.info(\"Evaluating final student model...\")\n",
    "            student_metrics = evaluate_student(models['student'], test_loader_student, config)\n",
    "            logger.info(\"Exporting final student model...\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            final_model_path = os.path.join(config.export_dir, f\"mutual_learning_{timestamp}_final_student.pth\")\n",
    "            torch.save({'model_state_dict': models['student'].state_dict(), 'test_metrics': student_metrics, 'timestamp': timestamp, 'config': config.__dict__}, final_model_path)\n",
    "            logger.info(f\"Final student model exported to {final_model_path}\")\n",
    "        else: logger.error(\"Skipping final evaluation and export due to missing test loader.\")\n",
    "\n",
    "        logger.info(\"Mutual learning script finished.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in main: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\"\n",
    "Mutual Learning Approach Explanation:\n",
    "\n",
    "Unlike conventional knowledge distillation where pre-trained teachers transfer knowledge to a student,\n",
    "mutual learning enables all models to learn collaboratively by sharing knowledge throughout training.\n",
    "This creates a dynamic learning environment where each model both contributes to and benefits from\n",
    "the ensemble's collective knowledge.\n",
    "\n",
    "Key components of the mutual learning approach:\n",
    "\n",
    "1. Two-Phase Training:\n",
    "   - Initialization Phase: Each model briefly trains independently to establish baseline performance\n",
    "   - Mutual Learning Phase: Models exchange knowledge via soft predictions while continuing to learn\n",
    "\n",
    "2. Knowledge Exchange Mechanism:\n",
    "   - Each model learns from both hard labels (ground truth) and soft predictions from peers\n",
    "   - KL divergence measures the alignment between a model's predictions and its peers'\n",
    "   - Peer predictions are detached from the computation graph to prevent gradient propagation back\n",
    "   \n",
    "3. Calibration Awareness:\n",
    "   - Beyond accuracy, models are trained to provide well-calibrated uncertainty estimates\n",
    "   - Calibration loss penalizes overconfidence or underconfidence in predictions\n",
    "   - Temperature scaling helps control the sharpness of probability distributions\n",
    "\n",
    "4. Curriculum Learning:\n",
    "   - Mutual learning and calibration weights gradually increase during training\n",
    "   - This allows models to first learn basic patterns before focusing on knowledge exchange\n",
    "   - Learnable temperature parameters adapt to each model's optimal confidence level\n",
    "   \n",
    "5. Feature Alignment (Optional):\n",
    "   - Beyond output-level knowledge exchange, intermediate feature representations can be aligned\n",
    "   - Feature alignment can help transfer richer representational knowledge between architectures\n",
    "   \n",
    "6. Dynamic Overall Loss:\n",
    "   Loss = (1-Î±) * CE_Loss + Î± * Mutual_KL_Loss + Î»_cal * Calibration_Loss + Î»_feat * Feature_Loss\n",
    "   - CE_Loss: Standard cross-entropy with ground truth labels\n",
    "   - Mutual_KL_Loss: Average KL divergence with peer models' predictions\n",
    "   - Calibration_Loss: Penalizes miscalibration of confidence\n",
    "   - Feature_Loss: Optional component aligning intermediate feature representations\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
